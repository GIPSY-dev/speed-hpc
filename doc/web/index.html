<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Speed: The GCS ENCS Cluster</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='speed-manual.css' rel='stylesheet' type='text/css' /> 
<meta content='speed-manual.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
  <link href='https://latex.now.sh/style.css' rel='stylesheet' />  
  </head><body>
   <div class='maketitle'>
                                                                               

                                                                               
                                                                               

                                                                               

<h2 class='titleHead'>Speed: The GCS ENCS Cluster</h2>
<div class='author'> Serguei A. Mokhov <br class='and' />Gillian A. Roper <br class='and' />Carlos Alarcón Meza <br class='and' />Network, Security and HPC Group<span class='thank-mark'><a href='#tk-1'><span class='tcrm-1000'>∗</span></a></span>
<br />                  <span class='cmr-9'>Gina Cody School of Engineering and Computer Science</span>
<br />                                <span class='cmr-9'>Concordia University</span>
<br />                              <span class='cmr-9'>Montreal, Quebec, Canada</span>
<br />                          <a class='url' href='rt-ex-hpc~AT~encs.concordia.ca'><span class='cmtt-9'>rt-ex-hpc~AT~encs.concordia.ca</span></a><br /></div><br />
<div class='date'><span class='cmbx-10'>Version 7.2-dev-02</span></div>
   <div class='thanks'><br /><a id='tk-1'></a><span class='thank-mark'><span class='tcrm-1000'>∗</span></span>The group acknowledges the initial manual version VI produced by Dr. Scott Bunnell while with
us as well as Dr. Tariq Daradkeh for his instructional support of the users and contribution of
examples.</div></div>
   <section class='abstract' role='doc-abstract'> 
<h3 class='abstracttitle'>
<span class='cmbx-9'>Abstract</span>
</h3>
     <!-- l. 79 --><p class='noindent'><span class='cmr-9'>This document presents a quick start guide to the usage of the Gina Cody School of
     Engineering and Computer Science compute server farm called “Speed” – the GCS Speed
     cluster,  managed  by  the  HPC/NAG  group  of  the  Academic  Information  Technology
     Services (AITS) at GCS, Concordia University, Montreal, Canada.</span>
</p>
</section>
   <h3 class='likesectionHead' id='contents'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
   <span class='sectionToc'>1 <a href='#introduction' id='QQ2-1-2'>Introduction</a></span>
<br />    <span class='subsectionToc'>1.1 <a href='#resources' id='QQ2-1-3'>Resources</a></span>
<br />    <span class='subsectionToc'>1.2 <a href='#team' id='QQ2-1-4'>Team</a></span>
<br />    <span class='subsectionToc'>1.3 <a href='#what-speed-consists-of' id='QQ2-1-5'>What Speed Consists of</a></span>
<br />    <span class='subsectionToc'>1.4 <a href='#what-speed-is-ideal-for' id='QQ2-1-8'>What Speed Is Ideal For</a></span>
<br />    <span class='subsectionToc'>1.5 <a href='#what-speed-is-not' id='QQ2-1-9'>What Speed Is Not</a></span>
<br />    <span class='subsectionToc'>1.6 <a href='#available-software' id='QQ2-1-10'>Available Software</a></span>
<br />    <span class='subsectionToc'>1.7 <a href='#requesting-access' id='QQ2-1-11'>Requesting Access</a></span>
<br />   <span class='sectionToc'>2 <a href='#job-management' id='QQ2-1-12'>Job Management</a></span>
<br />    <span class='subsectionToc'>2.1 <a href='#getting-started' id='QQ2-1-13'>Getting Started</a></span>
<br />     <span class='subsubsectionToc'>2.1.1 <a href='#ssh-connections' id='QQ2-1-14'>SSH Connections</a></span>
<br />     <span class='subsubsectionToc'>2.1.2 <a href='#environment-set-up' id='QQ2-1-15'>Environment Set Up</a></span>
<br />    <span class='subsectionToc'>2.2 <a href='#job-submission-basics' id='QQ2-1-16'>Job Submission Basics</a></span>
<br />     <span class='subsubsectionToc'>2.2.1 <a href='#directives' id='QQ2-1-17'>Directives</a></span>
<br />     <span class='subsubsectionToc'>2.2.2 <a href='#module-loads' id='QQ2-1-18'>Module Loads</a></span>
<br />     <span class='subsubsectionToc'>2.2.3 <a href='#user-scripting' id='QQ2-1-19'>User Scripting</a></span>
<br />    <span class='subsectionToc'>2.3 <a href='#sample-job-script' id='QQ2-1-20'>Sample Job Script</a></span>
<br />    <span class='subsectionToc'>2.4 <a href='#common-job-management-commands-summary' id='QQ2-1-23'>Common Job Management Commands Summary</a></span>
<br />    <span class='subsectionToc'>2.5 <a href='#advanced-sbatch-options' id='QQ2-1-24'>Advanced <span class='cmtt-10'>sbatch </span>Options</a></span>
                                                                               

                                                                               
<br />    <span class='subsectionToc'>2.6 <a href='#array-jobs' id='QQ2-1-25'>Array Jobs</a></span>
<br />    <span class='subsectionToc'>2.7 <a href='#requesting-multiple-cores-ie-multithreading-jobs' id='QQ2-1-26'>Requesting Multiple Cores (i.e., Multithreading Jobs)</a></span>
<br />    <span class='subsectionToc'>2.8 <a href='#interactive-jobs' id='QQ2-1-27'>Interactive Jobs</a></span>
<br />     <span class='subsubsectionToc'>2.8.1 <a href='#command-line' id='QQ2-1-28'>Command Line</a></span>
<br />     <span class='subsubsectionToc'>2.8.2 <a href='#graphical-applications' id='QQ2-1-29'>Graphical Applications</a></span>
<br />     <span class='subsubsectionToc'>2.8.3 <a href='#jupyter-notebooks-in-singularity' id='QQ2-1-31'>Jupyter Notebooks in Singularity</a></span>
<br />     <span class='subsubsectionToc'>2.8.4 <a href='#jupyter-labs-in-conda-and-pytorch' id='QQ2-1-35'>Jupyter Labs in Conda and Pytorch</a></span>
<br />     <span class='subsubsectionToc'>2.8.5 <a href='#jupyter-labs-pytorch-in-python-venv' id='QQ2-1-36'>Jupyter Labs + Pytorch in Python venv</a></span>
<br />     <span class='subsubsectionToc'>2.8.6 <a href='#vscode' id='QQ2-1-37'>VScode</a></span>
<br />    <span class='subsectionToc'>2.9 <a href='#scheduler-environment-variables' id='QQ2-1-39'>Scheduler Environment Variables</a></span>
<br />    <span class='subsectionToc'>2.10 <a href='#ssh-keys-for-mpi' id='QQ2-1-42'>SSH Keys For MPI</a></span>
<br />    <span class='subsectionToc'>2.11 <a href='#creating-virtual-environments' id='QQ2-1-43'>Creating Virtual Environments</a></span>
<br />     <span class='subsubsectionToc'>2.11.1 <a href='#anaconda' id='QQ2-1-44'>Anaconda</a></span>
<br />     <span class='subsubsectionToc'>2.11.2 <a href='#python' id='QQ2-1-47'>Python</a></span>
<br />    <span class='subsectionToc'>2.12 <a href='#example-job-script-fluent' id='QQ2-1-48'>Example Job Script: Fluent</a></span>
<br />    <span class='subsectionToc'>2.13 <a href='#example-job-efficientdet' id='QQ2-1-51'>Example Job: efficientdet</a></span>
<br />    <span class='subsectionToc'>2.14 <a href='#java-jobs' id='QQ2-1-52'>Java Jobs</a></span>
<br />    <span class='subsectionToc'>2.15 <a href='#scheduling-on-the-gpu-nodes' id='QQ2-1-53'>Scheduling On The GPU Nodes</a></span>
<br />     <span class='subsubsectionToc'>2.15.1 <a href='#p-on-multigpu-multinode' id='QQ2-1-54'>P6 on Multi-GPU, Multi-Node</a></span>
<br />     <span class='subsubsectionToc'>2.15.2 <a href='#cuda' id='QQ2-1-55'>CUDA</a></span>
<br />     <span class='subsubsectionToc'>2.15.3 <a href='#special-notes-for-sending-cuda-jobs-to-the-gpu-queue' id='QQ2-1-56'>Special Notes for sending CUDA jobs to the GPU Queue</a></span>
<br />     <span class='subsubsectionToc'>2.15.4 <a href='#openiss-examples' id='QQ2-1-57'>OpenISS Examples</a></span>
<br />    <span class='subsectionToc'>2.16 <a href='#singularity-containers' id='QQ2-1-60'>Singularity Containers</a></span>
<br />   <span class='sectionToc'>3 <a href='#conclusion' id='QQ2-1-61'>Conclusion</a></span>
<br />    <span class='subsectionToc'>3.1 <a href='#important-limitations' id='QQ2-1-62'>Important Limitations</a></span>
<br />    <span class='subsectionToc'>3.2 <a href='#tipstricks' id='QQ2-1-63'>Tips/Tricks</a></span>
<br />    <span class='subsectionToc'>3.3 <a href='#use-cases' id='QQ2-1-64'>Use Cases</a></span>
<br />   <span class='sectionToc'>A <a href='#history' id='QQ2-1-65'>History</a></span>
<br />    <span class='subsectionToc'>A.1 <a href='#acknowledgments' id='QQ2-1-66'>Acknowledgments</a></span>
<br />    <span class='subsectionToc'>A.2 <a href='#migration-from-uge-to-slurm' id='QQ2-1-67'>Migration from UGE to SLURM</a></span>
<br />    <span class='subsectionToc'>A.3 <a href='#phases' id='QQ2-1-69'>Phases</a></span>
<br />     <span class='subsubsectionToc'>A.3.1 <a href='#phase-' id='QQ2-1-70'>Phase 4</a></span>
<br />     <span class='subsubsectionToc'>A.3.2 <a href='#phase-1' id='QQ2-1-71'>Phase 3</a></span>
<br />     <span class='subsubsectionToc'>A.3.3 <a href='#phase-2' id='QQ2-1-72'>Phase 2</a></span>
<br />     <span class='subsubsectionToc'>A.3.4 <a href='#phase-3' id='QQ2-1-73'>Phase 1</a></span>
<br />   <span class='sectionToc'>B <a href='#frequently-asked-questions' id='QQ2-1-74'>Frequently Asked Questions</a></span>
<br />    <span class='subsectionToc'>B.1 <a href='#where-do-i-learn-about-linux' id='QQ2-1-75'>Where do I learn about Linux?</a></span>
<br />    <span class='subsectionToc'>B.2 <a href='#how-to-use-the-bash-shell-on-speed' id='QQ2-1-78'>How to use the “bash shell” on Speed?</a></span>
<br />     <span class='subsubsectionToc'>B.2.1 <a href='#how-do-i-set-bash-as-my-login-shell' id='QQ2-1-79'>How do I set bash as my login shell?</a></span>
<br />     <span class='subsubsectionToc'>B.2.2 <a href='#how-do-i-move-into-a-bash-shell-on-speed' id='QQ2-1-80'>How do I move into a bash shell on Speed?</a></span>
<br />     <span class='subsubsectionToc'>B.2.3 <a href='#how-do-i-use-the-bash-shell-in-an-interactive-session-on-speed' id='QQ2-1-81'>How do I use the bash shell in an interactive session on Speed?</a></span>
<br />     <span class='subsubsectionToc'>B.2.4 <a href='#how-do-i-run-scripts-written-in-bash-on-speed' id='QQ2-1-82'>How do I run scripts written in bash on Speed?</a></span>
<br />    <span class='subsectionToc'>B.3 <a href='#how-to-resolve-disk-quota-exceeded-errors' id='QQ2-1-83'>How to resolve “Disk quota exceeded” errors?</a></span>
<br />     <span class='subsubsectionToc'>B.3.1 <a href='#probable-cause' id='QQ2-1-84'>Probable Cause</a></span>
<br />     <span class='subsubsectionToc'>B.3.2 <a href='#possible-solutions' id='QQ2-1-85'>Possible Solutions</a></span>
<br />     <span class='subsubsectionToc'>B.3.3 <a href='#example-of-setting-working-directories-for-comsol' id='QQ2-1-86'>Example of setting working directories for <span class='cmtt-10'>COMSOL</span></a></span>
<br />     <span class='subsubsectionToc'>B.3.4 <a href='#example-of-setting-working-directories-for-python-modules' id='QQ2-1-87'>Example of setting working directories for <span class='cmtt-10'>Python Modules</span></a></span>
<br />    <span class='subsectionToc'>B.4 <a href='#how-do-i-check-my-jobs-status' id='QQ2-1-88'>How do I check my job’s status?</a></span>
                                                                               

                                                                               
<br />    <span class='subsectionToc'>B.5 <a href='#why-is-my-job-pending-when-nodes-are-empty' id='QQ2-1-89'>Why is my job pending when nodes are empty?</a></span>
<br />     <span class='subsubsectionToc'>B.5.1 <a href='#disabled-nodes' id='QQ2-1-90'>Disabled nodes</a></span>
<br />     <span class='subsubsectionToc'>B.5.2 <a href='#error-in-job-submit-request' id='QQ2-1-91'>Error in job submit request.</a></span>
<br />   <span class='sectionToc'>C <a href='#sister-facilities' id='QQ2-1-92'>Sister Facilities</a></span>
<br />   <span class='sectionToc'><a href='#annotated-bibliography'>Annotated Bibliography</a></span>
   </div>
                                                                               

                                                                               
   <h3 class='sectionHead' id='introduction'><span class='titlemark'>1   </span> <a id='x1-20001'></a>Introduction</h3>
<!-- l. 92 --><p class='noindent'>This document contains basic information required to use “Speed” as well as tips and tricks,
examples, and references to projects and papers that have used Speed. User contributions
of sample jobs and/ or references are welcome. Details are sent to the <span class='cmtt-10'>hpc-ml </span>mailing
list.
</p><!-- l. 97 --><p class='indent'>   <span class='cmbx-10'>Note: </span>On October 20, 2023 with workshops prior, we have completed migration to SLURM (see
Figure <a href='#-speed-slurm-architecture'>2<!-- tex4ht:ref: fig:slurm-arch  --></a>) from Grid Engine (UGE/AGE) as our job scheduler, so this manual has been ported to use
SLURM’s syntax and commands. If you are a long-time GE user, see Appendix <a href='#migration-from-uge-to-slurm'>A.2<!-- tex4ht:ref: appdx:uge-to-slurm  --></a> key highlights of
the move needed to translate your GE jobs to SLURM as well as environment changes. These are also
elaborated throughout this document and our examples as well in case you desire to re-read
it.
</p><!-- l. 104 --><p class='indent'>   If you wish to cite this work in your acknowledgements, you can use our general DOI found on our
GitHub page <a class='url' href='https://dx.doi.org/10.5281/zenodo.5683642'><span class='cmtt-10'>https://dx.doi.org/10.5281/zenodo.5683642</span></a> or a specific version of the manual and
scripts from that link individually.
</p><!-- l. 110 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='resources'><span class='titlemark'>1.1   </span> <a id='x1-30001.1'></a>Resources</h4>
     <ul class='itemize1'>
     <li class='itemize'>Our  public  GitHub  page  where  the  manual  and  sample  job  scripts  are  maintained
     (pull-requests (PRs), subject to review, are welcome):<br class='newline' /><a class='url' href='https://github.com/NAG-DevOps/speed-hpc'><span class='cmtt-10'>https://github.com/NAG-DevOps/speed-hpc</span></a><br class='newline' /><a class='url' href='https://github.com/NAG-DevOps/speed-hpc/pulls'><span class='cmtt-10'>https://github.com/NAG-DevOps/speed-hpc/pulls</span></a>
     </li>
     <li class='itemize'>PDF version of this manual:<br class='newline' /><a class='url' href='https://github.com/NAG-DevOps/speed-hpc/blob/master/doc/speed-manual.pdf'><span class='cmtt-10'>https://github.com/NAG-DevOps/speed-hpc/blob/master/doc/speed-manual.pdf</span></a><br class='newline' />HTML version of this manual:<br class='newline' /><a class='url' href='https://nag-devops.github.io/speed-hpc/'><span class='cmtt-10'>https://nag-devops.github.io/speed-hpc/</span></a>
     </li>
     <li class='itemize'>Our official Concordia page for the “Speed” cluster:<br class='newline' /><a class='url' href='https://www.concordia.ca/ginacody/aits/speed.html'><span class='cmtt-10'>https://www.concordia.ca/ginacody/aits/speed.html</span></a><br class='newline' />which includes access request instructions.
     </li>
     <li class='itemize'>All Speed users are subscribed to the <span class='cmtt-10'>hpc-ml </span>mailing list.
</li></ul>
                                                                               

                                                                               
<!-- l. 145 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='team'><span class='titlemark'>1.2   </span> <a id='x1-40001.2'></a>Team</h4>
<!-- l. 148 --><p class='noindent'>Speed is supported by:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Serguei Mokhov, PhD, Manager, Networks, Security and HPC, AITS
     </li>
     <li class='itemize'>Gillian Roper, Senior Systems Administrator, HPC, AITS
     </li>
     <li class='itemize'>Carlos Alarcón Meza, Systems Administrator, HPC and Networking, AITS</li></ul>
<!-- l. 161 --><p class='noindent'>We receive support from the rest of AITS teams, such as NAG, SAG, FIS, and DOG.<br class='newline' /><a class='url' href='https://www.concordia.ca/ginacody/aits.html'><span class='cmtt-10'>https://www.concordia.ca/ginacody/aits.html</span></a>
</p><!-- l. 167 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='what-speed-consists-of'><span class='titlemark'>1.3   </span> <a id='x1-50001.3'></a>What Speed Consists of</h4>
     <ul class='itemize1'>
     <li class='itemize'>Twenty four (24) 32-core compute nodes, each with 512 GB of memory and approximately
     1 TB of local volatile-scratch disk space (pictured in Figure <a href='#-speed'>1<!-- tex4ht:ref: fig:speed-pics  --></a>).
     </li>
     <li class='itemize'>Twelve  (12)  NVIDIA  Tesla  P6  GPUs,  with  16 GB  of  memory  (compatible  with  the
     CUDA, OpenGL, OpenCL, and Vulkan APIs).
     </li>
     <li class='itemize'>4 VIDPRO nodes, with 6 P6 cards, and 6 V100 cards (32GB), and 256GB of RAM.
     </li>
     <li class='itemize'>7 new SPEED2 servers with 64 CPU cores each 4x A100 80 GB GPUs, partitioned into
     4x 20GB each; larger local storage for TMPDIR.
                                                                               

                                                                               
     </li>
     <li class='itemize'>One AMD FirePro S7150 GPU, with 8 GB of memory (compatible with the Direct X,
     OpenGL, OpenCL, and Vulkan APIs).</li></ul>
   <figure class='figure' id='-speed'> 

                                                                               

                                                                               
<a id='x1-50011'></a>
                                                                               

                                                                               
<!-- l. 193 --><p class='noindent'><img alt='PIC' height='412' src='images/speed-pics.png' width='412' />
</p>
<figcaption class='caption'><span class='id'>Figure 1: </span><span class='content'>Speed</span></figcaption><!-- tex4ht:label?: x1-50011  -->
                                                                               

                                                                               
   </figure>
   <figure class='figure' id='-speed-slurm-architecture'> 

                                                                               

                                                                               
<a id='x1-50022'></a>
                                                                               

                                                                               
<!-- l. 199 --><p class='noindent'><img alt='PIC' height='412' src='images/slurm-arch.png' width='412' />
</p>
<figcaption class='caption'><span class='id'>Figure 2: </span><span class='content'>Speed SLURM Architecture</span></figcaption><!-- tex4ht:label?: x1-50022  -->
                                                                               

                                                                               
   </figure>
   <h4 class='subsectionHead' id='what-speed-is-ideal-for'><span class='titlemark'>1.4   </span> <a id='x1-60001.4'></a>What Speed Is Ideal For</h4>
     <ul class='itemize1'>
     <li class='itemize'>To design and develop, test and run parallel, batch, and other algorithms, scripts with
     partial data sets. “Speed” has been optimised for compute jobs that are multi-core aware,
     require a large memory space, or are iteration intensive.
     </li>
     <li class='itemize'>
     <!-- l. 216 --><p class='noindent'>Prepare them for big clusters: </p>
         <ul class='itemize2'>
         <li class='itemize'>Digital Research Alliance of Canada (Calcul Quebec and Compute Canada)
         </li>
         <li class='itemize'>Cloud platforms</li></ul>
     </li>
     <li class='itemize'>Jobs that are too demanding for a desktop.
     </li>
     <li class='itemize'>Single-core batch jobs; multithreaded jobs typically up to 32 cores (i.e., a single
     machine).
     </li>
     <li class='itemize'>Multi-node multi-core jobs (MPI).
     </li>
     <li class='itemize'>Anything that can fit into a 500-GB memory space and a <span class='cmbx-10'>scratch </span>space of approximately
     10 TB.
     </li>
     <li class='itemize'>CPU-based jobs.
     </li>
     <li class='itemize'>CUDA GPU jobs (<span class='cmtt-10'>speed-01|-03|-05</span>, <span class='cmtt-10'>speed-17</span>, <span class='cmtt-10'>speed-37</span>–<span class='cmtt-10'>speed-43</span>).
                                                                               

                                                                               
     </li>
     <li class='itemize'>Non-CUDA GPU jobs using OpenCL (<span class='cmtt-10'>speed-19 </span>and <span class='cmtt-10'>-01|03|05|17|25|27|37-43</span>).</li></ul>
<!-- l. 240 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='what-speed-is-not'><span class='titlemark'>1.5   </span> <a id='x1-70001.5'></a>What Speed Is Not</h4>
     <ul class='itemize1'>
     <li class='itemize'>Speed is not a web host and does not host websites.
     </li>
     <li class='itemize'>Speed is not meant for Continuous Integration (CI) automation deployments for Ansible
     or similar tools.
     </li>
     <li class='itemize'>Does not run Kubernetes or other container orchestration software.
     </li>
     <li class='itemize'>Does not run Docker. (<span class='cmbx-10'>Note: </span>Speed does run Singularity and many Docker containers
     can be converted to Singularity containers with a single command. See Section <a href='#singularity-containers'>2.16<!-- tex4ht:ref: sect:singularity-containers  --></a>.)
     </li>
     <li class='itemize'>Speed is not for jobs executed outside of the scheduler. (Jobs running outside of the
     scheduler will be killed and all data lost.)</li></ul>
<!-- l. 252 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='available-software'><span class='titlemark'>1.6   </span> <a id='x1-80001.6'></a>Available Software</h4>
<!-- l. 254 --><p class='noindent'>We have a great number of open-source software available and installed on “Speed” – various
Python, CUDA versions, C++/Java compilers, OpenGL, OpenFOAM, OpenCV, TensorFlow,
OpenMPI, OpenISS, MARF <span class='cite'>[<a href='#Xmarf'>24</a>]</span>, etc. There are also a number of commercial packages, subject to
licensing contributions, available, such as MATLAB <span class='cite'>[<a href='#Xmatlab'>13</a>, <a href='#Xscholarpedia-matlab'>23</a>]</span>, Abaqus <span class='cite'>[<a href='#Xabaqus'>1</a>]</span>, Ansys, Fluent <span class='cite'>[<a href='#Xfluent'>2</a>]</span>,
etc.
</p><!-- l. 261 --><p class='indent'>   To see the packages available, run <span class='cmtt-10'>ls -al /encs/pkg/ </span>on <span class='cmtt-10'>speed.encs</span>. In particular, there are
over 2200 programs available in <span class='cmtt-10'>/encs/bin </span>and <span class='cmtt-10'>/encs/pkg </span>under Scientific Linux 7 (EL7). We are
building an equivalent array of programs for the EL9 SPEED2 nodes.
</p>
                                                                               

                                                                               
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 269 --><p class='noindent'>Popular concrete examples: </p>
         <ul class='itemize2'>
         <li class='itemize'>MATLAB (R2016b, R2018a, R2018b, ...)
         </li>
         <li class='itemize'>Fluent (19.2, ...)
         </li>
         <li class='itemize'>Singularity containers (see Section <a href='#singularity-containers'>2.16<!-- tex4ht:ref: sect:singularity-containers  --></a>) can run other operating systems and Linux
         distributions, like Ubuntu’s, as well as converted Docker containers.</li></ul>
     </li>
     <li class='itemize'>We do our best to accommodate custom software requests. Python environments can use
     user-custom installs from within the scratch directory.
     </li>
     <li class='itemize'>
     <!-- l. 285 --><p class='noindent'>A number of specific environments are available and can be loaded using the <span class='cmtt-10'>module </span>command:
     </p>
         <ul class='itemize2'>
         <li class='itemize'>Python (2.3.x - 3.11.x)
         </li>
         <li class='itemize'>Gurobi (7.0.1, 7.5.0, 8.0.0, 8.1.0)
         </li>
         <li class='itemize'>Ansys (16, 17, 18, 19)
         </li>
         <li class='itemize'>OpenFOAM (2.3.1, 3.0.1, 5.0, 6.0)
         </li>
         <li class='itemize'>Cplex 12.6.x to 12.8.x
         </li>
         <li class='itemize'>OpenMPI 1.6.x, 1.8.x, 3.1.3</li></ul>
     </li></ul>
                                                                               

                                                                               
<!-- l. 304 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='requesting-access'><span class='titlemark'>1.7   </span> <a id='x1-90001.7'></a>Requesting Access</h4>
<!-- l. 307 --><p class='noindent'>After reviewing the “What Speed is” (Section <a href='#what-speed-is-ideal-for'>1.4<!-- tex4ht:ref: sect:speed-is-for  --></a>) and “What Speed is Not” (Section <a href='#what-speed-is-not'>1.5<!-- tex4ht:ref: sect:speed-is-not  --></a>), request
access to the “Speed” cluster by emailing: <span class='cmtt-10'>rt-ex-hpc AT encs.concordia.ca</span>. GCS ENCS
faculty and staff may request access directly. Students must include the following in their
message:
</p>
     <ul class='itemize1'>
     <li class='itemize'>GCS ENCS username
     </li>
     <li class='itemize'>Name and email (CC) of the supervisor or instructor
     </li>
     <li class='itemize'>Written request from the supervisor or instructor for the ENCS username to be granted
     access to “Speed”</li></ul>
<!-- l. 320 --><p class='indent'>   Non-GCS faculty / students need to get a “sponsor” within GCS, such that your guest GCS ENCS
account is created first. A sponsor can be any GCS Faculty member you collaborate with. Failing
that, request the approval from our Dean’s Office; via our Associate Deans Drs. Eddie Hoi Ng or
Emad Shihab. External entities to Concordia who collaborate with GCS Concordia researchers, should
also go through the Dean’s office for approvals. Non-GCS students taking a GCS course do have their
GCS ENCS account created automatically, but still need the course instructor’s approval to use the
service.
</p><!-- l. 332 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='job-management'><span class='titlemark'>2   </span> <a id='x1-100002'></a>Job Management</h3>
<!-- l. 335 --><p class='noindent'>In these instructions, anything bracketed like so, <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>&lt;&gt;</span></span></span>, indicates a label/value to be replaced (the entire
bracketed term needs replacement). We use SLURM as the Workload Manager. It supports
primarily two types of jobs: batch and interactive. Batch jobs are used to run unattended
tasks.
</p><!-- l. 342 --><p class='indent'>   TL;DR: Job instructions in a script start with <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>#SBATCH</span></span></span> prefix, for example:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-1'>
#SBATCH --account=speed1 --mem=100M -t 600 -J job-name
#SBATCH --gpus=2 --mail-type=ALL -t 600 --mail-user=YOUR_USERNAME
</pre>
<!-- l. 347 --><p class='nopar'> We use <span class='cmtt-10'>srun </span>for every complex compute step inside the script. Use interactive jobs to set up virtual
environments, compilation, and debugging. <span class='cmtt-10'>salloc </span>is preferred; allows multiple steps. <span class='cmtt-10'>srun </span>can start
interactive jobs as well (see Section <a href='#interactive-jobs'>2.8<!-- tex4ht:ref: sect:interactive-jobs  --></a>). Required and common job parameters: job-name (J),
mail-type, mem, ntasks (n), cpus-per-task, account, -p (partition).
</p><!-- l. 358 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='getting-started'><span class='titlemark'>2.1   </span> <a id='x1-110002.1'></a>Getting Started</h4>
<!-- l. 360 --><p class='noindent'>Before getting started, please review the “What Speed is” (Section <a href='#what-speed-is-ideal-for'>1.4<!-- tex4ht:ref: sect:speed-is-for  --></a>) and “What Speed is Not”
(Section <a href='#what-speed-is-not'>1.5<!-- tex4ht:ref: sect:speed-is-not  --></a>). Once your GCS ENCS account has been granted access to “Speed”, use
your GCS ENCS account credentials to create an SSH connection to <span class='cmtt-10'>speed </span>(an alias for
<span class='cmtt-10'>speed-submit.encs.concordia.ca</span>). All users are expected to have a basic understanding of Linux
and its commonly used commands (see Appendix <a href='#where-do-i-learn-about-linux'>B.1<!-- tex4ht:ref: sect:faqs-linux  --></a> for resources).
</p><!-- l. 370 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='ssh-connections'><span class='titlemark'>2.1.1   </span> <a id='x1-120002.1.1'></a>SSH Connections</h5>
<!-- l. 373 --><p class='noindent'>Requirements to create connections to Speed:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-12002x1'>An active <span class='cmbx-10'>GCS ENCS user account</span>, which has permission to connect to Speed (see
     Section <a href='#requesting-access'>1.7<!-- tex4ht:ref: sect:access  --></a>).
     </li>
<li class='enumerate' id='x1-12004x2'>If you are off campus, an active connection to Concordia’s VPN. Accessing Concordia’s
     VPN requires a Concordia <span class='cmbx-10'>netname</span>.
     </li>
<li class='enumerate' id='x1-12006x3'>Windows systems require a terminal emulator such as PuTTY, Cygwin, or MobaXterm.
     </li>
<li class='enumerate' id='x1-12008x4'>macOS systems do have a Terminal app for this or <span class='cmtt-10'>xterm </span>that comes with XQuarz.</li></ol>
                                                                               

                                                                               
<!-- l. 387 --><p class='indent'>   Open up a terminal window and type in the following SSH command being sure to replace
<span class='obeylines-h'><span class='verb'><span class='cmtt-10'>&lt;ENCSusername&gt;</span></span></span> with your ENCS account’s username.
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-2'>
ssh &lt;ENCSusername&gt;@speed.encs.concordia.ca
</pre>
<!-- l. 392 --><p class='nopar'>
</p><!-- l. 394 --><p class='noindent'>Read the AITS FAQ: <a href='https://www.concordia.ca/ginacody/aits/support/faq/ssh-to-gcs.html'>How do I securely connect to a GCS server?</a>
</p><!-- l. 2 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='environment-set-up'><span class='titlemark'>2.1.2   </span> <a id='x1-130002.1.2'></a>Environment Set Up</h5>
<!-- l. 5 --><p class='noindent'>After creating an SSH connection to Speed, you will need to make sure the <span class='cmtt-10'>srun</span>, <span class='cmtt-10'>sbatch</span>, and <span class='cmtt-10'>salloc</span>
commands are available to you. Type the command name at the command prompt and press enter.
If the command is not available, e.g., (“command not found”) is returned, you need to
make sure your <span class='tctt-1000'>$</span><span class='cmtt-10'>PATH </span>has <span class='cmtt-10'>/local/bin </span>in it. To view your <span class='tctt-1000'>$</span><span class='cmtt-10'>PATH </span>type <span class='cmtt-10'>echo </span><span class='tctt-1000'>$</span><span class='cmtt-10'>PATH </span>at the
prompt.
</p><!-- l. 41 --><p class='indent'>   The next step is to copy a job template to your home directory and to set up your cluster-specific
storage. Execute the following command from within your home directory. (To move to your home
directory, type <span class='cmtt-10'>cd </span>at the Linux prompt and press <span class='cmtt-10'>Enter</span>.)
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-3'>
cp /home/n/nul-uge/template.sh . &amp;&amp; mkdir /speed-scratch/$USER
</pre>
<!-- l. 48 --><p class='nopar'>
</p><!-- l. 52 --><p class='indent'>   <span class='cmbx-10'>Tip: </span>the default shell for GCS ENCS users is <span class='cmtt-10'>tcsh</span>. If you would like to use <span class='cmtt-10'>bash</span>, please contact
<span class='cmtt-10'>rt-ex-hpc AT encs.concordia.ca</span>.
</p><!-- l. 94 --><p class='indent'>   <span class='cmbx-10'>Note: </span>If a “command not found” error appears after you log in to speed, your user account many
have probably have defunct Grid Engine environment commands. See Appendix <a href='#migration-from-uge-to-slurm'>A.2<!-- tex4ht:ref: appdx:uge-to-slurm  --></a> to learn how to
prevent this error on login.
</p><!-- l. 406 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='job-submission-basics'><span class='titlemark'>2.2   </span> <a id='x1-140002.2'></a>Job Submission Basics</h4>
<!-- l. 408 --><p class='noindent'>Preparing your job for submission is fairly straightforward. Start by basing your job script on one of the
examples available in the <span class='cmtt-10'>src/ </span>directory of our GitHub’s (<a class='url' href='https://github.com/NAG-DevOps/speed-hpc'><span class='cmtt-10'>https://github.com/NAG-DevOps/speed-hpc</span></a>).
Job scripts are broken into four main sections:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Directives
     </li>
     <li class='itemize'>Module Loads
     </li>
     <li class='itemize'>User Scripting</li></ul>
<!-- l. 420 --><p class='indent'>   You can clone the tip of our repository to get the examples to start with or download them
individually via a browser or command line:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-4'>
git clone --depth=1 https://github.com/NAG-DevOps/speed-hpc.git
cd speed-hpc/src
</pre>
<!-- l. 427 --><p class='nopar'>
</p><!-- l. 430 --><p class='noindent'>Then to quickly run some sample jobs, you can:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-5'>
sbatch -p ps -t 10 bash.sh
sbatch -p ps -t 10 env.sh
sbatch -p ps -t 10 manual.sh
sbatch -p pg -t 10 lambdal-singularity.sh
</pre>
<!-- l. 438 --><p class='nopar'>
</p><!-- l. 2 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='directives'><span class='titlemark'>2.2.1   </span> <a id='x1-150002.2.1'></a>Directives</h5>
<!-- l. 5 --><p class='noindent'>Directives are comments included at the beginning of a job script that set the shell and the options for
the job scheduler. The shebang directive is always the first line of a script. In your job script, this
directive sets which shell your script’s commands will run in. On “Speed”, we recommend that your
script use a shell from the <span class='cmtt-10'>/encs/bin </span>directory.
</p><!-- l. 12 --><p class='indent'>   To use the <span class='cmtt-10'>tcsh </span>shell, start your script with <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>#!/encs/bin/tcsh</span></span></span>. For <span class='cmtt-10'>bash</span>, start with
<span class='obeylines-h'><span class='verb'><span class='cmtt-10'>#!/encs/bin/bash</span></span></span>. Directives that start with <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>#SBATCH</span></span></span>, set the options for the cluster’s Slurm job
scheduler. The script template, <span class='cmtt-10'>template.sh</span>, provides the essentials:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-6'>
#SBATCH --job-name=&lt;jobname&gt;        ## or -J. Give the job a name
#SBATCH --mail-type=&lt;type&gt;          ## Set type of email notifications
#SBATCH --mail-user=&lt;YOUR_USERNAME&gt;@encs.concordia.ca
#SBATCH --chdir=&lt;directory&gt;         ## or -D, Set working directory where output files will go
#SBATCH --nodes=1                   ## or -N, Node count required for the job
#SBATCH --ntasks=1                  ## or -n, Number of tasks to be launched
#SBATCH --cpus-per-task=&lt;corecount&gt; ## or -c, Core count requested, e.g. 8 cores
#SBATCH --mem=&lt;memory&gt;              ## Assign memory for this job, e.g., 32G memory per node
</pre>
<!-- l. 36 --><p class='nopar'>
</p><!-- l. 38 --><p class='indent'>   Replace the following to adjust the job script for your project(s)
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-15002x1'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>&lt;jobname&gt;</span></span></span> with a job name for the job
     </li>
<li class='enumerate' id='x1-15004x2'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>&lt;YOUR_USERNAME&gt;</span></span></span> with your GCS username
     </li>
<li class='enumerate' id='x1-15006x3'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>&lt;directory&gt;</span></span></span> with the fullpath to your job’s working directory, e.g., where your code,
     source files and where the standard output files will be written to. By default, <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>--chdir</span></span></span>
     sets the current directory as the job’s working directory
     </li>
<li class='enumerate' id='x1-15008x4'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>&lt;type&gt;</span></span></span> with the type of e-mail notifications you wish to receive. Valid options are: NONE,
     BEGIN, END, FAIL, REQUEUE, ALL
     </li>
<li class='enumerate' id='x1-15010x5'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>&lt;corecount&gt;</span></span></span> with the degree of multithreaded parallelism (i.e., cores) allocated to your
     job. Up to 32 by default.
     </li>
<li class='enumerate' id='x1-15012x6'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>&lt;memory&gt;</span></span></span> with the amount of memory, in GB, that you want to be allocated per node. Up
     to 500 depending on the node. NOTE: All jobs MUST set a value for the <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>--mem</span></span></span> option.</li></ol>
                                                                               

                                                                               
<!-- l. 51 --><p class='indent'>   Example with short option equivalents:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-7'>
#SBATCH -J tmpdir                   ## Job’s name set to ’tmpdir’
#SBATCH --mail-type=ALL             ## Receive all email type notifications
#SBATCH --mail-user=a_user@encs.concordia.ca
#SBATCH -D ./                       ## Use current directory as working directory
#SBATCH -N 1                        ## Node count required for the job
#SBATCH -n 1                        ## Number of tasks to be launched
#SBATCH -c 1                        ## Request 8 cores
#SBATCH --mem=32G                   ## Allocate 32G memory per node
</pre>
<!-- l. 62 --><p class='nopar'>
</p><!-- l. 65 --><p class='indent'>   If you are unsure about memory footprints, err on assigning a generous memory space to
your job, so that it does not get prematurely terminated. You can refine <span class='cmtt-10'>--mem </span>values
for future jobs by monitoring the size of a job’s active memory space on <span class='cmtt-10'>speed-submit</span>
with:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-8'>
sacct -j &lt;jobID&gt;
sstat -j &lt;jobID&gt;
</pre>
<!-- l. 81 --><p class='nopar'>
</p><!-- l. 83 --><p class='noindent'>This can be customized to show specific columns:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-9'>
sacct -o jobid,maxvmsize,ntasks%7,tresusageouttot%25 -j &lt;jobID&gt;
sstat -o jobid,maxvmsize,ntasks%7,tresusageouttot%25 -j &lt;jobID&gt;
</pre>
<!-- l. 89 --><p class='nopar'>
</p><!-- l. 91 --><p class='indent'>   Memory-footprint values are also provided for completed jobs in the final e-mail notification as
“maxvmsize”. <span class='cmti-10'>Jobs that request a low-memory footprint are more likely to load on a busy
cluster.</span>
</p><!-- l. 97 --><p class='indent'>   Other essential options are <span class='cmtt-10'>--time</span>, or <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>-t</span></span></span>, and <span class='cmtt-10'>--account</span>, or <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>-A</span></span></span>. </p>
     <ul class='itemize1'>
     <li class='itemize'><span class='cmtt-10'>--time=&lt;time&gt; </span>– is the estimate of wall clock time required for your job to run. As
     preiviously mentioned, the maximum is 7 days for batch and 24 hours for interactive jobs.
     Jobs with a smaller <span class='cmtt-10'>time </span>value will have a higher priority and may result in your job
     being scheduled sooner.
     </li>
     <li class='itemize'><span class='cmtt-10'>--account=&lt;name&gt; </span>– specifies which Account, aka project or association, that the Speed
     resources your job uses should be attributed to. When moving from GE to SLURM users
     most users were assigned to Speed’s two default accounts <span class='cmtt-10'>speed1 </span>and <span class='cmtt-10'>speed2</span>. However,
     users that belong to a particular research group or project are will have a default Account
     like the following <span class='cmtt-10'>aits</span>, <span class='cmtt-10'>vidpro</span>, <span class='cmtt-10'>gipsy</span>, <span class='cmtt-10'>ai2</span>, <span class='cmtt-10'>mpackir</span>, <span class='cmtt-10'>cmos</span>, among others.
</li></ul>
<!-- l. 447 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='module-loads'><span class='titlemark'>2.2.2   </span> <a id='x1-160002.2.2'></a>Module Loads</h5>
<!-- l. 450 --><p class='noindent'>As your job will run on a compute or GPU “Speed” node, and not the submit node, any software that
is needed must be loaded by the job script. Software is loaded within the script just as it would be
from the command line.
</p><!-- l. 454 --><p class='indent'>   To see a list of which modules are available, execute the following from the command line on
<span class='cmtt-10'>speed-submit</span>.
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-10'>
module avail
</pre>
<!-- l. 459 --><p class='nopar'>
</p><!-- l. 461 --><p class='indent'>   To list for a particular program (<span class='cmtt-10'>matlab</span>, for example):
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-11'>
module -t avail matlab
</pre>
<!-- l. 465 --><p class='nopar'>
</p><!-- l. 467 --><p class='indent'>   Which, of course, can be shortened to match all that start with a particular letter:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-12'>
module -t avail m
</pre>
<!-- l. 472 --><p class='nopar'>
</p><!-- l. 474 --><p class='indent'>   Insert the following in your script to load the <span class='cmtt-10'>matlab/R2020a</span>) module:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-13'>
module load matlab/R2020a/default
</pre>
<!-- l. 478 --><p class='nopar'>
</p><!-- l. 480 --><p class='indent'>   Use, <span class='cmtt-10'>unload</span>, in place of, <span class='cmtt-10'>load</span>, to remove a module from active use.
</p><!-- l. 482 --><p class='indent'>   To list loaded modules:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-14'>
module list
</pre>
<!-- l. 486 --><p class='nopar'>
</p><!-- l. 488 --><p class='indent'>   To purge all software in your working environment:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-15'>
module purge
</pre>
<!-- l. 492 --><p class='nopar'>
</p><!-- l. 494 --><p class='indent'>   Typically, only the <span class='cmtt-10'>module load </span>command will be used in your script.
</p><!-- l. 2 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='user-scripting'><span class='titlemark'>2.2.3   </span> <a id='x1-170002.2.3'></a>User Scripting</h5>
<!-- l. 5 --><p class='noindent'>The last part the job script is the scripting that will be executed by the job. This part of
the job script includes all commands required to set up and execute the task your script
has been written to do. Any Linux command can be used at this step. This section can
be a simple call to an executable or a complex loop which iterates through a series of
commands.
</p><!-- l. 11 --><p class='indent'>   Any compute heavy step is preferably should be prefixed by <span class='cmtt-10'>srun </span>as the best practice.
</p><!-- l. 14 --><p class='indent'>   Every software program has a unique execution framework. It is the responsibility of the script’s
author (e.g., you) to know what is required for the software used in your script by reviewing the
software’s documentation. Regardless of which software your script calls, your script should be written
so that the software knows the location of the input and output files as well as the degree of
parallelism.
</p><!-- l. 24 --><p class='indent'>   Jobs which touch data-input and data-output files more than once, should make use of <span class='cmtt-10'>TMPDIR</span>, a
scheduler-provided working space almost 1 TB in size. <span class='cmtt-10'>TMPDIR </span>is created when a job starts, and
exists on the local disk of the compute node executing your job. Using <span class='cmtt-10'>TMPDIR </span>results
in faster I/O operations than those to and from shared storage (which is provided over
NFS).
</p><!-- l. 30 --><p class='indent'>   An sample job script using <span class='cmtt-10'>TMPDIR </span>is available at <span class='cmtt-10'>/home/n/nul-uge/templateTMPDIR.sh</span>: the job
is instructed to change to <span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR</span>, to make the new directory <span class='cmtt-10'>input</span>, to copy data from
<span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_SUBMIT_DIR/references/ </span>to <span class='cmtt-10'>input/ </span>(<span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_SUBMIT_DIR </span>represents the current working
directory), to make the new directory <span class='cmtt-10'>results</span>, to execute the program (which takes input from
<span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR/input/ </span>and writes output to <span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR/results/</span>), and finally to copy the total end results
to an existing directory, <span class='cmtt-10'>processed</span>, that is located in the current working directory. TMPDIR only
exists for the duration of the job, though, so it is very important to copy relevant results from it at
job’s end.
</p><!-- l. 44 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='sample-job-script'><span class='titlemark'>2.3   </span> <a id='x1-180002.3'></a>Sample Job Script</h4>
<!-- l. 46 --><p class='noindent'>Now, let’s look at a basic job script, <a class='url' href='tcsh.sh'><span class='cmtt-10'>tcsh.sh</span></a> in Figure <a href='#-source-code-for-tcshsh'>3<!-- tex4ht:ref: fig:tcsh.sh  --></a> (you can copy it from our GitHub page or
from <span class='cmtt-10'>/home/n/nul-uge</span>).
</p>
   <figure class='figure' id='-source-code-for-tcshsh'> 

                                                                               

                                                                               
<a id='x1-180103'></a>
                                                                               

                                                                               
<!-- l. 50 --><pre class='lstinputlisting' id='listing-1'><span class='label'><a id='x1-18002r1'></a></span><span style='color:#000000'><span class='cmitt-10'>#</span></span><span style='color:#000000'><span class='cmitt-10'>!/</span></span><span style='color:#000000'><span class='cmitt-10'>encs</span></span><span style='color:#000000'><span class='cmitt-10'>/</span></span><span style='color:#000000'><span class='cmitt-10'>bin</span></span><span style='color:#000000'><span class='cmitt-10'>/</span></span><span style='color:#000000'><span class='cmitt-10'>tcsh</span></span> 
<span class='label'><a id='x1-18003r2'></a></span> 
<span class='label'><a id='x1-18004r3'></a></span><span style='color:#000000'><span class='cmitt-10'>#</span></span><span style='color:#000000'><span class='cmitt-10'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10'>--job-name=tcsh-test</span> 
</span><span class='label'><a id='x1-18005r4'></a></span><span style='color:#000000'><span class='cmitt-10'>#</span></span><span style='color:#000000'><span class='cmitt-10'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10'>--mem=1G</span> 
</span><span class='label'><a id='x1-18006r5'></a></span> 
<span class='label'><a id='x1-18007r6'></a></span><span style='color:#000000'><span class='cmtt-10'>sleep</span></span><span style='color:#000000'> <span class='cmtt-10'>30</span> 
</span><span class='label'><a id='x1-18008r7'></a></span><span style='color:#000000'><span class='cmtt-10'>module</span></span><span style='color:#000000'> <span class='cmtt-10'>load gurobi/8.1.0</span> 
</span><span class='label'><a id='x1-18009r8'></a></span><span style='color:#000000'><span class='cmtt-10'>module</span></span><span style='color:#000000'> <span class='cmtt-10'>list</span></span>
</pre>
<figcaption class='caption'><span class='id'>Figure 3: </span><span class='content'>Source code for <a class='url' href='tcsh.sh'><span class='cmtt-10'>tcsh.sh</span></a></span></figcaption><!-- tex4ht:label?: x1-180103  -->
                                                                               

                                                                               
   </figure>
<!-- l. 55 --><p class='indent'>   The first line is the shell declaration (also know as a shebang) and sets the shell to <span class='cmti-10'>tcsh</span>. The lines
that begin with <span class='cmtt-10'>#SBATCH </span>are directives for the scheduler.
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='cmtt-10'>-J </span>(or <span class='cmtt-10'>--job-name</span>) sets <span class='cmti-10'>tcsh-test </span>as the job name
     </li>
     <li class='itemize'><span class='cmtt-10'>--chdir </span>tells the scheduler to execute the job from the current working directory
     </li>
     <li class='itemize'><span class='cmtt-10'>--mem=1GB </span>requests and assigns 1GB of memory to the job. Jobs <span class='cmti-10'>require </span>the <span class='cmtt-10'>--mem </span>option
     to be set either in the script or on the command line; <span class='cmbx-10'>if it’s missing job submission
     will be rejected.</span></li></ul>
<!-- l. 70 --><p class='indent'>   The script then:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Sleeps on a node for 30 seconds
     </li>
     <li class='itemize'>Uses the <span class='cmtt-10'>module </span>command to load the <span class='cmtt-10'>gurobi/8.1.0 </span>environment
     </li>
     <li class='itemize'>Prints the list of loaded modules into a file</li></ul>
<!-- l. 79 --><p class='indent'>   The scheduler command, <span class='cmtt-10'>sbatch</span>, is used to submit (non-interactive) jobs. From an ssh session on
speed-submit, submit this job with <span class='cmtt-10'>sbatch ./tcsh.sh</span>. You will see, <span class='cmtt-10'>"Submitted batch job 2653"</span>
where \(2653\) is a job ID assigned. The commands, <span class='cmtt-10'>squeue </span>and <span class='cmtt-10'>sinfo </span>can be used to look at the status of
the cluster: <span class='cmtt-10'>squeue -l</span>. You will see something like this:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-16'>
[serguei@speed-submit src] % squeue -l
Thu Oct 19 11:38:54 2023
JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)
 2641        ps interact   b_user  RUNNING   19:16:09 1-00:00:00      1 speed-07
 2652        ps interact   a_user  RUNNING      41:40 1-00:00:00      1 speed-07
 2654        ps tcsh-tes  serguei  RUNNING       0:01 7-00:00:00      1 speed-07
[serguei@speed-submit src] % sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
ps*          up 7-00:00:00     14  drain speed-[08-10,12,15-16,20-22,30-32,35-36]
ps*          up 7-00:00:00      1    mix speed-07
ps*          up 7-00:00:00      7   idle speed-[11,19,23-24,29,33-34]
pg           up 1-00:00:00      1  drain speed-17
pg           up 1-00:00:00      3   idle speed-[05,25,27]
pt           up 7-00:00:00      7   idle speed-[37-43]
pa           up 7-00:00:00      4   idle speed-[01,03,25,27]
</pre>
<!-- l. 155 --><p class='nopar'>
</p><!-- l. 158 --><p class='indent'>   Remember that you only have 30 seconds before the job is essentially over, so if you do not see a
similar output, either adjust the sleep time in the script, or execute the <span class='cmtt-10'>sbatch </span>statement more
quickly. The <span class='cmtt-10'>squeue </span>output listed above shows you that your job is running on node <span class='cmtt-10'>speed-07</span>, that it
has a job number of 2654, its time limit of 7 days, etc.
</p><!-- l. 170 --><p class='indent'>   Once the job finishes, there will be a new file in the directory that the job was started from,
with the syntax of, <span class='cmtt-10'>slurm-"job id".out</span>, so in this example the file is, <a class='url' href='slurm-2654.out'><span class='cmtt-10'>slurm-2654.out</span></a>.
This file represents the standard output (and error, if there is any) of the job in question.
If you look at the contents of your newly created file, you will see that it contains the
output of the, <span class='cmtt-10'>module list </span>command. Important information is often written to this
file.
</p>
   <h4 class='subsectionHead' id='common-job-management-commands-summary'><span class='titlemark'>2.4   </span> <a id='x1-190002.4'></a>Common Job Management Commands Summary</h4>
<!-- l. 186 --><p class='noindent'>Here are useful job-management commands:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='cmtt-10'>sbatch -A &lt;ACCOUNT&gt; --t
     &lt;MINUTES&gt; --mem=20G -p &lt;PARTITION&gt; ./&lt;myscript&gt;.sh</span>: once that your job script is
     ready, on <span class='cmtt-10'>speed-submit </span>you can submit it using this
     </li>
     <li class='itemize'><span class='cmtt-10'>squeue -u &lt;ENCSusername&gt;</span>: you can check the status of your job(s)
                                                                               

                                                                               
     </li>
     <li class='itemize'><span class='cmtt-10'>squeue</span>: display cluster status for all users. <span class='cmtt-10'>-A </span>shows per account (e.g., <span class='cmtt-10'>vidpro</span>, <span class='cmtt-10'>gipsy</span>,
     <span class='cmtt-10'>speed1</span>, <span class='cmtt-10'>ai2</span>, <span class='cmtt-10'>aits</span>, etc.), <span class='cmtt-10'>-p </span>per partition (<span class='cmtt-10'>ps</span>, <span class='cmtt-10'>pg</span>, <span class='cmtt-10'>pt</span>, <span class='cmtt-10'>pa</span>), and others. <span class='cmtt-10'>man squeue </span>for
     details.
     </li>
     <li class='itemize'><span class='cmtt-10'>squeue --job [job-ID]</span>: display job information for [job-ID] (said job may be actually
     running, or waiting in the queue).
     </li>
     <li class='itemize'><span class='cmtt-10'>squeue -las</span>: displays individual job steps (for debugging easier to see which step failed
     if you used <span class='cmtt-10'>srun</span>).
     </li>
     <li class='itemize'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>watch -n 1 "sinfo -Nel -pps,pt,pg,pa &amp;&amp; squeue -la"</span></span></span>:  view  <span class='cmtt-10'>sinfo </span>information
     and watch the queue for your job(s).
     </li>
     <li class='itemize'><span class='cmtt-10'>scancel [job-ID]</span>: cancel job [job-ID].
     </li>
     <li class='itemize'><span class='cmtt-10'>scontrol hold [job-ID]</span>: hold queued job, [job-ID], from running.
     </li>
     <li class='itemize'><span class='cmtt-10'>scontrol release [job-ID]</span>: release held job [job-ID].
     </li>
     <li class='itemize'>
     <!-- l. 236 --><p class='noindent'><span class='cmtt-10'>sacct -j [job-ID]</span>: get job stats. <span class='cmtt-10'>maxvmem </span>is one of the more useful stats that you can
     elect to display as a format option.
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-17'>
     % sacct -j 2654
     JobID           JobName  Partition    Account  AllocCPUS      State ExitCode
     ------------ ---------- ---------- ---------- ---------- ---------- --------
     2654          tcsh-test         ps     speed1          1  COMPLETED      0:0
     2654.batch        batch                speed1          1  COMPLETED      0:0
     2654.extern      extern                speed1          1  COMPLETED      0:0
     % sacct -j 2654 -o jobid,user,account,MaxVMSize,Reason%10,TRESUsageOutMax%30
     JobID             User    Account  MaxVMSize     Reason        TRESUsageOutMax
     ------------ --------- ---------- ---------- ---------- ----------------------
     2654           serguei     speed1                  None
     2654.batch                 speed1    296840K             energy=0,fs/disk=1975
     2654.extern                speed1    296312K              energy=0,fs/disk=343
</pre>
     <!-- l. 255 --><p class='nopar'>
     </p><!-- l. 258 --><p class='noindent'>See <span class='cmtt-10'>man sacct </span>or <span class='cmtt-10'>sacct -e </span>for details of the available formatting options. You can define your
     preferred default format in the <span class='cmtt-10'>SACCT_FORMAT </span>environment variable in your <span class='cmtt-10'>.cshrc </span>or <span class='cmtt-10'>.bashrc</span>
     files.
     </p></li>
     <li class='itemize'>
     <!-- l. 264 --><p class='noindent'><span class='cmtt-10'>seff [job-ID]</span>: reports on the efficiency of a job’s cpu and memory utilization. Don’t execute it
     on RUNNING jobs (only on completed/finished jobs), efficiency statistics may be
     misleading.
     </p><!-- l. 267 --><p class='noindent'>If you define the following directives in your batch script, you will receive seff output in your
     email when your job is finished.
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-18'>
     #SBATCH --mail-type=ALL
     #SBATCH --mail-user=USER_NAME@encs.concordia.ca
     ## Replace USER_NAME with your encs username.
</pre>
     <!-- l. 273 --><p class='nopar'>
     </p><!-- l. 276 --><p class='noindent'>Output example:
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-19'>
     Job ID: XXXXX
     Cluster: speed
     User/Group: user1/user1
     State: COMPLETED (exit code 0)
     Nodes: 1
     Cores per node: 4
     CPU Utilized: 00:04:29
     CPU Efficiency: 0.35% of 21:32:20 core-walltime
     Job Wall-clock time: 05:23:05
     Memory Utilized: 2.90 GB
     Memory Efficiency: 2.90% of 100.00 GB
</pre>
     <!-- l. 290 --><p class='nopar'>
</p>
     </li></ul>
<!-- l. 299 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='advanced-sbatch-options'><span class='titlemark'>2.5   </span> <a id='x1-200002.5'></a>Advanced <span class='cmtt-10'>sbatch </span>Options</h4>
<!-- l. 303 --><p class='noindent'>In addition to the basic <span class='cmtt-10'>sbatch </span>options presented earlier, there are a few additional options that are
generally useful:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='cmtt-10'>--mail-type=TYPE</span>: requests that the scheduler e-mail you when a job changes state.
     Where <span class='cmtt-10'>TYPE </span>is <span class='cmtt-10'>ALL</span>, <span class='cmtt-10'>BEGIN</span>, <span class='cmtt-10'>END</span>, or <span class='cmtt-10'>FAIL</span>. Mail is sent to the default address of, <br class='newline' /><span class='cmtt-10'>"&lt;ENCSusername&gt;@encs.concordia.ca"</span>, which you can consult via <span class='cmtt-10'>webmail.encs </span>via
     the VPN, on login.encs via <span class='cmtt-10'>alpine </span>or setup forwarding to @concordia.ca address or offsite,
     unless a different address is supplied (see, <span class='cmtt-10'>--mail-user</span>). The report sent when a job ends
     includes job runtime, as well as the maximum memory value hit (<span class='cmtt-10'>maxvmem</span>).
     </li>
     <li class='itemize'><span class='cmtt-10'>--mail-user email@domain.com</span>: requests that the scheduler use this e-mail notification
     address, rather than the default (see, <span class='cmtt-10'>--mail-type</span>).
     </li>
     <li class='itemize'><span class='cmtt-10'>--export=[ALL | NONE | variables]</span>: exports environment variable(s) that can be used
     by the script.
                                                                               

                                                                               
     </li>
     <li class='itemize'><span class='cmtt-10'>-t [min] </span>or <span class='cmtt-10'>DAYS-HH:MM:SS</span>: sets a job runtime of min or HH:MM:SS. Note that if you
     give a single number, that represents <span class='cmti-10'>minutes</span>, not hours.
     </li>
     <li class='itemize'><span class='cmtt-10'>--depend=[state:job-ID]</span>: run this job only when job [job-ID] finishes. Held jobs appear
     in the queue.
</li></ul>
<!-- l. 347 --><p class='indent'>   The many <span class='cmtt-10'>sbatch </span>options available are read with, <span class='cmtt-10'>man sbatch</span>. Also note that <span class='cmtt-10'>sbatch </span>options can
be specified during the job-submission command, and these <span class='cmti-10'>override </span>existing script options (if
present). The syntax is, <span class='cmtt-10'>sbatch [options] PATHTOSCRIPT</span>, but unlike in the script, the options are
specified without the leading <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>#SBATCH</span></span></span> (e.g., <span class='cmtt-10'>sbatch -J sub-test --chdir=./ --mem=1G
./tcsh.sh</span>).
</p><!-- l. 356 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='array-jobs'><span class='titlemark'>2.6   </span> <a id='x1-210002.6'></a>Array Jobs</h4>
<!-- l. 359 --><p class='noindent'>Array jobs are those that start a batch job or a parallel job multiple times. Each iteration of the job
array is called a task and receives a unique job ID. Only supported for batch jobs; submit time \(&lt; 1\)
second, compared to repeatedly submitting the same regular job over and over even from a
script.
</p><!-- l. 366 --><p class='indent'>   To submit an array job, use the <span class='cmtt-10'>--array </span>option of the <span class='cmtt-10'>sbatch </span>command as follows:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-20'>
sbatch --array=n-m[:s]] &lt;batch_script&gt;
</pre>
<!-- l. 374 --><p class='nopar'>
</p><!-- l. 376 --><p class='indent'>   <span class='cmbx-10'>-t Option Syntax:</span> </p>
     <ul class='itemize1'>
     <li class='itemize'><span class='cmtt-10'>n</span>: indicates the start-id.
     </li>
     <li class='itemize'><span class='cmtt-10'>m</span>: indicates the max-id.
     </li>
     <li class='itemize'><span class='cmtt-10'>s</span>: indicates the step size.</li></ul>
<!-- l. 386 --><p class='indent'>   <span class='cmbx-10'>Examples:</span> </p>
     <ul class='itemize1'>
     <li class='itemize'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>sbatch --array=1-50000 -N1 -i my_in_%a -o my_out_%a array.sh</span></span></span>:  submits  a  job
     with 50000 elements, %a maps to the task-id between 1 and 50K.
     </li>
     <li class='itemize'><span class='cmtt-10'>sbatch --array=10 array.sh</span>: submits a job with 1 task where the task-id is 10.
     </li>
     <li class='itemize'><span class='cmtt-10'>sbatch --array=1-10 array.sh</span>: submits a job with 10 tasks numbered consecutively
     from 1 to 10.
     </li>
     <li class='itemize'><span class='cmtt-10'>sbatch --array=3-15:3 array.sh</span>: submits a jobs with 5 tasks numbered consecutively
     with step size 3 (task-ids 3,6,9,12,15).</li></ul>
<!-- l. 403 --><p class='indent'>   <span class='cmbx-10'>Output files for Array Jobs:</span>
</p><!-- l. 405 --><p class='indent'>   The default and output and error-files are <span class='cmtt-10'>slurm-job_id_task_id.out</span>. This means that Speed
creates an output and an error-file for each task generated by the array-job as well as
one for the super-ordinate array-job. To alter this behavior use the <span class='cmtt-10'>-o </span>and <span class='cmtt-10'>-e </span>option of
<span class='cmtt-10'>sbatch</span>.
</p><!-- l. 416 --><p class='indent'>   For more details about Array Job options, please review the manual pages for <span class='cmtt-10'>sbatch </span>by executing
the following at the command line on speed-submit <span class='cmtt-10'>man sbatch</span>.
                                                                               

                                                                               
</p><!-- l. 423 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='requesting-multiple-cores-ie-multithreading-jobs'><span class='titlemark'>2.7   </span> <a id='x1-220002.7'></a>Requesting Multiple Cores (i.e., Multithreading Jobs)</h4>
<!-- l. 425 --><p class='noindent'>For jobs that can take advantage of multiple machine cores, up to 32 cores (per job) can be requested
in your script with:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-21'>
#SBATCH -n [#cores for processes]
</pre>
<!-- l. 433 --><p class='nopar'>
</p><!-- l. 435 --><p class='indent'>   or
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-22'>
#SBATCH -n 1
#SBATCH -c [#cores for threads of a single process]
</pre>
<!-- l. 440 --><p class='nopar'>
</p><!-- l. 442 --><p class='indent'>   Both <span class='cmtt-10'>sbatch </span>and <span class='cmtt-10'>salloc </span>support <span class='cmtt-10'>-n </span>on the command line, and it should always be used either in
the script or on the command line as the default \(n=1\). <span class='cmbx-10'>Do not request more cores than you think
will be useful</span>, as larger-core jobs are more difficult to schedule. On the flip side, though, if you are
going to be running a program that scales out to the maximum single-machine core count available,
please (please) request 32 cores, to avoid node oversubscription (i.e., to avoid overloading the
CPUs).
</p><!-- l. 451 --><p class='indent'>   <span class='cmbx-10'>Important </span>note about <span class='cmtt-10'>--ntasks </span>or <span class='cmtt-10'>--ntasks-per-node </span>(<span class='cmtt-10'>-n</span>) talks about processes (usually the
ones ran with <span class='cmtt-10'>srun</span>). <span class='cmtt-10'>--cpus-per-task </span>(<span class='cmtt-10'>-c</span>) corresponds to threads per process. Some programs
consider them equivalent, some don’t. Fluent for example uses <span class='cmtt-10'>--ntasks-per-node=8 </span>and
<span class='cmtt-10'>--cpus-per-task=1</span>, some just set <span class='cmtt-10'>--cpus-per-task=8 </span>and <span class='cmtt-10'>--ntasks-per-node=1</span>. If one of them is
not \(1\) then some applications need to be told to use \(n*c\) total cores.
</p><!-- l. 461 --><p class='indent'>   Core count associated with a job appears under, “AllocCPUS”, in the, <span class='cmtt-10'>qacct -j</span>, output.
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-23'>
[serguei@speed-submit src] % squeue -l
Thu Oct 19 20:32:32 2023
JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)
 2652        ps interact   a_user  RUNNING   9:35:18 1-00:00:00      1 speed-07
[serguei@speed-submit src] % sacct -j 2652
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode
------------ ---------- ---------- ---------- ---------- ---------- --------
2652         interacti+         ps     speed1         20    RUNNING      0:0
2652.intera+ interacti+                speed1         20    RUNNING      0:0
2652.extern      extern                speed1         20    RUNNING      0:0
2652.0       gydra_pmi+                speed1         20  COMPLETED      0:0
2652.1       gydra_pmi+                speed1         20  COMPLETED      0:0
2652.2       gydra_pmi+                speed1         20     FAILED      7:0
2652.3       gydra_pmi+                speed1         20     FAILED      7:0
2652.4       gydra_pmi+                speed1         20  COMPLETED      0:0
2652.5       gydra_pmi+                speed1         20  COMPLETED      0:0
2652.6       gydra_pmi+                speed1         20  COMPLETED      0:0
2652.7       gydra_pmi+                speed1         20  COMPLETED      0:0
</pre>
<!-- l. 485 --><p class='nopar'>
</p><!-- l. 489 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='interactive-jobs'><span class='titlemark'>2.8   </span> <a id='x1-230002.8'></a>Interactive Jobs</h4>
<!-- l. 492 --><p class='noindent'>Job sessions can be interactive, instead of batch (script) based. Such sessions can be useful for testing,
debugging, and optimising code and resource requirements, conda or python virtual environments
setup, or any likewise preparatory work prior to batch submission.
</p><!-- l. 498 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='command-line'><span class='titlemark'>2.8.1   </span> <a id='x1-240002.8.1'></a>Command Line</h5>
<!-- l. 500 --><p class='noindent'>To request an interactive job session, use, <span class='cmtt-10'>salloc [options]</span>, similarly to a <span class='cmtt-10'>sbatch </span>command-line
job, e.g.,
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-24'>
salloc -J interactive-test --mem=1G -p ps -n 8
</pre>
<!-- l. 508 --><p class='nopar'> Inside the allocated <span class='cmtt-10'>salloc </span>session you can run shell commands as usual; it is recommended to use
<span class='cmtt-10'>srun </span>for the heavy compute steps inside <span class='cmtt-10'>salloc</span>. If it is a quick a short job just to compile something,
e.g., on a GPU node you can use an interactive <span class='cmtt-10'>srun </span>directly (note no <span class='cmtt-10'>srun </span>can run within <span class='cmtt-10'>srun</span>),
e.g., a 1 hour allocation:
</p><!-- l. 522 --><p class='indent'>   For <span class='cmtt-10'>tcsh</span>:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-25'>
srun --pty -n 8 -p pg --gpus=1 --mem=1Gb -t 60 /encs/bin/tcsh
</pre>
<!-- l. 525 --><p class='nopar'>
</p><!-- l. 527 --><p class='indent'>   For <span class='cmtt-10'>bash</span>:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-26'>
srun --pty -n 8 -p pg --gpus=1 --mem=1Gb -t 60 /encs/bin/bash
</pre>
<!-- l. 530 --><p class='nopar'>
</p><!-- l. 533 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='graphical-applications'><span class='titlemark'>2.8.2   </span> <a id='x1-250002.8.2'></a>Graphical Applications</h5>
<!-- l. 535 --><p class='noindent'>If you need to run an on-Speed graphical-based UI application (e.g., MALTLAB, Abaqus CME, etc.),
or an IDE (PyCharm, VSCode, Eclipse) to develop and test your job’s code interactively you need to
enable X11-forwarding from your client machine to speed then to the compute node. To do
so:
</p><!-- l. 541 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-25002x1'>
     <!-- l. 543 --><p class='noindent'>you need to run an X server on your client machine, such as, </p>
         <ul class='itemize1'>
         <li class='itemize'>on  Windows:  MobaXterm  with  X  turned  on,  or  Xming  +  PuTTY  with  X11
         forwarding, or XOrg under Cygwin
         </li>
         <li class='itemize'>on macOS: XQuarz – use its <span class='cmtt-10'>xterm </span>and <span class='cmtt-10'>ssh -X</span>
         </li>
         <li class='itemize'>on Linux just use <span class='cmtt-10'>ssh -X speed.encs.concordia.ca</span></li></ul>
     <!-- l. 550 --><p class='noindent'>See <a class='url' href='https://www.concordia.ca/ginacody/aits/support/faq/xserver.html'><span class='cmtt-10'>https://www.concordia.ca/ginacody/aits/support/faq/xserver.html</span></a> for
     details.
     </p></li>
<li class='enumerate' id='x1-25004x2'>
     <!-- l. 554 --><p class='noindent'>verify your X connection was properly forwarded by printing the <span class='cmtt-10'>DISPLAY </span>variable:
     </p><!-- l. 556 --><p class='noindent'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>echo $DISPLAY</span></span></span> If it has no output, then your X forwarding is not on and you may need to
     re-login to Speed.
     </p></li>
<li class='enumerate' id='x1-25006x3'>
     <!-- l. 560 --><p class='noindent'>Use the <span class='cmtt-10'>--x11 </span>with <span class='cmtt-10'>salloc </span>or <span class='cmtt-10'>srun</span>:
     </p><!-- l. 562 --><p class='noindent'><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>salloc ... --x11=first ...</span></span></span>
                                                                               

                                                                               
     </p></li>
<li class='enumerate' id='x1-25008x4'>Once landed on a compute node, verify <span class='cmtt-10'>DISPLAY </span>again.
     </li>
<li class='enumerate' id='x1-25010x5'>
     <!-- l. 568 --><p class='noindent'>While running under scheduler, create a run-user directory and set the variable
     <span class='cmtt-10'>XDG_RUNTIME_DIR</span>.
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-27'>
     mkdir -p /speed-scratch/$USER/run-dir
     setenv XDG_RUNTIME_DIR /speed-scratch/$USER/run-dir
</pre>
     <!-- l. 572 --><p class='nopar'>
     </p></li>
<li class='enumerate' id='x1-25012x6'>
     <!-- l. 575 --><p class='noindent'>Launch your graphical application:
     </p><!-- l. 577 --><p class='noindent'><span class='cmtt-10'>module load </span>the required version, then <span class='cmtt-10'>matlab</span>, or <span class='cmtt-10'>abaqus cme</span>, etc.</p></li></ol>
<!-- l. 581 --><p class='indent'>   Here’s an example of starting PyCharm (see Figure <a href='#-pycharm-starting-up-on-a-speed-node'>4<!-- tex4ht:ref: fig:pycharm  --></a>), of which we made a sample local
installation. You can make a similar install under your own directory. If using VSCode, it’s currently
only supported with the <span class='cmtt-10'>--no-sandbox </span>option. <br class='newline' />
</p><!-- l. 585 --><p class='indent'>   BASH version:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-28'>
bash-3.2$ ssh -X speed (XQuartz xterm, PuTTY or MobaXterm have X11 forwarding too)
serguei@speed’s password:
[serguei@speed-submit ~] % echo $DISPLAY
localhost:14.0
[serguei@speed-submit ~] % salloc -p ps --x11=first --mem=4Gb -t 0-06:00
bash-4.4$ echo $DISPLAY
localhost:77.0
bash-4.4$ hostname
speed-01.encs.concordia.ca
bash-4.4$ export XDG_RUNTIME_DIR=/speed-scratch/$USER/run-dir
bash-4.4$ /speed-scratch/nag-public/bin/pycharm.sh
</pre>
<!-- l. 600 --><p class='nopar'>
</p><!-- l. 603 --><p class='indent'>   TCSH version:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-29'>
ssh -X speed (XQuartz xterm, PuTTY or MobaXterm have X11 forwarding too)
[speed-submit] [/home/c/carlos] &gt; echo $DISPLAY
localhost:14.0
[speed-submit] [/home/c/carlos] &gt; cd /speed-scratch/$USER
[speed-submit] [/speed-scratch/carlos] &gt; echo $DISPLAY
localhost:13.0
[speed-submit] [/speed-scratch/carlos] &gt; salloc -pps --x11=first --mem=4Gb -t 0-06:00
[speed-07] [/speed-scratch/carlos] &gt; echo $DISPLAY
localhost:42.0
[speed-07] [/speed-scratch/carlos] &gt; hostname
speed-07.encs.concordia.ca
[speed-07] [/speed-scratch/carlos] &gt; setenv XDG_RUNTIME_DIR /speed-scratch/$USER/run-dir
[speed-07] [/speed-scratch/carlos] &gt; /speed-scratch/nag-public/bin/pycharm.sh
</pre>
<!-- l. 619 --><p class='nopar'>
</p>
   <figure class='figure' id='-pycharm-starting-up-on-a-speed-node'> 

                                                                               

                                                                               
<a id='x1-250134'></a>
                                                                               

                                                                               
<!-- l. 624 --><p class='noindent'><img alt='PIC' height='412' src='images/pycharm.png' width='412' />
</p>
<figcaption class='caption'><span class='id'>Figure 4: </span><span class='content'>PyCharm Starting up on a Speed Node</span></figcaption><!-- tex4ht:label?: x1-250134  -->
                                                                               

                                                                               
   </figure>
   <h5 class='subsubsectionHead' id='jupyter-notebooks-in-singularity'><span class='titlemark'>2.8.3   </span> <a id='x1-260002.8.3'></a>Jupyter Notebooks in Singularity</h5>
<!-- l. 633 --><p class='noindent'>This is an example of running Jupyter notebooks together with Singularity (more on Singularity see
Section <a href='#singularity-containers'>2.16<!-- tex4ht:ref: sect:singularity-containers  --></a>). Here we are using one of the OpenISS-derived containers (see Section <a href='#openiss-examples'>2.15.4<!-- tex4ht:ref: sect:openiss-examples  --></a> as
well).
</p><!-- l. 637 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-26002x1'>Use the <span class='cmtt-10'>--x11 </span>with <span class='cmtt-10'>salloc </span>or <span class='cmtt-10'>srun </span>as described in the above example
     </li>
<li class='enumerate' id='x1-26004x2'>Load Singularity module <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>module load singularity/3.10.4/default</span></span></span>
     </li>
<li class='enumerate' id='x1-26006x3'>
     <!-- l. 646 --><p class='noindent'>Execute this Singularity command on a single line. It’s best to save it in a shell script that you
     could call, since it’s long.
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-30'>
     srun singularity exec -B $PWD\:/speed-pwd,/speed-scratch/$USER\:/my-speed-scratch,/nettemp \
      --env SHELL=/bin/bash --nv /speed-scratch/nag-public/openiss-cuda-conda-jupyter.sif \
      /bin/bash -c ’/opt/conda/bin/jupyter notebook --no-browser --notebook-dir=/speed-pwd \
      --ip="*" --port=8888 --allow-root’
</pre>
     <!-- l. 654 --><p class='nopar'>
     </p></li>
<li class='enumerate' id='x1-26008x4'>
     <!-- l. 658 --><p class='noindent'>Create an <span class='cmtt-10'>ssh </span>tunnel between your computer and the node (<span class='cmtt-10'>speed-XX</span>) where Jupyter is
     running (Using <span class='cmtt-10'>speed-submit </span>as a “jump server”) (Preferably: PuTTY, see Figure <a href='#-ssh-tunnel-configuration-'>5<!-- tex4ht:ref: fig:putty1  --></a> and
     Figure <a href='#-ssh-tunnel-configuration-1'>6<!-- tex4ht:ref: fig:putty2  --></a>)
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-31'>
     ssh -L 8888:speed-XX:8888 YOUR_USER@speed-submit.encs.concordia.ca
</pre>
     <!-- l. 662 --><p class='nopar'> Don’t close the tunnel.
     </p></li>
<li class='enumerate' id='x1-26010x5'>
     <!-- l. 666 --><p class='noindent'>Open a browser, and copy your Jupyter’s token, in the screenshot example in Figure <a href='#-jupyter-running-on-a-speed-node'>7<!-- tex4ht:ref: fig:jupyter  --></a>; each
     time the token will be different, as it printed to you in the terminal.
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-32'>
     http://localhost:8888/?token=5a52e6c0c7dfc111008a803e5303371ed0462d3d547ac3fb
</pre>
     <!-- l. 673 --><p class='nopar'>
     </p></li>
<li class='enumerate' id='x1-26012x6'>Work with your notebook.
</li></ol>
   <figure class='figure' id='-ssh-tunnel-configuration-'> 

                                                                               

                                                                               
<a id='x1-260135'></a>
                                                                               

                                                                               
<div class='fbox'><img alt='PIC' src='images/putty1.png' /></div>
<figcaption class='caption'><span class='id'>Figure 5: </span><span class='content'>SSH tunnel configuration 1</span></figcaption><!-- tex4ht:label?: x1-260135  -->
                                                                               

                                                                               
   </figure>
   <figure class='figure' id='-ssh-tunnel-configuration-1'> 

                                                                               

                                                                               
<a id='x1-260146'></a>
                                                                               

                                                                               
<div class='fbox'><img alt='PIC' src='images/putty2.png' /></div>
<figcaption class='caption'><span class='id'>Figure 6: </span><span class='content'>SSH tunnel configuration 2</span></figcaption><!-- tex4ht:label?: x1-260146  -->
                                                                               

                                                                               
   </figure>
   <figure class='figure' id='-jupyter-running-on-a-speed-node'> 

                                                                               

                                                                               
<a id='x1-260157'></a>
                                                                               

                                                                               
<div class='fbox'><img alt='PIC' height='412' src='images/jupyter.png' width='412' /></div>
<figcaption class='caption'><span class='id'>Figure 7: </span><span class='content'>Jupyter running on a Speed node</span></figcaption><!-- tex4ht:label?: x1-260157  -->
                                                                               

                                                                               
   </figure>
   <h5 class='subsubsectionHead' id='jupyter-labs-in-conda-and-pytorch'><span class='titlemark'>2.8.4   </span> <a id='x1-270002.8.4'></a>Jupyter Labs in Conda and Pytorch</h5>
<!-- l. 706 --><p class='noindent'>This is an example of Jupyter Labs running in a Conda environment, with Pytorch
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 710 --><p class='noindent'>Environment preparation: for the FIRST time:
         </p><ol class='enumerate1'>
<li class='enumerate' id='x1-27002x1'>Go to your speed-scratch directory: <span class='cmtt-10'>cd /speed-scratch/</span><span class='tctt-1000'>$</span><span class='cmtt-10'>USER</span>
         </li>
<li class='enumerate' id='x1-27004x2'>Create a Jupyter (name of your choice) directory: <span class='cmtt-10'>mkdir -p Jupyter</span>
         </li>
<li class='enumerate' id='x1-27006x3'>Go to Jupyter: <span class='cmtt-10'>cd Jupyter</span>
         </li>
<li class='enumerate' id='x1-27008x4'>Open an Interactive session: <span class='cmtt-10'>salloc --mem=50G --gpus=1 -ppg </span>(or -ppt)
         </li>
<li class='enumerate' id='x1-27010x5'>
         <!-- l. 721 --><p class='noindent'>Set env. variables, conda environment, jupyter+pytorch installation
                                                                               

                                                                               
</p>
         <pre class='verbatim' id='verbatim-33'>
         module load anaconda3/2023.03/default
         setenv TMPDIR /speed-scratch/$USER/tmp
         setenv TMP /speed-scratch/$USER/tmp
         setenv CONDA_PKGS_DIRS /speed-scratch/$USER/Jupyter/pkgs
         conda create -p /speed-scratch/$USER/Jupyter/jupyter-env
         conda activate /speed-scratch/$USER/Jupyter/jupyter-env
         conda install -c conda-forge jupyterlab
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
         exit
</pre>
         <!-- l. 733 --><p class='nopar'></p></li></ol>
     </li>
     <li class='itemize'>
     <!-- l. 737 --><p class='noindent'>Running Jupyter Labs, from <span class='cmbx-10'>speed-submit</span>:
         </p><ol class='enumerate1'>
<li class='enumerate' id='x1-27012x1'>
         <!-- l. 740 --><p class='noindent'>Open an Interactive session: <span class='cmtt-10'>salloc --mem=50G --gpus=1 -ppg </span>(or -ppt)
                                                                               

                                                                               
</p>
         <pre class='verbatim' id='verbatim-34'>
         cd /speed-scratch/$USER/Jupyter
         module load anaconda3/2023.03/default
         setenv TMPDIR /speed-scratch/$USER/tmp
         setenv TMP /speed-scratch/$USER/tmp
         setenv CONDA_PKGS_DIRS /speed-scratch/$USER/Jupyter/pkgs
         conda activate /speed-scratch/$USER/Jupyter/jupyter-env
         jupyter lab --no-browser --notebook-dir=$PWD --ip="*" --port=8888 --port-retries=50
</pre>
         <!-- l. 750 --><p class='nopar'>
         </p></li>
<li class='enumerate' id='x1-27014x2'>Verify which port the system has assigned to Jupyter: <span class='cmtt-10'>http://localhost:XXXX/lab?token=</span>
         </li>
<li class='enumerate' id='x1-27016x3'>SSH Tunnel creation: similar to Jupyter in Singularity, see Section <a href='#jupyter-notebooks-in-singularity'>2.8.3<!-- tex4ht:ref: sect:jupyter  --></a>
         </li>
<li class='enumerate' id='x1-27018x4'>Open a browser and type: <span class='cmtt-10'>localhost:XXXX </span>(port assigned)</li></ol>
     </li></ul>
<!-- l. 763 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='jupyter-labs-pytorch-in-python-venv'><span class='titlemark'>2.8.5   </span> <a id='x1-280002.8.5'></a>Jupyter Labs + Pytorch in Python venv</h5>
<!-- l. 766 --><p class='noindent'>This is an example of Jupyter Labs running in a Python Virtual environment (venv), with
Pytorch
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 770 --><p class='noindent'>Environment preparation: for the FIRST time:
         </p><ol class='enumerate1'>
<li class='enumerate' id='x1-28002x1'>Go to your speed-scratch directory: <span class='cmtt-10'>cd /speed-scratch/</span><span class='tctt-1000'>$</span><span class='cmtt-10'>USER</span>
         </li>
<li class='enumerate' id='x1-28004x2'>Open an Interactive session: <span class='cmtt-10'>salloc --mem=50G --gpus=1 --constraint=el9</span>
                                                                               

                                                                               
         </li>
<li class='enumerate' id='x1-28006x3'>
         <!-- l. 777 --><p class='noindent'>Create Python venv and install jupyterlab+pytorch
                                                                               

                                                                               
</p>
         <pre class='verbatim' id='verbatim-35'>
         module load python/3.11.5/default
         setenv TMPDIR /speed-scratch/$USER/tmp
         setenv TMP /speed-scratch/$USER/tmp
         setenv PIP_CACHE_DIR /speed-scratch/$USER/tmp/cache
         python -m venv /speed-scratch/$USER/tmp/jupyter-venv
         source /speed-scratch/$USER/tmp/jupyter-venv/bin/activate.csh
         pip install jupyterlab
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
         exit
</pre>
         <!-- l. 789 --><p class='nopar'></p></li></ol>
     </li>
     <li class='itemize'>
     <!-- l. 793 --><p class='noindent'>Running Jupyter Labs, from <span class='cmbx-10'>speed-submit</span>:
         </p><ol class='enumerate1'>
<li class='enumerate' id='x1-28008x1'>
         <!-- l. 796 --><p class='noindent'>Open an Interactive session: <span class='cmtt-10'>salloc --mem=50G --gpus=1 --constraint=el9</span>
                                                                               

                                                                               
</p>
         <pre class='verbatim' id='verbatim-36'>
         cd /speed-scratch/$USER
         module load python/3.11.5/default
         setenv PIP_CACHE_DIR /speed-scratch/$USER/tmp/cache
         source /speed-scratch/$USER/tmp/jupyter-venv/bin/activate.csh
         jupyter lab --no-browser --notebook-dir=$PWD --ip="0.0.0.0" --port=8888 --port-retries=50
</pre>
         <!-- l. 804 --><p class='nopar'>
         </p></li>
<li class='enumerate' id='x1-28010x2'>Verify which port the system has assigned to Jupyter: <span class='cmtt-10'>http://localhost:XXXX/lab?token=</span>
         </li>
<li class='enumerate' id='x1-28012x3'>SSH Tunnel creation: similar to Jupyter in Singularity, see Section <a href='#jupyter-notebooks-in-singularity'>2.8.3<!-- tex4ht:ref: sect:jupyter  --></a>
         </li>
<li class='enumerate' id='x1-28014x4'>Open a browser and type: <span class='cmtt-10'>localhost:XXXX </span>(port assigned)</li></ol>
     </li></ul>
<!-- l. 818 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='vscode'><span class='titlemark'>2.8.6   </span> <a id='x1-290002.8.6'></a>VScode</h5>
<!-- l. 821 --><p class='noindent'>This is an example of running VScode, it’s similar to Jupyter notebooks, but it doesn’t use containers.
This a Web version, it exists the local(workstation)-remote(speed-node) version too, but it is for
Advanced users (no support, execute it at your own risk).
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 826 --><p class='noindent'>Environment preparation: for the FIRST time:
         </p><ol class='enumerate1'>
<li class='enumerate' id='x1-29002x1'>Go to your speed-scratch directory: <span class='cmtt-10'>cd /speed-scratch/</span><span class='tctt-1000'>$</span><span class='cmtt-10'>USER</span>
         </li>
<li class='enumerate' id='x1-29004x2'>Create a vscode directory: <span class='cmtt-10'>mkdir vscode</span>
                                                                               

                                                                               
         </li>
<li class='enumerate' id='x1-29006x3'>Go to vscode: <span class='cmtt-10'>cd vscode</span>
         </li>
<li class='enumerate' id='x1-29008x4'>Create home and projects: <span class='cmtt-10'>mkdir </span><span class='cmsy-10'>{</span><span class='cmtt-10'>home,projects</span><span class='cmsy-10'>}</span>
         </li>
<li class='enumerate' id='x1-29010x5'>Create this directory: <span class='cmtt-10'>mkdir -p /speed-scratch/</span><span class='tctt-1000'>$</span><span class='cmtt-10'>USER/run-user</span></li></ol>
     </li>
     <li class='itemize'>
     <!-- l. 840 --><p class='noindent'>Running VScode
         </p><ol class='enumerate1'>
<li class='enumerate' id='x1-29012x1'>Go to your vscode directory: <span class='cmtt-10'>cd /speed-scratch/</span><span class='tctt-1000'>$</span><span class='cmtt-10'>USER/vscode</span>
         </li>
<li class='enumerate' id='x1-29014x2'>Open interactive session: <span class='cmtt-10'>salloc --mem=10Gb --constraint=el9</span>
         </li>
<li class='enumerate' id='x1-29016x3'>Set              environment              variable:               <span class='cmtt-10'>setenv XDG_RUNTIME_DIR
         /speed-scratch/</span><span class='tctt-1000'>$</span><span class='cmtt-10'>USER/run-user</span>
         </li>
<li class='enumerate' id='x1-29018x4'>
         <!-- l. 849 --><p class='noindent'>Run VScode, change the port if needed.
                                                                               

                                                                               
</p>
         <pre class='verbatim' id='verbatim-37'>
         /speed-scratch/nag-public/code-server-4.22.1/bin/code-server --user-data-dir=$PWD\/projects \
         --config=$PWD\/home/.config/code-server/config.yaml --bind-addr="0.0.0.0:8080" $PWD\/projects
</pre>
         <!-- l. 854 --><p class='nopar'>
         </p></li>
<li class='enumerate' id='x1-29020x5'>SSH Tunnel creation: similar to Jupyter, see Section <a href='#jupyter-notebooks-in-singularity'>2.8.3<!-- tex4ht:ref: sect:jupyter  --></a>
         </li>
<li class='enumerate' id='x1-29022x6'>Open a browser and type: <span class='cmtt-10'>localhost:8080</span>
         </li>
<li class='enumerate' id='x1-29024x7'>
         <!-- l. 861 --><p class='noindent'>If the browser asks for password:
                                                                               

                                                                               
</p>
         <pre class='verbatim' id='verbatim-38'>
         cat /speed-scratch/$USER/vscode/home/.config/code-server/config.yaml
</pre>
         <!-- l. 864 --><p class='nopar'>
</p>
         </li></ol>
     </li></ul>
   <figure class='figure' id='-vscode-running-on-a-speed-node'> 

                                                                               

                                                                               
<a id='x1-290258'></a>
                                                                               

                                                                               
<div class='fbox'><img alt='PIC' height='412' src='images/vscode.png' width='412' /></div>
<figcaption class='caption'><span class='id'>Figure 8: </span><span class='content'>VScode running on a Speed node</span></figcaption><!-- tex4ht:label?: x1-290258  -->
                                                                               

                                                                               
   </figure>
   <h4 class='subsectionHead' id='scheduler-environment-variables'><span class='titlemark'>2.9   </span> <a id='x1-300002.9'></a>Scheduler Environment Variables</h4>
<!-- l. 880 --><p class='noindent'>The scheduler presents a number of environment variables that can be used in your jobs. You can
invoke <span class='cmtt-10'>env </span>or <span class='cmtt-10'>printenv </span>in your job to know what hose are (most begin with the prefix <span class='cmtt-10'>SLURM</span>). Some
of the more useful ones are:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR </span>– the path to the job’s temporary space on the node. It <span class='cmti-10'>only  </span>exists for the
     duration of the job, so if data in the temporary space are important, they absolutely need
     to be accessed before the job terminates.
     </li>
     <li class='itemize'><span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_SUBMIT_DIR </span>– the path to the job’s working directory (likely an NFS-mounted
     path). If, <span class='cmtt-10'>--chdir</span>, was stipulated, that path is taken; if not, the path defaults to your
     home directory.
     </li>
     <li class='itemize'><span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_JOBID </span>– your current jobs ID, useful for some manipulation and reporting.
     </li>
     <li class='itemize'><span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_JOB_NODELIST</span>=nodes participating in your job.
     </li>
     <li class='itemize'><span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_ARRAY_TASK_ID</span>=for array jobs (see Section <a href='#array-jobs'>2.6<!-- tex4ht:ref: sect:array-jobs  --></a>).
     </li>
     <li class='itemize'>
     <!-- l. 921 --><p class='noindent'>See a more complete list here:
</p>
         <ul class='itemize2'>
         <li class='itemize'><a class='url' href='https://slurm.schedmd.com/srun.html#SECTION_INPUT-ENVIRONMENT-VARIABLES'><span class='cmtt-9'>https://slurm.schedmd.com/srun.html#SECTION_INPUT-ENVIRONMENT-VARIABLES</span></a>
         </li>
         <li class='itemize'><a class='url' href='https://slurm.schedmd.com/srun.html#SECTION_OUTPUT-ENVIRONMENT-VARIABLES'><span class='cmtt-9'>https://slurm.schedmd.com/srun.html#SECTION_OUTPUT-ENVIRONMENT-VARIABLES</span></a></li></ul>
     </li></ul>
<!-- l. 934 --><p class='noindent'>In Figure <a href='#-source-code-for-tmpdirsh'>9<!-- tex4ht:ref: fig:tmpdir.sh  --></a> is a sample script, using some of these.
</p>
   <figure class='figure' id='-source-code-for-tmpdirsh'> 

                                                                               

                                                                               
<a id='x1-300199'></a>
                                                                               

                                                                               
<!-- l. 938 --><pre class='lstinputlisting' id='listing-2'><span class='label'><a id='x1-30002r1'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>!/</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>encs</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>/</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>bin</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>/</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>tcsh</span></span> 
<span class='label'><a id='x1-30003r2'></a></span> 
<span class='label'><a id='x1-30004r3'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--job-name=tmpdir</span> </span><span class='cmitt-10x-x-80'>     </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Give the job a name</span> 
</span><span class='label'><a id='x1-30005r4'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--mail-type=ALL</span> </span><span class='cmitt-10x-x-80'>       </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Receive all email type notifications</span> 
</span><span class='label'><a id='x1-30006r5'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--mail-user=YOUR_USER_NAME@encs.concordia.ca</span> 
</span><span class='label'><a id='x1-30007r6'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--chdir=./</span> </span><span class='cmitt-10x-x-80'>            </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Use currect directory as working directory</span> 
</span><span class='label'><a id='x1-30008r7'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--nodes=1</span> 
</span><span class='label'><a id='x1-30009r8'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--ntasks=1</span> 
</span><span class='label'><a id='x1-30010r9'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--cpus-per-task=8</span> </span><span class='cmitt-10x-x-80'>     </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Request 8 cores</span> 
</span><span class='label'><a id='x1-30011r10'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--mem=32G</span> </span><span class='cmitt-10x-x-80'>             </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Assign 32G memory per node</span> 
</span><span class='label'><a id='x1-30012r11'></a></span> 
<span class='label'><a id='x1-30013r12'></a></span><span style='color:#000000'><span class='cmtt-8'>cd</span></span><span style='color:#000000'> <span class='tctt-0800'>$</span><span class='cmtt-8'>TMPDIR</span> 
</span><span class='label'><a id='x1-30014r13'></a></span><span style='color:#000000'><span class='cmtt-8'>mkdir</span></span><span style='color:#000000'> <span class='cmtt-8'>input</span> 
</span><span class='label'><a id='x1-30015r14'></a></span><span style='color:#000000'><span class='cmtt-8'>rsync</span></span><span style='color:#000000'> <span class='cmtt-8'>-av </span><span class='tctt-0800'>$</span><span class='cmtt-8'>SLURM_SUBMIT_DIR/references/ input/</span> 
</span><span class='label'><a id='x1-30016r15'></a></span><span style='color:#000000'><span class='cmtt-8'>mkdir</span></span><span style='color:#000000'> <span class='cmtt-8'>results</span> 
</span><span class='label'><a id='x1-30017r16'></a></span><span style='color:#000000'><span class='cmtt-8'>srun</span></span><span style='color:#000000'> <span class='cmtt-8'>STAR --inFiles </span><span class='tctt-0800'>$</span><span class='cmtt-8'>TMPDIR/input --parallel </span><span class='tctt-0800'>$</span><span class='cmtt-8'>SRUN_CPUS_PER_TASK --outFiles </span><span class='tctt-0800'>$</span><span class='cmtt-8'>TMPDIR/results</span> 
</span><span class='label'><a id='x1-30018r17'></a></span><span style='color:#000000'><span class='cmtt-8'>rsync</span></span><span style='color:#000000'> <span class='cmtt-8'>-av </span><span class='tctt-0800'>$</span><span class='cmtt-8'>TMPDIR/results/ </span><span class='tctt-0800'>$</span><span class='cmtt-8'>SLURM_SUBMIT_DIR/processed/</span></span>
</pre>
<figcaption class='caption'><span class='id'>Figure 9: </span><span class='content'>Source code for <a class='url' href='tmpdir.sh'><span class='cmtt-10'>tmpdir.sh</span></a></span></figcaption><!-- tex4ht:label?: x1-300199  -->
                                                                               

                                                                               
   </figure>
   <h4 class='subsectionHead' id='ssh-keys-for-mpi'><span class='titlemark'>2.10   </span> <a id='x1-310002.10'></a>SSH Keys For MPI</h4>
<!-- l. 504 --><p class='noindent'>Some programs effect their parallel processing via MPI (which is a communication protocol). An
example of such software is Fluent. MPI needs to have ‘passwordless login’ set up, which means SSH
keys. In your NFS-mounted home directory:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='cmtt-10'>cd .ssh</span>
     </li>
     <li class='itemize'><span class='cmtt-10'>ssh-keygen -t ed25519 </span>(default location; blank passphrase)
     </li>
     <li class='itemize'><span class='cmtt-10'>cat id_ed25519.pub &gt;&gt; authorized_keys </span>(if the <a href='https://www.ssh.com/academy/ssh/authorized-keys-file'><span class='cmtt-10'>authorized_keys</span></a> file already exists)
     <span class='cmti-10'>OR </span><span class='cmtt-10'>cat id_ed25519.pub &gt; authorized_keys </span>(if does not)
     </li>
     <li class='itemize'>Set  file  permissions  of  <span class='cmtt-10'>authorized_keys </span>to  600;  of  your  NFS-mounted  home  to  700
     (note that you likely will not have to do anything here, as most people will have those
     permissions by default).</li></ul>
<!-- l. 524 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='creating-virtual-environments'><span class='titlemark'>2.11   </span> <a id='x1-320002.11'></a>Creating Virtual Environments</h4>
<!-- l. 528 --><p class='noindent'>The following documentation is specific to the <span class='cmbx-10'>Speed </span>HPC Facility at the Gina Cody School of
Engineering and Computer Science. Virtual environments typically instantiated via Conda or Python.
Another option is Singularity detailed in Section <a href='#singularity-containers'>2.16<!-- tex4ht:ref: sect:singularity-containers  --></a>. Usually, virtual environments are created once
during an interactive session before submitting a batch job to the scheduler. The job script submitted
to the scheduler is then written to (1) activate the virtual environment, (2) use it, and (3) close it at
the end of the job.
</p><!-- l. 539 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='anaconda'><span class='titlemark'>2.11.1   </span> <a id='x1-330002.11.1'></a>Anaconda</h5>
<!-- l. 542 --><p class='noindent'>Request an interactive session in the queue you wish to submit your jobs to (e.g., salloc -p pg
–gpus=1 for GPU jobs). Once your interactive has started, create an anaconda environment in your
speed-scratch directory by using the <span class='cmtt-10'>prefix </span>option when executing <span class='cmtt-10'>conda create</span>. For example,
                                                                               

                                                                               
to create an anaconda environment for <span class='cmtt-10'>a_user</span>, execute the following at the command
line:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-39'>
module load anaconda3/2023.03/default
conda create --prefix /speed-scratch/a_user/myconda
</pre>
<!-- l. 550 --><p class='nopar'>
</p><!-- l. 553 --><p class='noindent'><span class='cmbx-10'>Note: </span>Without the <span class='cmtt-10'>prefix </span>option, the <span class='cmtt-10'>conda create </span>command creates the environment in <span class='cmtt-10'>a_user</span>’s
home directory by default.
</p>
<!-- l. 559 --><p class='noindent'><span class='paragraphHead' id='list-environments'><a id='x1-34000'></a><span class='cmbx-10'>List Environments.</span></span>
   To view your conda environments, type: <span class='cmtt-10'>conda info --envs</span>
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-40'>
# conda environments:
#
base                  *  /encs/pkg/anaconda3-2023.03/root
                         /speed-scratch/a_user/myconda
</pre>
<!-- l. 568 --><p class='nopar'>
</p>
<!-- l. 571 --><p class='noindent'><span class='paragraphHead' id='activate-an-environment'><a id='x1-35000'></a><span class='cmbx-10'>Activate an Environment.</span></span>
   Activate the environment <span class='cmtt-10'>speedscratcha_usermyconda </span>as follows
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-41'>
conda activate /speed-scratch/a_user/myconda
</pre>
<!-- l. 576 --><p class='nopar'> After activating your environment, add <span class='cmtt-10'>pip </span>to your environment by using
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-42'>
conda install pip
</pre>
<!-- l. 580 --><p class='nopar'> This will install <span class='cmtt-10'>pip </span>and <span class='cmtt-10'>pip</span>’s dependencies, including python, into the environment.
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 586 --><p class='noindent'>A consolidated example using Conda:
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-43'>
     salloc -p pg --gpus=1 --mem=10GB -A &lt;slurm account name&gt;
     cd /speed-scratch/$USER
     module load python/3.11.0/default
     conda create -p /speed-scratch/$USER/pytorch-env
     conda activate /speed-scratch/$USER/pytorch-env
     conda install python=3.11.0
     pip3 install torch torchvision torchaudio --index-url \
       https://download.pytorch.org/whl/cu117
     ....
     conda deactivate
     exit # end the salloc session
</pre>
     <!-- l. 600 --><p class='nopar'>
     </p></li>
     <li class='itemize'>No Space left error: <span class='cmtt-10'>R</span>ead our Github <a href='https://github.com/NAG-DevOps/speed-hpc/tree/master/src#no-space-left-error-when-creating-conda-environment'><span class='cmtt-10'>HERE</span></a></li></ul>
<!-- l. 610 --><p class='noindent'><span class='cmbx-10'>Important Note: </span><span class='cmtt-10'>pip </span>(and <span class='cmtt-10'>pip3</span>) are used to install modules from the python distribution while
<span class='cmtt-10'>conda install </span>installs modules from anaconda’s repository.
</p><!-- l. 616 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='python'><span class='titlemark'>2.11.2   </span> <a id='x1-360002.11.2'></a>Python</h5>
<!-- l. 619 --><p class='noindent'>Setting up a Python virtual environment is fairly straightforward. The first step is to request an
interactive session in the queue you wish to submit your jobs to.
</p><!-- l. 622 --><p class='indent'>   We have a simple example that use a Python virtual environment:
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 626 --><p class='noindent'>Using Python Venv
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-44'>
     salloc -p pg --gpus=1 --mem=10GB -A &lt;slurm account name&gt;
     cd /speed-scratch/$USER
     module load python/3.9.1/default
     mkdir -p /speed-scratch/$USER/tmp
     setenv TMPDIR /speed-scratch/$USER/tmp
     setenv TMP /speed-scratch/$USER/tmp
     python -m venv $TMPDIR/testenv (testenv=name of the virtualEnv)
     source /speed-scratch/$USER/tmp/testenv/bin/activate.csh
     pip install modules…
     deactivate
     exit
</pre>
     <!-- l. 640 --><p class='nopar'>
     </p></li>
     <li class='itemize'>See, e.g., <a href='https://github.com/NAG-DevOps/speed-hpc/blob/master/src/gurobi-with-python.sh'><span class='cmtt-10'>gurobi-with-python.sh</span></a></li></ul>
<!-- l. 649 --><p class='noindent'><span class='cmbx-10'>Important Note: </span>partition <span class='cmtt-10'>ps </span>is used for CPU jobs, partitions <span class='cmtt-10'>pg</span>, <span class='cmtt-10'>pt </span>are used for GPU jobs, no
need to use <span class='cmtt-10'>--gpus= </span>when preparing environments for CPU jobs.
</p><!-- l. 2 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='example-job-script-fluent'><span class='titlemark'>2.12   </span> <a id='x1-370002.12'></a>Example Job Script: Fluent</h4>
   <figure class='figure' id='-source-code-for-fluentsh'> 

                                                                               

                                                                               
<a id='x1-3703510'></a>
                                                                               

                                                                               
<!-- l. 5 --><pre class='lstinputlisting' id='listing-3'><span class='label'><a id='x1-37002r1'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>!/</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>encs</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>/</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>bin</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>/</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>tcsh</span></span> 
<span class='label'><a id='x1-37003r2'></a></span> 
<span class='label'><a id='x1-37004r3'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--job-name=flu10000</span> </span><span class='cmitt-10x-x-80'>   </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Give the job a name</span> 
</span><span class='label'><a id='x1-37005r4'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--mail-type=ALL</span> </span><span class='cmitt-10x-x-80'>       </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Receive all email type notifications</span> 
</span><span class='label'><a id='x1-37006r5'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--mail-user=YOUR_USER_NAME@encs.concordia.ca</span> 
</span><span class='label'><a id='x1-37007r6'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--chdir=./</span> </span><span class='cmitt-10x-x-80'>            </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Use currect directory as working directory</span> 
</span><span class='label'><a id='x1-37008r7'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--nodes=1</span> </span><span class='cmitt-10x-x-80'>             </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Number of nodes to run on</span> 
</span><span class='label'><a id='x1-37009r8'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--ntasks-per-node=32</span> </span><span class='cmitt-10x-x-80'>  </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Number of cores</span> 
</span><span class='label'><a id='x1-37010r9'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--cpus-per-task=1</span> </span><span class='cmitt-10x-x-80'>     </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Number of MPI threads</span> 
</span><span class='label'><a id='x1-37011r10'></a></span><span style='color:#000000'><span class='cmitt-10x-x-80'>#</span></span><span style='color:#000000'><span class='cmitt-10x-x-80'>SBATCH</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>--mem=160G</span> </span><span class='cmitt-10x-x-80'>            </span><span style='color:#000000'><span class='cmitt-10x-x-80'>##</span></span><span style='color:#000000'> <span class='cmitt-10x-x-80'>Assign 160G memory per node</span> 
</span><span class='label'><a id='x1-37012r11'></a></span> 
<span class='label'><a id='x1-37013r12'></a></span><span style='color:#000000'><span class='cmtt-8'>date</span></span> 
<span class='label'><a id='x1-37014r13'></a></span> 
<span class='label'><a id='x1-37015r14'></a></span><span style='color:#000000'><span class='cmtt-8'>module</span></span><span style='color:#000000'> <span class='cmtt-8'>avail ansys</span> 
</span><span class='label'><a id='x1-37016r15'></a></span> 
<span class='label'><a id='x1-37017r16'></a></span><span style='color:#000000'><span class='cmtt-8'>module</span></span><span style='color:#000000'> <span class='cmtt-8'>load ansys/19.2/default</span> 
</span><span class='label'><a id='x1-37018r17'></a></span><span style='color:#000000'><span class='cmtt-8'>cd</span></span><span style='color:#000000'> <span class='tctt-0800'>$</span><span class='cmtt-8'>TMPDIR</span> 
</span><span class='label'><a id='x1-37019r18'></a></span> 
<span class='label'><a id='x1-37020r19'></a></span><span style='color:#000000'><span class='cmtt-8'>set</span></span><span style='color:#000000'> <span class='cmtt-8'>FLUENTNODES = "‘scontrol␣show␣hostnames‘"</span> 
</span><span class='label'><a id='x1-37021r20'></a></span><span style='color:#000000'><span class='cmtt-8'>set</span></span><span style='color:#000000'> <span class='cmtt-8'>FLUENTNODES = ‘echo </span><span class='tctt-0800'>$</span><span class='cmtt-8'>FLUENTNODES | tr ’ ’ ’,’‘</span> 
</span><span class='label'><a id='x1-37022r21'></a></span> 
<span class='label'><a id='x1-37023r22'></a></span><span style='color:#000000'><span class='cmtt-8'>date</span></span> 
<span class='label'><a id='x1-37024r23'></a></span> 
<span class='label'><a id='x1-37025r24'></a></span><span style='color:#000000'><span class='cmtt-8'>srun</span></span><span style='color:#000000'> <span class='cmtt-8'>fluent 3ddp \</span> 
</span><span class='label'><a id='x1-37026r25'></a></span><span class='cmtt-8'>        </span><span style='color:#000000'><span class='cmtt-8'>-</span></span><span style='color:#000000'><span class='cmtt-8'>g</span></span><span style='color:#000000'> <span class='cmtt-8'>-t</span><span class='tctt-0800'>$</span><span class='cmtt-8'>SLURM_NTASKS \</span> 
</span><span class='label'><a id='x1-37027r26'></a></span><span class='cmtt-8'>        </span><span style='color:#000000'><span class='cmtt-8'>-</span></span><span style='color:#000000'><span class='cmtt-8'>g</span></span><span style='color:#000000'><span class='cmtt-8'>-</span></span><span style='color:#000000'><span class='cmtt-8'>cnf</span></span><span style='color:#000000'><span class='cmtt-8'>=</span></span><span style='color:#000000'><span class='tctt-0800'>$</span><span class='cmtt-8'>FLUENTNODES</span></span><span style='color:#000000'> <span class='cmtt-8'>\</span> 
</span><span class='label'><a id='x1-37028r27'></a></span><span class='cmtt-8'>        </span><span style='color:#000000'><span class='cmtt-8'>-</span></span><span style='color:#000000'><span class='cmtt-8'>i</span></span><span style='color:#000000'> <span class='tctt-0800'>$</span><span class='cmtt-8'>SLURM_SUBMIT_DIR/fluentdata/info.jou &gt; call.txt</span> 
</span><span class='label'><a id='x1-37029r28'></a></span> 
<span class='label'><a id='x1-37030r29'></a></span><span style='color:#000000'><span class='cmtt-8'>date</span></span> 
<span class='label'><a id='x1-37031r30'></a></span> 
<span class='label'><a id='x1-37032r31'></a></span><span style='color:#000000'><span class='cmtt-8'>srun</span></span><span style='color:#000000'> <span class='cmtt-8'>rsync -av </span><span class='tctt-0800'>$</span><span class='cmtt-8'>TMPDIR/ </span><span class='tctt-0800'>$</span><span class='cmtt-8'>SLURM_SUBMIT_DIR/fluentparallel/</span> 
</span><span class='label'><a id='x1-37033r32'></a></span> 
<span class='label'><a id='x1-37034r33'></a></span><span style='color:#000000'><span class='cmtt-8'>date</span></span>
</pre>
<figcaption class='caption'><span class='id'>Figure 10: </span><span class='content'>Source code for <a class='url' href='fluent.sh'><span class='cmtt-10'>fluent.sh</span></a></span></figcaption><!-- tex4ht:label?: x1-3703510  -->
                                                                               

                                                                               
   </figure>
<!-- l. 10 --><p class='indent'>   The job script in Figure <a href='#-source-code-for-fluentsh'>10<!-- tex4ht:ref: fig:fluent.sh  --></a> runs Fluent in parallel over 32 cores. Of note, we have requested
e-mail notifications (<span class='cmtt-10'>--mail-type</span>), are defining the parallel environment for, <span class='cmtt-10'>fluent</span>, with,
<span class='cmtt-10'>-t</span><span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_NTASKS </span>and <span class='cmtt-10'>-g-cnf=</span><span class='tctt-1000'>$</span><span class='cmtt-10'>FLUENTNODES </span>(<span class='cmbx-10'>very important</span>), and are setting <span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR </span>as
the in-job location for the “moment” <a class='url' href='rfile.out'><span class='cmtt-10'>rfile.out</span></a> file (in-job, because the last line of the
script copies everything from <span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR </span>to a directory in the user’s NFS-mounted home).
Job progress can be monitored by examining the standard-out file (e.g., <span class='cmtt-10'>slurm-249.out</span>),
and/or by examining the “moment” file in <span class='cmtt-10'>/disk/nobackup/&lt;yourjob&gt; </span>(hint: it starts
with your job-ID) on the node running the job. <span class='cmbx-10'>Caveat: </span>take care with journal-file file
paths.
</p>
   <h4 class='subsectionHead' id='example-job-efficientdet'><span class='titlemark'>2.13   </span> <a id='x1-380002.13'></a>Example Job: efficientdet</h4>
<!-- l. 27 --><p class='noindent'>The following steps describing how to create an efficientdet environment on <span class='cmti-10'>Speed</span>, were submitted by
a member of Dr. Amer’s research group.
</p>
     <ul class='itemize1'>
     <li class='itemize'>Enter your ENCS user account’s speed-scratch directory<br class='newline' /><span class='obeylines-h'><span class='verb'><span class='cmtt-10'>cd /speed-scratch/&lt;encs_username&gt;</span></span></span>
     </li>
     <li class='itemize'>
     <!-- l. 35 --><p class='noindent'>Next </p>
         <ul class='itemize2'>
         <li class='itemize'>load python <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>module load python/3.8.3</span></span></span>
         </li>
         <li class='itemize'>create virtual environment <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>python3 -m venv &lt;env_name&gt;</span></span></span>
         </li>
         <li class='itemize'>activate virtual environment <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>source &lt;env_name&gt;/bin/activate.csh</span></span></span>
         </li>
         <li class='itemize'>install DL packages for Efficientdet</li></ul>
     </li></ul>
                                                                               

                                                                               
<pre class='verbatim' id='verbatim-45'>
pip install tensorflow==2.7.0
pip install lxml&gt;=4.6.1
pip install absl-py&gt;=0.10.0
pip install matplotlib&gt;=3.0.3
pip install numpy&gt;=1.19.4
pip install Pillow&gt;=6.0.0
pip install PyYAML&gt;=5.1
pip install six&gt;=1.15.0
pip install tensorflow-addons&gt;=0.12
pip install tensorflow-hub&gt;=0.11
pip install neural-structured-learning&gt;=1.3.1
pip install tensorflow-model-optimization&gt;=0.5
pip install Cython&gt;=0.29.13
pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI
</pre>
<!-- l. 63 --><p class='nopar'>
</p><!-- l. 67 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='java-jobs'><span class='titlemark'>2.14   </span> <a id='x1-390002.14'></a>Java Jobs</h4>
<!-- l. 70 --><p class='noindent'>Jobs that call <span class='cmtt-10'>java </span>have a memory overhead, which needs to be taken into account when assigning a
value to <span class='cmtt-10'>--mem</span>. Even the most basic <span class='cmtt-10'>java </span>call, <span class='cmtt-10'>java -Xmx1G -version</span>, will need to have,
<span class='cmtt-10'>--mem=5G</span>, with the 4-GB difference representing the memory overhead. Note that this memory
overhead grows proportionally with the value of <span class='cmtt-10'>-Xmx</span>. To give you an idea, when <span class='cmtt-10'>-Xmx </span>has a
value of 100G, <span class='cmtt-10'>--mem </span>has to be at least 106G; for 200G, at least 211G; for 300G, at least
314G.
</p><!-- l. 84 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='scheduling-on-the-gpu-nodes'><span class='titlemark'>2.15   </span> <a id='x1-400002.15'></a>Scheduling On The GPU Nodes</h4>
<!-- l. 86 --><p class='noindent'>The primary cluster has two GPU nodes, each with six Tesla (CUDA-compatible) P6 cards: each card
has 2048 cores and 16GB of RAM. Though note that the P6 is mainly a single-precision card, so
unless you need the GPU double precision, double-precision calculations will be faster on a CPU
node.
</p><!-- l. 91 --><p class='indent'>   Job scripts for the GPU queue differ in that they need this statement, which attaches either a
single GPU, or, two GPUs, to the job:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-46'>
#SBATCH --gpus=[1|2]
</pre>
<!-- l. 109 --><p class='nopar'>
</p><!-- l. 117 --><p class='indent'>   Once that your job script is ready, you can submit it to the GPU partition (queue)
with:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-47'>
sbatch -p pg ./&lt;myscript&gt;.sh
</pre>
<!-- l. 125 --><p class='nopar'>
</p><!-- l. 127 --><p class='indent'>   And you can query <span class='cmtt-10'>nvidia-smi </span>on the node that is running your job with:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-48'>
ssh &lt;username&gt;@speed[-05|-17|37-43] nvidia-smi
</pre>
<!-- l. 131 --><p class='nopar'>
</p><!-- l. 133 --><p class='indent'>   Status of the GPU queue can be queried with:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-49'>
sinfo -p pg --long --Node
</pre>
<!-- l. 140 --><p class='nopar'>
</p><!-- l. 142 --><p class='noindent'><span class='cmbx-10'>Very important note </span>regarding TensorFlow and PyTorch: if you are planning to run TensorFlow
and/or PyTorch multi-GPU jobs, <span class='cmbx-10'>do not </span>use the <span class='cmtt-10'>tf.distribute </span>and/or<br class='newline' /><span class='cmtt-10'>torch.nn.DataParallel </span>functions on <span class='cmbx-10'>speed-01,05,17</span>, as they will crash the compute node (100%
certainty). This appears to be the current hardware’s architecture’s defect. The workaround is to
either manually effect GPU parallelisation (TensorFlow has an example on how to do this), or to run
on a single GPU.
</p><!-- l. 156 --><p class='noindent'><span class='cmbx-10'>Important</span>
</p><!-- l. 161 --><p class='indent'>   Users without permission to use the GPU nodes can submit jobs to the <span class='cmtt-10'>pg </span>partition, but those
jobs will hang and never run. Their availability is seen with:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-50'>
[serguei@speed-submit src] % sinfo -p pg --long --Node
Thu Oct 19 22:31:04 2023
NODELIST   NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON
speed-05       1        pg        idle 32     2:16:1 515490        0      1    gpu16 none
speed-17       1        pg     drained 32     2:16:1 515490        0      1    gpu16 UGE
speed-25       1        pg        idle 32     2:16:1 257458        0      1    gpu32 none
speed-27       1        pg        idle 32     2:16:1 257458        0      1    gpu32 none
[serguei@speed-submit src] % sinfo -p pt --long --Node
Thu Oct 19 22:32:39 2023
NODELIST   NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON
speed-37       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-38       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-39       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-40       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-41       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-42       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-43       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
</pre>
<!-- l. 209 --><p class='nopar'>
</p><!-- l. 211 --><p class='indent'>   <span class='cmr-7'>This status demonstrates that most are available (i.e., have not been requested as resources). To specifically request a
GPU node, add, </span><span class='cmtt-8x-x-87'>--gpus=[#GPUs]</span><span class='cmr-7'>, to your </span><span class='cmtt-8x-x-87'>sbatch </span><span class='cmr-7'>(statement/script) or </span><span class='cmtt-8x-x-87'>salloc </span><span class='cmr-7'>(statement) request. For example,
</span><span class='cmtt-8x-x-87'>sbatch -t 10 --mem=1G --gpus=1 -p pg ./tcsh.sh</span><span class='cmr-7'>. You will see that this job has been assigned to one of the GPU
nodes.</span>
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-51'>
[serguei@speed-submit src] % squeue -p pg -o "%15N %.6D %7P %.11T %.4c %.8z %.6m %.8d %.6w %.8f %20G %20E"
NODELIST         NODES PARTITI       STATE MIN_    S:C:T MIN_ME MIN_TMP_  WCKEY FEATURES GROUP DEPENDENCY
speed-05             1 pg          RUNNING    1    *:*:*     1G        0 (null)   (null) 11929     (null)
[serguei@speed-submit src] % sinfo -p pg -o "%15N %.6D %7P %.11T %.4c %.8z %.6m %.8d %.6w %.8f %20G %20E"
NODELIST         NODES PARTITI       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE GRES      REASON
speed-17             1 pg          drained   32   2:16:1 515490        0      1    gpu16 gpu:6        UGE
speed-05             1 pg            mixed   32   2:16:1 515490        0      1    gpu16 gpu:6       none
speed-[25,27]        2 pg             idle   32   2:16:1 257458        0      1    gpu32 gpu:2       none
</pre>
<!-- l. 261 --><p class='nopar'>
</p><!-- l. 268 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='p-on-multigpu-multinode'><span class='titlemark'>2.15.1   </span> <a id='x1-410002.15.1'></a>P6 on Multi-GPU, Multi-Node</h5>
<!-- l. 270 --><p class='noindent'>As described lines above, P6 cards are not compatible with Distribute and DataParallel functions
(<span class='cmtt-10'>Pytorch, Tensorflow</span>) when running on Multi-GPUs. One workaround is to run the job in
Multi-node, single GPU per node; per example:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-52'>
#SBATCH --nodes=2
#SBATCH --gpus-per-node=1
</pre>
<!-- l. 277 --><p class='nopar'>
</p><!-- l. 279 --><p class='indent'>   On P6 nodes: <span class='cmtt-10'>speed-05, speed-17, speed-01</span>
</p><!-- l. 281 --><p class='indent'>   The example: <a href='https://github.com/NAG-DevOps/speed-hpc/blob/master/src/pytorch-multinode-multigpu.sh'>pytorch-multinode-multigpu.sh</a> illustrates a job for training on Multi-nodes,
Multi-GPUs
</p><!-- l. 287 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='cuda'><span class='titlemark'>2.15.2   </span> <a id='x1-420002.15.2'></a>CUDA</h5>
<!-- l. 289 --><p class='noindent'>When calling <span class='cmtt-10'>CUDA </span>within job scripts, it is important to create a link to the desired <span class='cmtt-10'>CUDA </span>libraries and
set the runtime link path to the same libraries. For example, to use the <span class='cmtt-10'>cuda-11.5 </span>libraries, specify
the following in your <span class='cmtt-10'>Makefile</span>.
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-53'>
-L/encs/pkg/cuda-11.5/root/lib64 -Wl,-rpath,/encs/pkg/cuda-11.5/root/lib64
</pre>
<!-- l. 296 --><p class='nopar'>
</p><!-- l. 298 --><p class='indent'>   In your job script, specify the version of <span class='cmtt-10'>gcc </span>to use prior to calling cuda. For example: <span class='cmtt-10'>module
load gcc/8.4 </span>or <span class='cmtt-10'>module load gcc/9.3</span>
</p><!-- l. 305 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='special-notes-for-sending-cuda-jobs-to-the-gpu-queue'><span class='titlemark'>2.15.3   </span> <a id='x1-430002.15.3'></a>Special Notes for sending CUDA jobs to the GPU Queue</h5>
<!-- l. 310 --><p class='noindent'>Interactive jobs (Section <a href='#interactive-jobs'>2.8<!-- tex4ht:ref: sect:interactive-jobs  --></a>) must be submitted to the <span class='cmbx-10'>GPU partition </span>in order to compile and link.
We have several versions of CUDA installed in:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-54'>
/encs/pkg/cuda-11.5/root/
/encs/pkg/cuda-10.2/root/
/encs/pkg/cuda-9.2/root
</pre>
<!-- l. 319 --><p class='nopar'>
</p><!-- l. 321 --><p class='indent'>   For CUDA to compile properly for the GPU partition, edit your <span class='cmtt-10'>Makefile </span>replacing
<span class='cmtt-10'>usrlocalcuda </span>with one of the above.
</p><!-- l. 325 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='openiss-examples'><span class='titlemark'>2.15.4   </span> <a id='x1-440002.15.4'></a>OpenISS Examples</h5>
<!-- l. 328 --><p class='noindent'>These represent more comprehensive research-like examples of jobs for computer vision and other
tasks with a lot longer runtime (a subject to the number of epochs and other parameters) derive from
the actual research works of students and their theses. These jobs require the use of CUDA
and GPUs. These examples are available as “native” jobs on Speed and as Singularity
containers.
</p>
<!-- l. 339 --><p class='noindent'><span class='paragraphHead' id='openiss-and-reid'><a id='x1-45000'></a><span class='cmbx-10'>OpenISS and REID</span></span>
<a id='x1-45000doc'></a>
   The example <a href='https://github.com/NAG-DevOps/speed-hpc/blob/master/src/openiss-reid-speed.sh'>openiss-reid-speed.sh</a> illustrates a job for a computer-vision based person
re-identification (e.g., motion capture-based tracking for stage performance) part of the OpenISS
project by Haotao Lai <span class='cite'>[<a href='#Xlai-haotao-mcthesis19'>10</a>]</span> using TensorFlow and Keras. The fork of the original repo <span class='cite'>[<a href='#Xopeniss-reid-tfk'>12</a>]</span> adjusted to
to run on Speed is here:
</p>
     <ul class='itemize1'>
     <li class='itemize'><a class='url' href='https://github.com/NAG-DevOps/openiss-reid-tfk'><span class='cmtt-10'>https://github.com/NAG-DevOps/openiss-reid-tfk</span></a></li></ul>
<!-- l. 354 --><p class='indent'>   and its detailed description on how to run it on Speed is in the README:
</p>
     <ul class='itemize1'>
     <li class='itemize'><a class='url' href='https://github.com/NAG-DevOps/speed-hpc/tree/master/src#openiss-reid-tfk'><span class='cmtt-10'>https://github.com/NAG-DevOps/speed-hpc/tree/master/src#openiss-reid-tfk</span></a></li></ul>
                                                                               

                                                                               
<!-- l. 362 --><p class='noindent'><span class='paragraphHead' id='openiss-and-yolov'><a id='x1-46000'></a><span class='cmbx-10'>OpenISS and YOLOv3</span></span>
<a id='x1-46000doc'></a>
   The related code using YOLOv3 framework is in the the fork of the original repo <span class='cite'>[<a href='#Xopeniss-yolov3'>11</a>]</span> adjusted to
to run on Speed is here:
</p>
     <ul class='itemize1'>
     <li class='itemize'><a class='url' href='https://github.com/NAG-DevOps/openiss-yolov3'><span class='cmtt-10'>https://github.com/NAG-DevOps/openiss-yolov3</span></a></li></ul>
<!-- l. 373 --><p class='indent'>   Its example job scripts can run on both CPUs and GPUs, as well as interactively using
TensorFlow:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Interactive mode: <a href='https://github.com/NAG-DevOps/speed-hpc/blob/master/src/openiss-yolo-interactive.sh'>openiss-yolo-interactive.sh</a>
     </li>
     <li class='itemize'>CPU-based job: <a href='https://github.com/NAG-DevOps/speed-hpc/blob/master/src/openiss-yolo-cpu.sh'>openiss-yolo-cpu.sh</a>
     </li>
     <li class='itemize'>GPU-based jon: <a href='https://github.com/NAG-DevOps/speed-hpc/blob/master/src/openiss-yolo-gpu.sh'>openiss-yolo-gpu.sh</a></li></ul>
<!-- l. 388 --><p class='indent'>   The detailed description on how to run these on Speed is in the README at:
</p>
     <ul class='itemize1'>
     <li class='itemize'><a class='url' href='https://github.com/NAG-DevOps/speed-hpc/tree/master/src#openiss-yolov3'><span class='cmtt-10'>https://github.com/NAG-DevOps/speed-hpc/tree/master/src#openiss-yolov3</span></a></li></ul>
<!-- l. 396 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='singularity-containers'><span class='titlemark'>2.16   </span> <a id='x1-470002.16'></a>Singularity Containers</h4>
<!-- l. 399 --><p class='noindent'>If the <span class='cmtt-10'>/encs </span>software tree does not have a required software instantaneously available, another option
is to run Singularity containers. We run EL7 flavor of Linux, and if some projects require Ubuntu or
other distributions, there is a possibility to run that software as a container, including the ones
translated from Docker.
</p><!-- l. 406 --><p class='indent'>   The example <a href='https://github.com/NAG-DevOps/speed-hpc/blob/master/src/lambdal-singularity.sh'>lambdal-singularity.sh</a> showcases an immediate use of a container built for the
Ubuntu-based LambdaLabs software stack, originally built as a Docker image then pulled
in as a Singularity container that is immediately available for use as that job example
illustrates. The source material used for the docker image was our fork of their official repo:
<a class='url' href='https://github.com/NAG-DevOps/lambda-stack-dockerfiles'><span class='cmtt-10'>https://github.com/NAG-DevOps/lambda-stack-dockerfiles</span></a>
                                                                               

                                                                               
</p><!-- l. 416 --><p class='indent'>   NOTE: It is important if you make your own containers or pull from DockerHub, use your
<span class='obeylines-h'><span class='verb'><span class='cmtt-10'>/speed-scratch/$USER</span></span></span> directory as these images may easily consume gigs of space in your home
directory and you’d run out of quota there very fast.
</p><!-- l. 422 --><p class='indent'>   TIP: To check for your quota, and the corresponding commands to find big files, see:
<a class='url' href='https://www.concordia.ca/ginacody/aits/encs-data-storage.html'><span class='cmtt-10'>https://www.concordia.ca/ginacody/aits/encs-data-storage.html</span></a>
</p><!-- l. 425 --><p class='indent'>   We likewise built equivalent OpenISS (Section <a href='#openiss-examples'>2.15.4<!-- tex4ht:ref: sect:openiss-examples  --></a>) containers from their Docker
counter parts as they were used for teaching and research <span class='cite'>[<a href='#Xoi-containers-poster-siggraph2023'>14</a>]</span>. The images from
<a class='url' href='https://github.com/NAG-DevOps/openiss-dockerfiles'><span class='cmtt-10'>https://github.com/NAG-DevOps/openiss-dockerfiles</span></a> and their DockerHub equivalents
<a class='url' href='https://hub.docker.com/u/openiss'><span class='cmtt-10'>https://hub.docker.com/u/openiss</span></a> are found in the same public directory on
<span class='obeylines-h'><span class='verb'><span class='cmtt-10'>/speed-scratch/nag-public</span></span></span> as the LambdaLabs Singularity image. They all have <span class='cmtt-10'>.sif </span>extension.
Some of them can be ran in both batch or interactive mode, some make more sense to
run interactively. They cover some basics with CUDA, OpenGL rendering, and computer
vision tasks as examples from the OpenISS library and other libraries, including the base
images that use different distros. We also include Jupyter notebook example with Conda
support.
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-55'>
/speed-scratch/nag-public:

openiss-cuda-conda-jupyter.sif
openiss-cuda-devicequery.sif
openiss-opengl-base.sif
openiss-opengl-cubes.sif
openiss-opengl-triangle.sif
openiss-reid.sif
openiss-xeyes.sif
</pre>
<!-- l. 448 --><p class='nopar'>
</p><!-- l. 450 --><p class='indent'>   The currently recommended version of Singularity is <span class='cmtt-10'>singularity/3.10.4/default</span>.
</p><!-- l. 453 --><p class='indent'>   This section comprises an introduction to working with Singularity, its containers, and what can
and cannot be done with Singularity on the ENCS infrastructure. It is not intended to be an
exhaustive presentation of Singularity: the program’s authors do a good job of that here:
<a class='url' href='https://www.sylabs.io/docs/'><span class='cmtt-10'>https://www.sylabs.io/docs/</span></a>. It also assumes that you have successfully installed Singularity on a
user-managed/personal system (see next paragraph as to why).
</p><!-- l. 460 --><p class='indent'>   Singularity containers are essentially either built from an existing container, or are built from
scratch. Building from scratch requires a recipe file (think of like a Dockerfile), and the operation <span class='cmti-10'>must</span>
be effected as root. You will not have root on the ENCS infrastructure, so any built-from-scratch
containers must be created on a user-managed/personal system. Root-level permissions are also
required (in some cases, essential; in others, for proper build functionality) for building from an
existing container. Three types of Singularity containers can be built: file-system; sandbox; squashfs.
The first two are “writable” (meaning that changes can persist after the Singularity session ends).
File-system containers are built around the ext3 file system, and are a read-write “file”, sandbox
containers are essentially a directory in an existing read-write space, and squashfs containers are
a read-only compressed “file”. Note that file-system containers <span class='cmti-10'>cannot </span>be resized once
built.
</p><!-- l. 478 --><p class='indent'>   Note that the default build is a squashfs one. Also note what Singularity’s authors have to say
about the builds, “A common workflow is to use the “sandbox” mode for development of
the container, and then build it as a default (squashfs) Singularity image when done.”
File-system containers are considered to be, “legacy”, at this point in time. When built, a
<span class='cmti-10'>very small </span>overhead is allotted to a file-system container (think, MB), and that <span class='cmti-10'>cannot </span>be
changed.
</p><!-- l. 486 --><p class='indent'>   Probably for the most of your workflows you might find there is a Docker container exists for your
tasks, in this case you can use the docker pull function of Singularity as a part of you virtual
environment setup as an interactive job allocation:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-56'>
salloc --gpus=1 -n8 --mem=4Gb -t60
cd /speed-scratch/$USER/
singularity pull openiss-cuda-devicequery.sif docker://openiss/openiss-cuda-devicequery
INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
</pre>
<!-- l. 499 --><p class='nopar'>
</p><!-- l. 502 --><p class='noindent'>This method can be used for converting Docker containers directly on Speed. On GPU nodes make
sure to pass on the <span class='cmtt-10'>--nv </span>flag to Singularity, so its containers could access the GPUs. See the linked
example.
</p><!-- l. 658 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='conclusion'><span class='titlemark'>3   </span> <a id='x1-480003'></a>Conclusion</h3>
<!-- l. 661 --><p class='noindent'>The cluster is, “first come, first served”, until it fills, and then job position in the queue is
based upon past usage. The scheduler does attempt to fill gaps, though, so sometimes a
single-core job of lower priority will schedule before a multi-core job of higher priority, for
example.
</p><!-- l. 667 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='important-limitations'><span class='titlemark'>3.1   </span> <a id='x1-490003.1'></a>Important Limitations</h4>
     <ul class='itemize1'>
     <li class='itemize'>New users are restricted to a total of 32 cores: write to <a class='url' href='rt-ex-hpc@encs.concordia.ca'><span class='cmtt-10'>rt-ex-hpc@encs.concordia.ca</span></a>
     if you need more temporarily (192 is the maximum, or, 6 jobs of 32 cores each).
     </li>
     <li class='itemize'>Batch job sessions are a maximum of one week in length (only 24 hours, though, for
     interactive jobs, see Section <a href='#interactive-jobs'>2.8<!-- tex4ht:ref: sect:interactive-jobs  --></a>).
     </li>
     <li class='itemize'>
     <!-- l. 680 --><p class='noindent'>Scripts can live in your NFS-provided home, but any substantial data need to be in your
     cluster-specific directory (located at <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>/speed-scratch/&lt;ENCSusername&gt;/</span></span></span>).
     </p><!-- l. 684 --><p class='noindent'>NFS is great for acute activity, but is not ideal for chronic activity. Any data that a job will
     read more than once should be copied at the start to the scratch disk of a compute node
     using <span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR </span>(and, perhaps, <span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_SUBMIT_DIR</span>), any intermediary job data should be
     produced in <span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR</span>, and once a job is near to finishing, those data should be copied
                                                                               

                                                                               
     to your NFS-mounted home (or other NFS-mounted space) from <span class='tctt-1000'>$</span><span class='cmtt-10'>TMPDIR </span>(to, perhaps,
     <span class='tctt-1000'>$</span><span class='cmtt-10'>SLURM_SUBMIT_DIR</span>). In other words, IO-intensive operations should be effected locally
     whenever possible, saving network activity for the start and end of jobs.
     </p></li>
     <li class='itemize'>Your current resource allocation is based upon past usage, which is an amalgamation of
     approximately one week’s worth of past wallclock (i.e., time spent on the node(s)) and
     compute activity (on the node(s)).
     </li>
     <li class='itemize'>Jobs should NEVER be run outside of the province of the scheduler. Repeat offenders
     risk loss of cluster access.
</li></ul>
<!-- l. 2 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='tipstricks'><span class='titlemark'>3.2   </span> <a id='x1-500003.2'></a>Tips/Tricks</h4>
     <ul class='itemize1'>
     <li class='itemize'>Files/scripts must have Linux line breaks in them (not Windows ones). Use <span class='cmtt-10'>file </span>command
     to verify; and <span class='cmtt-10'>dos2unix </span>command to convert.
     </li>
     <li class='itemize'>Use <span class='cmtt-10'>rsync</span>, not <span class='cmtt-10'>scp</span>, when copying or moving large amounts of data.
     </li>
     <li class='itemize'>Before moving a large amount of files between NFS-mounted storage and the cluster, <span class='cmtt-10'>tar</span>
     up the files you plan to move first.
     </li>
     <li class='itemize'>If you intend to use a different shell (e.g., <span class='cmtt-10'>bash</span> <span class='cite'>[<a href='#Xaosa-book-vol1-bash'>22</a>]</span>), you will need to change the shell
     declaration in your script(s).
     </li>
     <li class='itemize'><span class='cmbx-10'>Try to request resources that closely match what your job will use: requesting
     many more cores or much more memory than will be needed makes a job
     more difficult to schedule when resources are scarce.</span>
                                                                               

                                                                               
     </li>
     <li class='itemize'>E-mail, <span class='cmtt-10'>rt-ex-hpc AT encs.concordia.ca</span>, with any concerns/questions.</li></ul>
<!-- l. 711 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='use-cases'><span class='titlemark'>3.3   </span> <a id='x1-510003.3'></a>Use Cases</h4>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 716 --><p class='noindent'>HPC Committee’s initial batch about 6 students (end of 2019): </p>
         <ul class='itemize2'>
         <li class='itemize'>10000 iterations job in Fluent finished in \(&lt;26\) hours vs. 46 hours in Calcul Quebec</li></ul>
     </li>
     <li class='itemize'>
     <!-- l. 722 --><p class='noindent'>NAG’s MAC spoofer analyzer <span class='cite'>[<a href='#Xmac-spoofer-analyzer-intro-c3s2e2014'>18</a>, <a href='#Xmac-spoofer-analyzer-detail-fps2014'>17</a>]</span>, such as <a class='url' href='https://github.com/smokhov/atsm/tree/master/examples/flucid'><span class='cmtt-10'>https://github.com/smokhov/atsm/tree/master/examples/flucid</span></a>
     </p>
         <ul class='itemize2'>
         <li class='itemize'>compilation of forensic computing reasoning cases about false or true positives of
         hardware address spoofing in the labs</li></ul>
     </li>
     <li class='itemize'>
     <!-- l. 729 --><p class='noindent'>S4 LAB/GIPSY R&amp;D Group’s: </p>
         <ul class='itemize2'>
         <li class='itemize'>MARFCAT and MARFPCAT (OSS signal processing and machine learning tools for
         vulnerable and weak code analysis and network packet capture analysis) <span class='cite'>[<a href='#Xmarfcat-nlp-ai2014'>20</a>, <a href='#Xmarfcat-sate2010-nist'>15</a>, <a href='#Xfingerprinting-mal-traffic'>6</a>]</span>
         </li>
         <li class='itemize'>Web service data conversion and analysis
         </li>
         <li class='itemize'>Forensic Lucid encoders (translation of large log data into Forensic Lucid <span class='cite'>[<a href='#Xmokhov-phd-thesis-2013'>16</a>]</span> for
         forensic analysis)
         </li>
         <li class='itemize'>Genomic alignment exercises</li></ul>
                                                                               

                                                                               
     </li>
     <li class='itemize'>Serguei Mokhov, Jonathan Llewellyn, Carlos Alarcon Meza, Tariq Daradkeh, and Gillian Roper.
     The use of containers in OpenGL, ML and HPC for teaching and research support. In
     <span class='cmti-10'>ACM SIGGRAPH 2023 Posters</span>, SIGGRAPH ’23, New York, NY, USA, 2023. ACM.
     <a class='url' href='https://doi.org/10.1145/3588028.3603676'><span class='cmtt-10'>https://doi.org/10.1145/3588028.3603676</span></a>
     </li>
     <li class='itemize'>Goutam Yelluru Gopal and Maria Amer. Separable self and mixed attention transformers for
     efficient object tracking. In <span class='cmti-10'>IEEE/CVF Winter Conference on Applications of Computer Vision
     (WACV)</span>, Waikoloa, Hawaii, January 2024. <a class='url' href='https://arxiv.org/abs/2309.03979'><span class='cmtt-10'>https://arxiv.org/abs/2309.03979</span></a> and
     <a class='url' href='https://github.com/goutamyg/SMAT'><span class='cmtt-10'>https://github.com/goutamyg/SMAT</span></a>
     </li>
     <li class='itemize'>Goutam Yelluru Gopal and Maria Amer. Mobile vision transformer-based visual object
     tracking. In <span class='cmti-10'>34th British Machine Vision Conference (BMVC)</span>, Aberdeen, UK, November 2023.
     <a class='url' href='https://arxiv.org/abs/2309.05829'><span class='cmtt-10'>https://arxiv.org/abs/2309.05829</span></a> and <a class='url' href='https://github.com/goutamyg/MVT'><span class='cmtt-10'>https://github.com/goutamyg/MVT</span></a>
     </li>
     <li class='itemize'>Belkacem Belabes and Marius Paraschivoiu. CFD modeling of vertical-axis wind turbine wake
     interaction. <span class='cmti-10'>Transactions of the Canadian Society for Mechanical Engineering</span>, pages 1–10, 2023.
     <a class='url' href='https://doi.org/10.1139/tcsme-2022-0149'><span class='cmtt-10'>https://doi.org/10.1139/tcsme-2022-0149</span></a>
     </li>
     <li class='itemize'>Belkacem Belabes and Marius Paraschivoiu. CFD study of the aerodynamic performance of a
     vertical axis wind turbine in the wake of another turbine. In <span class='cmti-10'>Proceedings of the CSME
     International Congress</span>, 2022. <a class='url' href='https://doi.org/10.7939/r3-rker-1746'><span class='cmtt-10'>https://doi.org/10.7939/r3-rker-1746</span></a>
     </li>
     <li class='itemize'>Belkacem Belabes and Marius Paraschivoiu. Numerical study of the effect of turbulence intensity on
     VAWT performance. <span class='cmti-10'>Energy</span>, 233:121139, 2021. <a class='url' href='https://doi.org/10.1016/j.energy.2021.121139'><span class='cmtt-10'>https://doi.org/10.1016/j.energy.2021.121139</span></a>
     </li>
     <li class='itemize'>Parna Niksirat, Adriana Daca, and Krzysztof Skonieczny. The effects of reduced-gravity on
     planetary rover mobility. <span class='cmti-10'>International Journal of Robotics Research</span>, 39(7):797–811, 2020.
     <a class='url' href='https://doi.org/10.1177/0278364920913945'><span class='cmtt-10'>https://doi.org/10.1177/0278364920913945</span></a>
     </li>
     <li class='itemize'>
     <!-- l. 758 --><p class='noindent'>The work “Haotao Lai. An OpenISS framework specialization for deep learning-based
     person re-identification.  Master’s thesis, Department of Computer Science and
     Software Engineering, Concordia University, Montreal, Canada, August 2019.
     <a class='url' href='https://spectrum.library.concordia.ca/id/eprint/985788/'><span class='cmtt-10'>https://spectrum.library.concordia.ca/id/eprint/985788/</span></a>” using TensorFlow and Keras
     on OpenISS adjusted to run on Speed based on the repositories: </p>
                                                                               

                                                                               
         <ul class='itemize2'>
         <li class='itemize'>Haotao  Lai  et al.    Openiss  person  re-identification  baseline  v0.1.1,  June  2021.
         <a class='url' href='https://github.com/OpenISS/openiss-reid-tfk'><span class='cmtt-10'>https://github.com/OpenISS/openiss-reid-tfk</span></a> and
         </li>
         <li class='itemize'>Haotao     Lai     et al.             OpenISS     keras-yolo3     v0.1.0,     June     2021.
         <a class='url' href='https://github.com/OpenISS/openiss-yolov3'><span class='cmtt-10'>https://github.com/OpenISS/openiss-yolov3</span></a></li></ul>
     <!-- l. 766 --><p class='noindent'>and theirs forks by the team.
</p>
     </li></ul>
<!-- l. 774 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='history'><span class='titlemark'>A   </span> <a id='x1-52000A'></a>History</h3>
<!-- l. 777 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='acknowledgments'><span class='titlemark'>A.1   </span> <a id='x1-53000A.1'></a>Acknowledgments</h4>
     <ul class='itemize1'>
     <li class='itemize'>The first 6 (to 6.5) versions of this manual and early UGE job script samples, Singularity
     testing and user support were produced/done by Dr. Scott Bunnell during his time at
     Concordia as a part of the NAG/HPC group. We thank him for his contributions.
     </li>
     <li class='itemize'>The HTML version with devcontainer support was contributed by Anh H Nguyen.
     </li>
     <li class='itemize'>Dr. Tariq  Daradkeh,  was  our  IT  Instructional  Specialist  August  2022  to  September
     2023; working on the scheduler, scheduling research, end user support, and integration
     of  examples,  such  as  YOLOv3  in  Section <a href='#x1-46000doc'>2.15.4.0<!-- tex4ht:ref: sect:openiss-yolov3  --></a>  other  tasks.  We  have  a  continued
     collaboration on HPC/scheduling research.</li></ul>
                                                                               

                                                                               
<!-- l. 796 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='migration-from-uge-to-slurm'><span class='titlemark'>A.2   </span> <a id='x1-54000A.2'></a>Migration from UGE to SLURM</h4>
<!-- l. 799 --><p class='noindent'>For long term users who started off with Grid Engine here are some resources to make a transition
and mapping to the job submission process.
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 804 --><p class='noindent'>Queues are called “partitions” in SLURM. Our mapping from the GE queues to SLURM
     partitions is as follows:
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-57'>
     GE  =&gt; SLURM
     s.q    ps
     g.q    pg
     a.q    pa
</pre>
     <!-- l. 811 --><p class='nopar'> We also have a new partition <span class='cmtt-10'>pt </span>that covers SPEED2 nodes, which previously did not
     exist.
     </p></li>
     <li class='itemize'>
     <!-- l. 816 --><p class='noindent'>Commands and command options mappings are found in Figure <a href='#-rosetta-mappings-of-scheduler-commands-from-schedmd'>11<!-- tex4ht:ref: fig:rosetta-mappings  --></a> from<br class='newline' /><a class='url' href='https://slurm.schedmd.com/rosetta.pdf'><span class='cmtt-10'>https://slurm.schedmd.com/rosetta.pdf</span></a><br class='newline' /><a class='url' href='https://slurm.schedmd.com/pdfs/summary.pdf'><span class='cmtt-10'>https://slurm.schedmd.com/pdfs/summary.pdf</span></a><br class='newline' />Other related helpful resources from similar organizations who either used SLURM for awhile or
     also transitioned to it:<br class='newline' /><a class='url' href='https://docs.alliancecan.ca/wiki/Running_jobs'><span class='cmtt-9'>https://docs.alliancecan.ca/wiki/Running_jobs</span></a><br class='newline' /><a class='url' href='https://www.depts.ttu.edu/hpcc/userguides/general_guides/Conversion_Table_1.pdf'><span class='cmtt-9'>https://www.depts.ttu.edu/hpcc/userguides/general_guides/Conversion_Table_1.pdf</span></a><br class='newline' /><a class='url' href='https://docs.mpcdf.mpg.de/doc/computing/clusters/aux/migration-from-sge-to-slurm'><span class='cmtt-9'>https://docs.mpcdf.mpg.de/doc/computing/clusters/aux/migration-from-sge-to-slurm</span></a>
</p>
     <figure class='figure' id='-rosetta-mappings-of-scheduler-commands-from-schedmd'> 
<a id='x1-5400111'></a> <img alt='PIC' height='412' src='images/rosetta-mapping.png' width='412' />
<figcaption class='caption'><span class='id'>Figure 11: </span><span class='content'>Rosetta Mappings of Scheduler Commands from SchedMD</span></figcaption><!-- tex4ht:label?: x1-5400111  -->
     </figure>
     </li>
     <li class='itemize'>
     <!-- l. 834 --><p class='noindent'><span class='cmbx-10'>NOTE: </span>If you have used UGE commands in the past you probably still have these lines there;
     <span class='cmbx-10'>they should now be removed</span>, as they have no use in SLURM and will start giving
     “command not found” errors on login when the software is removed:
     </p><!-- l. 839 --><p class='noindent'>csh/<span class='cmtt-10'>tcsh</span>: Sample <a class='url' href='.tcshrc'><span class='cmtt-10'>.tcshrc</span></a> file:
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-58'>
     # Speed environment set up
     if ($HOSTNAME == speed-submit.encs.concordia.ca) then
        source /local/pkg/uge-8.6.3/root/default/common/settings.csh
     endif
</pre>
     <!-- l. 846 --><p class='nopar'>
     </p><!-- l. 848 --><p class='noindent'>Bourne shell/<span class='cmtt-10'>bash</span>: Sample <a class='url' href='.bashrc'><span class='cmtt-10'>.bashrc</span></a> file:
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-59'>
     # Speed environment set up
     if [ $HOSTNAME = "speed-submit.encs.concordia.ca" ]; then
         . /local/pkg/uge-8.6.3/root/default/common/settings.sh
         printenv ORGANIZATION | grep -qw ENCS || . /encs/Share/bash/profile
     fi
</pre>
     <!-- l. 856 --><p class='nopar'>
     </p><!-- l. 858 --><p class='noindent'>Note that you will need to either log out and back in, or execute a new shell, for the
     environment changes in the updated <a class='url' href='.tcshrc'><span class='cmtt-10'>.tcshrc</span></a> or <a class='url' href='.bashrc'><span class='cmtt-10'>.bashrc</span></a> file to be applied (<span class='cmbx-10'>important</span>).
</p>
     </li></ul>
<!-- l. 866 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='phases'><span class='titlemark'>A.3   </span> <a id='x1-55000A.3'></a>Phases</h4>
<!-- l. 869 --><p class='noindent'>Brief summary of Speed evolution phases.
</p><!-- l. 872 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='phase-'><span class='titlemark'>A.3.1   </span> <a id='x1-56000A.3.1'></a>Phase 4</h5>
<!-- l. 874 --><p class='noindent'>Phase 4 had 7 SuperMicro servers with 4x A100 80GB GPUs each added, dubbed as “SPEED2”. We
also moved from Grid Engine to SLURM.
</p><!-- l. 878 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='phase-1'><span class='titlemark'>A.3.2   </span> <a id='x1-57000A.3.2'></a>Phase 3</h5>
<!-- l. 880 --><p class='noindent'>Phase 3 had 4 vidpro nodes added from Dr. Amer totalling 6x P6 and 6x V100 GPUs
added.
</p><!-- l. 884 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='phase-2'><span class='titlemark'>A.3.3   </span> <a id='x1-58000A.3.3'></a>Phase 2</h5>
<!-- l. 886 --><p class='noindent'>Phase 2 saw 6x NVIDIA Tesla P6 added and 8x more compute nodes. The P6s replaced 4x of FirePro
S7150.
                                                                               

                                                                               
</p><!-- l. 890 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='phase-3'><span class='titlemark'>A.3.4   </span> <a id='x1-59000A.3.4'></a>Phase 1</h5>
<!-- l. 892 --><p class='noindent'>Phase 1 of Speed was of the following configuration:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Sixteen,  32-core  nodes,  each  with  512 GB  of  memory  and  approximately  1 TB  of
     volatile-scratch disk space.
     </li>
     <li class='itemize'>Five AMD FirePro S7150 GPUs, with 8 GB of memory (compatible with the Direct X,
     OpenGL, OpenCL, and Vulkan APIs).</li></ul>
<!-- l. 2 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='frequently-asked-questions'><span class='titlemark'>B   </span> <a id='x1-60000B'></a>Frequently Asked Questions</h3>
<!-- l. 6 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='where-do-i-learn-about-linux'><span class='titlemark'>B.1   </span> <a id='x1-61000B.1'></a>Where do I learn about Linux?</h4>
<!-- l. 9 --><p class='noindent'>All Speed users are expected to have a basic understanding of Linux and its commonly used
commands.
</p><!-- l. 12 --><p class='noindent'>
</p>
   <h5 class='likesubsubsectionHead' id='software-carpentry'><a id='x1-62000'></a>Software Carpentry</h5>
<!-- l. 14 --><p class='noindent'>Software Carpentry provides free resources to learn software, including a workshop on the Unix shell.
<a class='url' href='https://software-carpentry.org/lessons/'><span class='cmtt-10'>https://software-carpentry.org/lessons/</span></a>
</p><!-- l. 18 --><p class='noindent'>
</p>
   <h5 class='likesubsubsectionHead' id='udemy'><a id='x1-63000'></a>Udemy</h5>
<!-- l. 20 --><p class='noindent'>There are a number of Udemy courses, including free ones, that will assist you in learning Linux.
Active Concordia faculty, staff and students have access to Udemy courses. The course <span class='cmbx-10'>Linux
Mastery: Master the Linux Command Line in 11.5 Hours </span>is a good starting point for
beginners. Visit <a class='url' href='https://www.concordia.ca/it/services/udemy.html'><span class='cmtt-10'>https://www.concordia.ca/it/services/udemy.html</span></a> to learn how Concordians
may access Udemy.
                                                                               

                                                                               
</p><!-- l. 28 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='how-to-use-the-bash-shell-on-speed'><span class='titlemark'>B.2   </span> <a id='x1-64000B.2'></a>How to use the “bash shell” on Speed?</h4>
<!-- l. 30 --><p class='noindent'>This section describes how to use the “bash shell” on Speed. Review Section <a href='#environment-set-up'>2.1.2<!-- tex4ht:ref: sect:envsetup  --></a> to ensure that your
bash environment is set up.
</p><!-- l. 34 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='how-do-i-set-bash-as-my-login-shell'><span class='titlemark'>B.2.1   </span> <a id='x1-65000B.2.1'></a>How do I set bash as my login shell?</h5>
<!-- l. 36 --><p class='noindent'>In order to set your default login shell to bash on Speed, your login shell on all GCS servers must be
changed to bash. To make this change, create a ticket with the Service Desk (or email <span class='cmtt-10'>help at
concordia.ca</span>) to request that bash become your default login shell for your ENCS user account on
all GCS servers.
</p><!-- l. 41 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='how-do-i-move-into-a-bash-shell-on-speed'><span class='titlemark'>B.2.2   </span> <a id='x1-66000B.2.2'></a>How do I move into a bash shell on Speed?</h5>
<!-- l. 43 --><p class='noindent'>To move to the bash shell, type <span class='cmbx-10'>bash </span>at the command prompt. For example:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-60'>
[speed-submit] [/home/a/a_user] &gt; bash
bash-4.4$ echo $0
bash
</pre>
<!-- l. 49 --><p class='nopar'>
</p><!-- l. 51 --><p class='indent'>   <span class='cmbx-10'>Note </span>how the command prompt changed from <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>[speed-submit] [/home/a/a_user] &gt;</span></span></span> to
<span class='obeylines-h'><span class='verb'><span class='cmtt-10'>bash-4.4$</span></span></span> after entering the bash shell.
</p><!-- l. 54 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='how-do-i-use-the-bash-shell-in-an-interactive-session-on-speed'><span class='titlemark'>B.2.3   </span> <a id='x1-67000B.2.3'></a>How do I use the bash shell in an interactive session on Speed?</h5>
<!-- l. 56 --><p class='noindent'>Below are examples of how to use <span class='cmtt-10'>bash </span>as a shell in your interactive job sessions with both the salloc
and srun commands.
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='cmtt-10'>salloc -ppt --mem=100G -N 1 -n 10 /encs/bin/bash</span>
     </li>
     <li class='itemize'><span class='cmtt-10'>srun --mem=50G -n 5 --pty /encs/bin/bash</span></li></ul>
<!-- l. 64 --><p class='indent'>   <span class='cmbx-10'>Note: </span>Make sure the interactive job requests memory, cores, etc.
</p>
   <h5 class='subsubsectionHead' id='how-do-i-run-scripts-written-in-bash-on-speed'><span class='titlemark'>B.2.4   </span> <a id='x1-68000B.2.4'></a>How do I run scripts written in bash on Speed?</h5>
<!-- l. 68 --><p class='noindent'>To execute bash scripts on Speed:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-68002x1'>Ensure that the shebang of your bash job script is <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>#!/encs/bin/bash</span></span></span>
     </li>
<li class='enumerate' id='x1-68004x2'>Use the <span class='cmtt-10'>sbatch </span>command to submit your job script to the scheduler.</li></ol>
<!-- l. 76 --><p class='indent'>   The Speed GitHub contains a sample <a href='https://github.com/NAG-DevOps/speed-hpc/blob/master/src/bash.sh'>bash job script</a>.
                                                                               

                                                                               
</p><!-- l. 82 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='how-to-resolve-disk-quota-exceeded-errors'><span class='titlemark'>B.3   </span> <a id='x1-69000B.3'></a>How to resolve “Disk quota exceeded” errors?</h4>
<!-- l. 85 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='probable-cause'><span class='titlemark'>B.3.1   </span> <a id='x1-70000B.3.1'></a>Probable Cause</h5>
<!-- l. 87 --><p class='noindent'>The “<span class='cmtt-10'>Disk quota exceeded</span>” Error occurs when your application has run out of disk space to write
to. On Speed this error can be returned when:
</p><!-- l. 90 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-70002x1'>Your NFS-provided home is full and cannot be written to. You can verify this using <span class='cmtt-10'>quota</span>
     and <span class='cmtt-10'>bigfiles </span>commands.
     </li>
<li class='enumerate' id='x1-70004x2'>The <span class='cmtt-10'>/tmp </span>directory on the speed node your application is running on is full and cannot
     be written to.</li></ol>
<!-- l. 99 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='possible-solutions'><span class='titlemark'>B.3.2   </span> <a id='x1-71000B.3.2'></a>Possible Solutions</h5>
<!-- l. 101 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-71002x1'>Use the <span class='cmtt-10'>--chdir </span>job script option to set the directory that the job script is submitted
     from the <span class='cmtt-10'>job working directory</span>. The <span class='cmtt-10'>job working directory </span>is the directory that
     the job will write output files in.
     </li>
<li class='enumerate' id='x1-71004x2'>
     <!-- l. 107 --><p class='noindent'>The use local disk space is generally recommended for IO intensive operations. However, as the
     size of <span class='cmtt-10'>/tmp </span>on speed nodes is <span class='cmtt-10'>1TB </span>it can be necessary for scripts to store temporary data
     elsewhere. Review the documentation for each module called within your script to determine
     how to set working directories for that application. The basic steps for this solution are:
     </p>
                                                                               

                                                                               
         <ul class='itemize1'>
         <li class='itemize'>Review the documentation on how to set working directories for each module called
         by the job script.
         </li>
         <li class='itemize'>
         <!-- l. 119 --><p class='noindent'>Create a working directory in speed-scratch for output files. For example, this
         command will create a subdirectory called <span class='cmbx-10'>output </span>in your <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>speed-scratch</span></span></span>
         directory:
                                                                               

                                                                               
</p>
         <pre class='verbatim' id='verbatim-61'>
         mkdir -m 750 /speed-scratch/$USER/output
          
</pre>
         <!-- l. 124 --><p class='nopar'>
         </p></li>
         <li class='itemize'>
         <!-- l. 126 --><p class='noindent'>To create a subdirectory for recovery files:
                                                                               

                                                                               
</p>
         <pre class='verbatim' id='verbatim-62'>
         mkdir -m 750 /speed-scratch/$USER/recovery
</pre>
         <!-- l. 129 --><p class='nopar'>
         </p></li>
         <li class='itemize'>Update the job script to write output to the subdirectories you created in your
         <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>speed-scratch</span></span></span> directory, e.g., <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>/speed-scratch/$USER/output</span></span></span>.</li></ul>
     </li></ol>
<!-- l. 135 --><p class='noindent'>In the above example, <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>$USER</span></span></span> is an environment variable containing your ENCS username.
</p><!-- l. 138 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='example-of-setting-working-directories-for-comsol'><span class='titlemark'>B.3.3   </span> <a id='x1-72000B.3.3'></a>Example of setting working directories for <span class='cmtt-10'>COMSOL</span></h5>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 142 --><p class='noindent'>Create directories for recovery, temporary, and configuration files. For example, to create these
     directories for your GCS ENCS user account:
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-63'>
     mkdir -m 750 -p /speed-scratch/$USER/comsol/{recovery,tmp,config}
</pre>
     <!-- l. 146 --><p class='nopar'>
     </p></li>
     <li class='itemize'>
     <!-- l. 148 --><p class='noindent'>Add the following command switches to the COMSOL command to use the directories created
     above:
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-64'>
     -recoverydir /speed-scratch/$USER/comsol/recovery
     -tmpdir /speed-scratch/$USER/comsol/tmp
     -configuration/speed-scratch/$USER/comsol/config
</pre>
     <!-- l. 154 --><p class='nopar'></p></li></ul>
<!-- l. 157 --><p class='noindent'>In the above example, <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>$USER</span></span></span> is an environment variable containing your ENCS username.
</p><!-- l. 160 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='example-of-setting-working-directories-for-python-modules'><span class='titlemark'>B.3.4   </span> <a id='x1-73000B.3.4'></a>Example of setting working directories for <span class='cmtt-10'>Python Modules</span></h5>
<!-- l. 162 --><p class='noindent'>By default when adding a python module the <span class='cmtt-10'>/tmp </span>directory is set as the temporary repository for
files downloads. The size of the <span class='cmtt-10'>/tmp </span>directory on <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>speed-submit</span></span></span> is too small for pytorch. To add a
python module </p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 167 --><p class='noindent'>Create your own tmp directory in your <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>speed-scratch</span></span></span> directory
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-65'>
       mkdir /speed-scratch/$USER/tmp
</pre>
     <!-- l. 170 --><p class='nopar'>
     </p></li>
     <li class='itemize'>
     <!-- l. 172 --><p class='noindent'>Use the tmp directory you created
                                                                               

                                                                               
</p>
     <pre class='verbatim' id='verbatim-66'>
       setenv TMPDIR /speed-scratch/$USER/tmp
</pre>
     <!-- l. 175 --><p class='nopar'>
     </p></li>
     <li class='itemize'>Attempt the installation of pytorch</li></ul>
<!-- l. 180 --><p class='indent'>   In the above example, <span class='obeylines-h'><span class='verb'><span class='cmtt-10'>$USER</span></span></span> is an environment variable containing your ENCS username.
</p><!-- l. 183 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='how-do-i-check-my-jobs-status'><span class='titlemark'>B.4   </span> <a id='x1-74000B.4'></a>How do I check my job’s status?</h4>
<!-- l. 188 --><p class='noindent'>When a job with a job id of 1234 is running or terminated, the status of that job can be tracked using
‘<span class='obeylines-h'><span class='verb'><span class='cmtt-10'>sacct -j 1234</span></span></span>’. <span class='cmtt-10'>squeue -j 1234 </span>can show while the job is sitting in the queue as well. Long term
statistics on the job after its terminated can be found using <span class='cmtt-10'>sstat -j 1234 </span>after <span class='cmtt-10'>slurmctld </span>purges it
its tracking state into the database.
</p><!-- l. 195 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='why-is-my-job-pending-when-nodes-are-empty'><span class='titlemark'>B.5   </span> <a id='x1-75000B.5'></a>Why is my job pending when nodes are empty?</h4>
<!-- l. 198 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='disabled-nodes'><span class='titlemark'>B.5.1   </span> <a id='x1-76000B.5.1'></a>Disabled nodes</h5>
<!-- l. 200 --><p class='noindent'>It is possible that one or a number of the Speed nodes are disabled. Nodes are disabled if they require
maintenance. To verify if Speed nodes are disabled, see if they are in a draining or drained
state:
                                                                               

                                                                               
</p>
   <pre class='verbatim' id='verbatim-67'>
[serguei@speed-submit src] % sinfo --long --Node
Thu Oct 19 21:25:12 2023
NODELIST   NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON
speed-01       1        pa        idle 32     2:16:1 257458        0      1    gpu16 none
speed-03       1        pa        idle 32     2:16:1 257458        0      1    gpu32 none
speed-05       1        pg        idle 32     2:16:1 515490        0      1    gpu16 none
speed-07       1       ps*       mixed 32     2:16:1 515490        0      1    cpu32 none
speed-08       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-09       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-10       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-11       1       ps*        idle 32     2:16:1 515490        0      1    cpu32 none
speed-12       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-15       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-16       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-17       1        pg     drained 32     2:16:1 515490        0      1    gpu16 UGE
speed-19       1       ps*        idle 32     2:16:1 515490        0      1    cpu32 none
speed-20       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-21       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-22       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-23       1       ps*        idle 32     2:16:1 515490        0      1    cpu32 none
speed-24       1       ps*        idle 32     2:16:1 515490        0      1    cpu32 none
speed-25       1        pg        idle 32     2:16:1 257458        0      1    gpu32 none
speed-25       1        pa        idle 32     2:16:1 257458        0      1    gpu32 none
speed-27       1        pg        idle 32     2:16:1 257458        0      1    gpu32 none
speed-27       1        pa        idle 32     2:16:1 257458        0      1    gpu32 none
speed-29       1       ps*        idle 32     2:16:1 515490        0      1    cpu32 none
speed-30       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-31       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-32       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-33       1       ps*        idle 32     2:16:1 515490        0      1    cpu32 none
speed-34       1       ps*        idle 32     2:16:1 515490        0      1    cpu32 none
speed-35       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-36       1       ps*     drained 32     2:16:1 515490        0      1    cpu32 UGE
speed-37       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-38       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-39       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-40       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-41       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-42       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
speed-43       1        pt        idle 256    2:64:2 980275        0      1 gpu20,mi none
</pre>
<!-- l. 264 --><p class='nopar'>
</p><!-- l. 267 --><p class='indent'>   Note which nodes are in the state of <span class='cmbx-10'>drained</span>. Why the state is drained can be found in the reason
column.
</p><!-- l. 269 --><p class='indent'>   Your job will run once an occupied node becomes availble or the maintenance has been completed
and the disabled nodes have a state of <span class='cmbx-10'>idle</span>.
</p><!-- l. 272 --><p class='noindent'>
</p>
   <h5 class='subsubsectionHead' id='error-in-job-submit-request'><span class='titlemark'>B.5.2   </span> <a id='x1-77000B.5.2'></a>Error in job submit request.</h5>
<!-- l. 274 --><p class='noindent'>It is possible that your job is pending, because the job requested resources that are not available
within Speed. To verify why job id 1234 is not running, execute ‘<span class='obeylines-h'><span class='verb'><span class='cmtt-10'>sacct -j 1234</span></span></span>’. A summary of the
reasons is available via the <span class='cmtt-10'>squeue </span>command.
</p><!-- l. 906 --><p class='noindent'>
</p>
                                                                               

                                                                               
   <h3 class='sectionHead' id='sister-facilities'><span class='titlemark'>C   </span> <a id='x1-78000C'></a>Sister Facilities</h3>
<!-- l. 908 --><p class='noindent'>Below is a list of resources and facilities similar to Speed at various capacities. Depending on your
research group and needs, they might be available to you. They are not managed by HPC/NAG of
AITS, so contact their respective representatives.
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='cmtt-10'>computation.encs </span>CPU only 3-machine cluster running longer jobs without a scheduler
     at the moment
     </li>
     <li class='itemize'><span class='cmtt-10'>apini.encs </span>cluster for teaching and MPI programming (see the corresponding course in
     CSSE)
     </li>
     <li class='itemize'>Computer  Science  and  Software  Engineering  (CSSE)  Virya  GPU  Cluster.  For  CSSE
     members only. The cluster has 4 nodes with total of 32 NVIDIA GPUs (a mix of V100s
     and A100s). To request access send email to <span class='cmtt-10'>virya.help AT concordia.ca</span>.
     </li>
     <li class='itemize'>Dr. Maria Amer’s VidPro group’s nodes in Speed (-01, -03, -25, -27) with additional V100
     and P6 GPUs.
     </li>
     <li class='itemize'>
     <!-- l. 926 --><p class='noindent'>There are various Lambda Labs other GPU servers and like computers acquired by individual
     researchers; if you are member of their research group, contact them directly. These resources
     are not managed by us. </p>
         <ul class='itemize2'>
         <li class='itemize'>Dr. Amin Hammad’s <span class='cmtt-10'>construction.encs </span>Lambda Labs station
         </li>
         <li class='itemize'>Dr. Hassan Rivaz’s <span class='cmtt-10'>impactlab.encs </span>Lambda Labs station
         </li>
         <li class='itemize'>Dr. Nizar Bouguila’s <span class='cmtt-10'>xailab.encs </span>Lambda Labs station
         </li>
         <li class='itemize'>Dr. Roch Glitho’s <span class='cmtt-10'>femto.encs </span>server
                                                                               

                                                                               
         </li>
         <li class='itemize'>Dr. Maria Amer’s <span class='cmtt-10'>venom.encs </span>Lambda Labs station
         </li>
         <li class='itemize'>Dr. Leon Wang’s <span class='cmtt-10'>guerrera.encs </span>DGX station</li></ul>
     </li>
     <li class='itemize'>Dr. Ivan Contreras’ servers (managed by AITS)
     </li>
     <li class='itemize'>If you are a member of School of Health (formerly PERFORM Center), you may have access to
     their local <a href='https://perform-wiki.concordia.ca/mediawiki/index.php/HPC_Cluster'>PERFORM’s High Performance Computing (HPC) Cluster</a>. Contact Thomas
     Beaudry for details and how to obtain access.
     </li>
     <li class='itemize'>All Concordia students have access to the Library’s small <a href='https://library.concordia.ca/technology/sandbox/'>Technology Sandbox</a> testing cluster
     that also runs Slurm. Email <span class='cmtt-10'>sean.cooney AT concordia.ca </span>for details.
     </li>
     <li class='itemize'>Digital Research Alliance Canada (Compute Canada / Calcul Quebec),<br class='newline' /><a class='url' href='https://alliancecan.ca/'><span class='cmtt-10'>https://alliancecan.ca/</span></a>. Follow <a href='https://alliancecan.ca/en/services/advanced-research-computing/account-management/apply-account'>this link</a> on the information how to obtain access (students
     need to be sponsored by their supervising faculty members, who should create accounts first).
     Their SLURM examples are here: <a class='url' href='https://docs.alliancecan.ca/wiki/Running_jobs'><span class='cmtt-10'>https://docs.alliancecan.ca/wiki/Running_jobs</span></a>
</li></ul>
                                                                               

                                                                               
<p id='annotated-bibliography'><a id='Q1-1-93'></a>
   </p><h3 class='likesectionHead' id='references'><a id='x1-79000'></a><span class='cmr-9'>References</span></h3>
<!-- l. 1 --><p class='noindent'>
    </p><div class='thebibliography'>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[1]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xabaqus'></a><span class='cmr-9'>3DS.                                  Abaqus.                                  [online],            2019–2021.</span>
    <a class='url' href='https://www.3ds.com/products-services/simulia/products/abaqus/'><span class='cmtt-9'>https://www.3ds.com/products-services/simulia/products/abaqus/</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[2]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xfluent'></a><span class='cmr-9'>ANSYS.                               FLUENT.                               [online],           2000–2012.</span>
    <a class='url' href='http://www.ansys.com/Products/Simulation+Technology/Fluid+Dynamics/ANSYS+FLUENT'><span class='cmtt-9'>http://www.ansys.com/Products/Simulation+Technology/Fluid+Dynamics/ANSYS+FLUENT</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[3]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xnumerical-turbulence-vawt-2021'></a><span class='cmr-9'>Belkacem                 Belabes                 and                 Marius                 Paraschivoiu.
    Numerical study of the effect of turbulence intensity on VAWT performance. </span><span class='cmti-9'>Energy</span><span class='cmr-9'>, 233:121139,
    2021. </span><a class='url' href='https://doi.org/10.1016/j.energy.2021.121139'><span class='cmtt-9'>https://doi.org/10.1016/j.energy.2021.121139</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[4]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xcfd-vaxis-turbine-wake-2022'></a><span class='cmr-9'>Belkacem Belabes and Marius Paraschivoiu. CFD study of the aerodynamic performance of a
    vertical axis wind turbine in the wake of another turbine. In </span><span class='cmti-9'>Proceedings of the CSME International
    Congress</span><span class='cmr-9'>, 2022. </span><a class='url' href='https://doi.org/10.7939/r3-rker-1746'><span class='cmtt-9'>https://doi.org/10.7939/r3-rker-1746</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[5]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xcfd-modeling-turbine-2023'></a><span class='cmr-9'>Belkacem Belabes and Marius Paraschivoiu. CFD modeling of vertical-axis wind turbine wake
    interaction. </span><span class='cmti-9'>Transactions of the Canadian Society for Mechanical Engineering</span><span class='cmr-9'>, pages 1–10, 2023.</span>
    <a class='url' href='https://doi.org/10.1139/tcsme-2022-0149'><span class='cmtt-9'>https://doi.org/10.1139/tcsme-2022-0149</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[6]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xfingerprinting-mal-traffic'></a><span class='cmr-9'>Amine  Boukhtouta,  Nour-Eddine  Lakhdari,  Serguei A.  Mokhov,  and  Mourad  Debbabi.
    Towards fingerprinting malicious traffic.  In </span><span class='cmti-9'>Proceedings of ANT’13</span><span class='cmr-9'>, volume 19, pages 548–555.
    Elsevier, June 2013.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[7]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xaosa-book-vol1'></a><span class='cmr-9'>Amy  Brown  and  Greg  Wilson,  editors.    </span><span class='cmti-9'>The  Architecture  of  Open  Source  Applications:
    Elegance, Evolution, and a Few Fearless Hacks</span><span class='cmr-9'>, volume I.  aosabook.org, March 2012.  Online at</span>
    <a class='url' href='http://aosabook.org'><span class='cmtt-9'>http://aosabook.org</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[8]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='XGopal2023Mob'></a><span class='cmr-9'>Goutam Yelluru  Gopal  and  Maria  Amer.   Mobile  vision  transformer-based  visual  object
    tracking.  In </span><span class='cmti-9'>34th British Machine Vision Conference (BMVC)</span><span class='cmr-9'>, Aberdeen, UK, November 2023.</span>
    <a class='url' href='https://arxiv.org/abs/2309.05829'><span class='cmtt-9'>https://arxiv.org/abs/2309.05829</span></a> <span class='cmr-9'>and </span><a class='url' href='https://github.com/goutamyg/MVT'><span class='cmtt-9'>https://github.com/goutamyg/MVT</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
  <span class='cmr-9'>[9]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='XGopal2024Sep'></a><span class='cmr-9'>Goutam Yelluru Gopal and Maria Amer.  Separable self and mixed attention transformers
    for  efficient  object  tracking.   In  </span><span class='cmti-9'>IEEE/CVF Winter Conference on Applications of Computer
    Vision  (WACV)</span><span class='cmr-9'>,  Waikoloa,  Hawaii,  January  2024.   </span><a class='url' href='https://arxiv.org/abs/2309.03979'><span class='cmtt-9'>https://arxiv.org/abs/2309.03979</span></a> <span class='cmr-9'>and</span>
    <a class='url' href='https://github.com/goutamyg/SMAT'><span class='cmtt-9'>https://github.com/goutamyg/SMAT</span></a><span class='cmr-9'>.</span>
                                                                               

                                                                               
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[10]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xlai-haotao-mcthesis19'></a><span class='cmr-9'>Haotao              Lai.                                       An              OpenISS             framework
    specialization for deep learning-based person re-identification.   Master’s thesis, Department of
    Computer Science and Software Engineering, Concordia University, Montreal, Canada, August
    2019. </span><a class='url' href='https://spectrum.library.concordia.ca/id/eprint/985788/'><span class='cmtt-9'>https://spectrum.library.concordia.ca/id/eprint/985788/</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[11]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xopeniss-yolov3'></a><span class='cmr-9'>Haotao      Lai      et al.                 OpenISS      keras-yolo3      v0.1.0,      June      2021.</span>
    <a class='url' href='https://github.com/OpenISS/openiss-yolov3'><span class='cmtt-9'>https://github.com/OpenISS/openiss-yolov3</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[12]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xopeniss-reid-tfk'></a><span class='cmr-9'>Haotao   Lai   et al.       Openiss   person   re-identification   baseline   v0.1.1,   June   2021.</span>
    <a class='url' href='https://github.com/OpenISS/openiss-reid-tfk'><span class='cmtt-9'>https://github.com/OpenISS/openiss-reid-tfk</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[13]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xmatlab'></a><span class='cmr-9'>MathWorks. MATLAB. [online], 2000–2012. </span><a class='url' href='http://www.mathworks.com/products/matlab/'><span class='cmtt-9'>http://www.mathworks.com/products/matlab/</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[14]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xoi-containers-poster-siggraph2023'></a><span class='cmr-9'>Serguei  Mokhov,  Jonathan  Llewellyn,  Carlos  Alarcon Meza,  Tariq  Daradkeh,  and  Gillian
    Roper.   The use of containers in OpenGL, ML and HPC for teaching and research support.
    In  </span><span class='cmti-9'>ACM  SIGGRAPH  2023  Posters</span><span class='cmr-9'>,  SIGGRAPH  ’23,  New  York,  NY,  USA,  2023.  ACM.</span>
    <a class='url' href='https://doi.org/10.1145/3588028.3603676'><span class='cmtt-9'>https://doi.org/10.1145/3588028.3603676</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[15]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xmarfcat-sate2010-nist'></a><span class='cmr-9'>Serguei A.  Mokhov.     The  use  of  machine  learning  with  signal-  and  NLP  processing
    of  source  code  to  fingerprint,  detect,  and  classify  vulnerabilities  and  weaknesses  with
    MARFCAT.       Technical   Report   NIST   SP   500-283,   NIST,   October   2011.       Report:</span>
    <a class='url' href='http://www.nist.gov/manuscript-publication-search.cfm?pub_id=909407'><span class='cmtt-9'>http://www.nist.gov/manuscript-publication-search.cfm?pub_id=909407</span></a><span class='cmr-9'>, online e-print at</span>
    <a class='url' href='http://arxiv.org/abs/1010.2511'><span class='cmtt-9'>http://arxiv.org/abs/1010.2511</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[16]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xmokhov-phd-thesis-2013'></a><span class='cmr-9'>Serguei A. Mokhov. </span><span class='cmti-9'>Intensional Cyberforensics</span><span class='cmr-9'>. PhD thesis, Department of Computer Science
    and Software Engineering, Concordia University, Montreal, Canada, September 2013.  Online at</span>
    <a class='url' href='http://arxiv.org/abs/1312.0466'><span class='cmtt-9'>http://arxiv.org/abs/1312.0466</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[17]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xmac-spoofer-analyzer-detail-fps2014'></a><span class='cmr-9'>Serguei A. Mokhov, Michael J. Assels, Joey Paquet, and Mourad Debbabi. Automating MAC
    spoofer evidence gathering and encoding for investigations.  In Frederic Cuppens et al., editors,
    </span><span class='cmti-9'>Proceedings of The 7th International Symposium on Foundations &amp; Practice of Security (FPS’14)</span><span class='cmr-9'>,
    LNCS 8930, pages 168–183. Springer, November 2014. Full paper.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[18]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xmac-spoofer-analyzer-intro-c3s2e2014'></a><span class='cmr-9'>Serguei A. Mokhov, Michael J. Assels, Joey Paquet, and Mourad Debbabi. Toward automated
    MAC spoofer investigations.  In </span><span class='cmti-9'>Proceedings of C3S2E’14</span><span class='cmr-9'>, pages 179–184. ACM, August 2014.
    Short paper.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[19]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xspeed-intro-preso'></a><span class='cmr-9'>Serguei A.      Mokhov      and      Scott      Bunnell.                 Speed      server      farm:
                                                                               

                                                                               
    Gina      Cody      School      of      ENCS      HPC      facility.               [online],      2018–2019.</span>
    <a class='url' href='https://docs.google.com/presentation/d/1bWbGQvYsuJ4U2WsfLYp8S3yb4i7OdU7QDn3l_Q9mYis'><span class='cmtt-9'>https://docs.google.com/presentation/d/1bWbGQvYsuJ4U2WsfLYp8S3yb4i7OdU7QDn3l_Q9mYis</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[20]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xmarfcat-nlp-ai2014'></a><span class='cmr-9'>Serguei A. Mokhov, Joey Paquet, and Mourad Debbabi. The use of NLP techniques in static
    code analysis to detect weaknesses and vulnerabilities.  In Maria Sokolova and Peter van Beek,
    editors, </span><span class='cmti-9'>Proceedings of Canadian Conference on AI’14</span><span class='cmr-9'>, volume 8436 of </span><span class='cmti-9'>LNAI</span><span class='cmr-9'>, pages 326–332.
    Springer, May 2014. Short paper.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[21]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xniksirat2020'></a><span class='cmr-9'>Parna  Niksirat,  Adriana  Daca,  and  Krzysztof  Skonieczny.   The  effects  of  reduced-gravity
    on planetary rover mobility.   </span><span class='cmti-9'>International Journal of Robotics Research</span><span class='cmr-9'>, 39(7):797–811, 2020.</span>
    <a class='url' href='https://doi.org/10.1177/0278364920913945'><span class='cmtt-9'>https://doi.org/10.1177/0278364920913945</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[22]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xaosa-book-vol1-bash'></a><span class='cmr-9'>Chet    Ramey.          The    Bourne-Again    Shell.          In    Brown    and    Wilson    </span><span class='cite'><span class='cmr-9'>[</span><a href='#Xaosa-book-vol1'><span class='cmr-9'>7</span></a><span class='cmr-9'>]</span></span><span class='cmr-9'>.</span>
    <a class='url' href='http://aosabook.org/en/bash.html'><span class='cmtt-9'>http://aosabook.org/en/bash.html</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[23]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xscholarpedia-matlab'></a><span class='cmr-9'>Rob      Schreiber.                 MATLAB.                 </span><span class='cmti-9'>Scholarpedia</span><span class='cmr-9'>,      2(6):2929,      2007.</span>
    <a class='url' href='http://www.scholarpedia.org/article/MATLAB'><span class='cmtt-9'>http://www.scholarpedia.org/article/MATLAB</span></a><span class='cmr-9'>.</span>
    </p>
    <p class='bibitem'><span class='biblabel'>
 <span class='cmr-9'>[24]</span><span class='bibsp'><span class='cmr-9'>   </span></span></span><a id='Xmarf'></a><span class='cmr-9'>The   MARF   Research   and   Development   Group.      The   Modular   Audio   Recognition
    Framework   and   its   Applications.        [online],   2002–2014.        </span><a class='url' href='http://marf.sf.net'><span class='cmtt-9'>http://marf.sf.net</span></a>  <span class='cmr-9'>and</span>
    <a class='url' href='http://arxiv.org/abs/0905.1235'><span class='cmtt-9'>http://arxiv.org/abs/0905.1235</span></a><span class='cmr-9'>, last viewed May 2015.</span>
</p>
    </div>
    
</body> 
</html>