The cluster has two GPU nodes, each with six Tesla (CUDA-compatible) P6
cards: each card has 2048 cores and 16 GB of RAM. Though note that the P6
is mainly a single-precision card, so unless you need the GPU double
precision, double-precision calculations will be faster on a CPU node.

Job scripts for the GPU queue differ in that they do not need these
statements:

#$ -pe smp <threadcount>
#$ -l h_vmem=<memory>G

But do need this statement, which attaches either a single GPU, or, two
GPUs, to the job:

#$ -l gpu=[1|2]

Single-GPU jobs are granted 5 CPU cores and 80 GB of system memory, and
dual-GPU jobs are granted 10 CPU cores and 160 GB of system memory. A
total of *four* GPUs can be actively attached to any one user at any given
time.

Once that your job script is ready, you can submit it to the GPU queue
with:

qsub -q g.q ./<myscript>.sh

And you can query nvidia-smi on the node that is running your job with:

ssh <username>@speed[-05|-17] nvidia-smi

Status of the GPU queue can be queried with:

qstat -f -u "*" -q g.q

Very important note regarding TensorFlow and PyTorch: if you are planning
to run TensorFlow and/or PyTorch multi-GPU jobs, do not use the
tf.distribute and/or torch.nn.DataParallel functions, as they will crash
the compute node (100-percent certainty). The workaround is to either
manually effect GPU parallelisation (TensorFlow has an example on how to
do this), or to run on a single GPU.

