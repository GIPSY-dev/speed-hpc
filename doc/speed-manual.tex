\documentclass{easychair}

% https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
\usepackage{listings}

% For inline citations
\usepackage{bibentry}
\nobibliography*

% Folders with images
\makeatletter
\providecommand*{\input@path}{}
\g@addto@macro\input@path{{../src/}{src/}}% append
\makeatother

\input{commands}

%% Document
%%
\begin{document}

% ------------------------------------------------------------------------------
%% Front Matter
%%
% Regular title as in the article class.
%
\title{Speed: The GCS ENCS Cluster}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. Use {\sf} for highlighting your system
% name, application, or a tool.
%
\titlerunning{Speed: The GCS ENCS Cluster}

% Previously VI
%\date{Version 6.5}
%\date{\textbf{Version 6.6-dev-07}}
%\date{\textbf{Version 6.6} (final GE version)}
\date{\textbf{Version 7.0-dev-01} (final GE version)}

% Authors are joined by \and and their affiliations are on the
% subsequent lines separated by \\ just like the article class
% allows.
%
\author{
    Serguei A. Mokhov
\and
    Gillian A. Roper
\and
    Network, Security and HPC Group\footnote{The group acknowledges the initial manual version VI produced by Dr.~Scott Bunnell while with us
		as well as Dr.~Tariq Daradkeh for his instructional support of the users and contribution of examples.}\\
    \affiliation{Gina Cody School of Engineering and Computer Science}\\
    \affiliation{Concordia University}\\
    \affiliation{Montreal, Quebec, Canada}\\
    \affiliation{\url{rt-ex-hpc~AT~encs.concordia.ca}}\\
}

% \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads.
%
\authorrunning{Mokhov, Roper, NAG/HPC, GCS ENCS}
\indexedauthor{Mokhov, Serguei}
\indexedauthor{Roper, Gillian}
\indexedauthor{NAG/HPC}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ------------------------------------------------------------------------------
\begin{abstract}
This document presents a quick start guide to the usage of the Gina Cody School 
of Engineering and Computer Science compute server farm called ``Speed'' -- the 
GCS Speed cluster, managed by the HPC/ NAG group of the Academic Information 
Technology Services (AITS) at GCS, Concordia University, Montreal, Canada.
\end{abstract}

% ------------------------------------------------------------------------------
\tableofcontents
\clearpage

% ------------------------------------------------------------------------------
\section{Introduction}

This document contains basic information required to use ``Speed'' as well as 
tips and tricks, examples, and references to projects and papers that have used Speed. 
User contributions of sample jobs and/ or references are welcome. 
Details are sent to the \texttt{hpc-ml} mailing list.

% ------------------------------------------------------------------------------
\subsection{Resources}

\begin{itemize}
\item
Our public GitHub page where the manual and sample job scripts
are maintained (pull-requests (PRs), subject to review, are welcome):\\
\url{https://github.com/NAG-DevOps/speed-hpc}\\
\url{https://github.com/NAG-DevOps/speed-hpc/pulls}

\item
PDF version of this manual:\\
\url{https://github.com/NAG-DevOps/speed-hpc/blob/master/doc/speed-manual.pdf}\\
HTML version of this manual:\\
\url{https://nag-devops.github.io/speed-hpc/}

\item
Our official Concordia page for the ``Speed'' cluster:\\
\url{https://www.concordia.ca/ginacody/aits/speed.html}\\
which includes access request instructions.

\item
All Speed users are subscribed to the \texttt{hpc-ml} mailing
list.

\item
\href
	{https://docs.google.com/presentation/d/1zu4OQBU7mbj0e34Wr3ILXLPWomkhBgqGZ8j8xYrLf44}
	{Speed Server Farm Presentation 2022}~\cite{speed-intro-preso}.

\end{itemize}

% ------------------------------------------------------------------------------
\subsection{Team}

\begin{itemize}
	\item 
Serguei Mokhov, PhD, Manager, Networks, Security and HPC, AITS
	\item 
Gillian Roper, Senior Systems Administrator, HPC, AITS
	\item 
Carlos Alarc√≥n Meza, Systems Administrator, HPC and Networking, AITS
	%\item 
%Tariq Daradkeh, PhD, IT Instructional Specialist, Information Technology
\end{itemize}

We receive support from the rest of AITS teams, such as NAG, SAG, FIS, and DOG.

% ------------------------------------------------------------------------------
\subsection{What Speed Consists of}

\begin{itemize}
\item
Twenty four (24) 32-core compute nodes, each with 512~GB of memory and 
approximately 1~TB of local volatile-scratch disk space. 

\item
Twelve (12) NVIDIA Tesla P6 GPUs, with 16~GB of memory (compatible with the 
CUDA, OpenGL, OpenCL, and Vulkan APIs). 

\item
4 VIDPRO nodes, with 6 P6 cards, and 6 V100 cards (32GB), and 
256GB of RAM.

\item
7 new SPEED2 servers with 4x A100 80GB GPUs each, partitioned
into 4x 20GB each.

\item
One AMD FirePro S7150 GPUs, with 8~GB of memory (compatible with the
Direct~X, OpenGL, OpenCL, and Vulkan APIs). 

\end{itemize}

% ------------------------------------------------------------------------------
\subsection{What Speed Is Ideal For}
\label{sect:speed-is-for}

\begin{itemize}
\item
To design and develop, test and run parallel, batch, and other algorithms, scripts with partial data sets. ``Speed'' has been optimised for compute jobs that are multi-core aware, require a large memory space, or are iteration intensive.
\item
Prepare them for big clusters:
	\begin{itemize}
	\item 
	Digital Research Alliance of Canada (Calcul Quebec and Compute Canada)
	\item 
	Cloud platforms
	\end{itemize}
\item
Jobs that are too demanding for a desktop. 
\item
Single-core batch jobs; multithreaded jobs up to 32 cores (i.e., a single machine).
\item
Anything that can fit into a 500-GB memory space and a scratch space of approximately ~10TB. 
\item
CPU-based jobs. 
\item
CUDA GPU jobs (\texttt{speed-05}, \texttt{speed-17}, \texttt{speed-37}--\texttt{speed-43}).
\item
Non-CUDA GPU jobs using OpenCL (\texttt{speed-19} and \texttt{speed-05|17}).
\end{itemize}

% ------------------------------------------------------------------------------
\subsection{What Speed Is Not}
\label{sect:speed-is-not}

\begin{itemize}
\item Speed is not a web host and does not host websites.
\item Speed is not meant for Continuous Integration (CI) automation deployments for Ansible or similar tools. 
\item Does not run Kubernetes or other container orchestration software.
\item Does not run Docker. (Note: Speed does run Singularity and many Docker containers can be converted to Singularity containers with a single command.)
\item Speed is not for jobs executed outside of the scheduler. (Jobs running outside of the scheduler will be killed and all data lost.)
\end{itemize}

% ------------------------------------------------------------------------------
\subsection{Available Software}

We have a great number of open-source software available and installed
on ``Speed''~--~various Python, CUDA versions, {\cpp}/{\java} compilers, OpenGL,
OpenFOAM, OpenCV, TensorFlow, OpenMPI, OpenISS, {\marf}~\cite{marf}, etc.
There are also a number of commercial packages, subject to licensing
contributions, available, such as MATLAB~\cite{matlab,scholarpedia-matlab}, Abaqus~\cite{abaqus}, 
Ansys, Fluent~\cite{fluent}, etc. 

To see the packages available, run \texttt{ls -al /encs/pkg/} on \texttt{speed.encs}.

In particular, there are over 2200 programs available in
\texttt{/encs/bin} and \texttt{/encs/pkg} under Scientific Linux 7 (EL7).
We are building an equivalent array of programs for the EL9 SPEED2 nodes.

\begin{itemize}
	\item 
Popular concrete examples:
\begin{itemize}
	\item 
MATLAB (R2016b, R2018a, R2018b)
	\item 
Fluent (19.2)
	\item 
Singularity containers can run other operating systems and linux distributions, like Ubuntu's, as well as converted Docker containers.
\end{itemize}
	\item 
We do our best to accommodate custom software requests.
Python environments can use user-custom installs 
from within the scratch directory.
	\item 
A number of specific environments are available and 
can be loaded using the \tool{module} command:
\begin{itemize}
	\item 
Python (2.3.0 - 3.5.1)
	\item 
Gurobi (7.0.1, 7.5.0, 8.0.0, 8.1.0)
	\item 
Ansys (16, 17, 18, 19)
	\item 
OpenFOAM (2.3.1, 3.0.1, 5.0, 6.0)
	\item 
Cplex 12.6.x to 12.8.x
	\item 
OpenMPI 1.6.x, 1.8.x, 3.1.3
\end{itemize}
\end{itemize}

% ------------------------------------------------------------------------------
\subsection{Requesting Access}

After reviewing the ``What Speed is'' (\xs{sect:speed-is-for}) and
``What Speed is Not'' (\xs{sect:speed-is-not}), request access to the ``Speed'' 
cluster by emailing: \texttt{rt-ex-hpc AT encs.concordia.ca}.
%
Faculty and staff may request access directly.
Students must include the following in their message:

\begin{itemize} 
	\item GCS ENCS username
	\item Name and email (CC) of the supervisor or instructor
	\item Written request from the supervisor or instructor for the ENCS username to be granted access to ``Speed''
\end{itemize}

% ------------------------------------------------------------------------------
\section{Job Management}
\label{sect:job-management}

In these instructions, anything bracketed like so, \verb+<>+, indicates a
label/value to be replaced (the entire bracketed term needs replacement).

% ------------------------------------------------------------------------------
\subsection{Getting Started}

Before getting started, please review the ``What Speed is'' (\xs{sect:speed-is-for})
and ``What Speed is Not'' (\xs{sect:speed-is-not}).
Once your GCS ENCS account has been granted access to ``Speed'',
use your GCS ENCS account credentials to create an SSH connection to 
\texttt{speed} (an alias for \texttt{speed-submit.encs.concordia.ca}). 

All users are expected to have a basic understanding of
 Linux and its commonly used commands. 
 % ------------------------------------------------------------------------------
\subsubsection{SSH Connections}
\label{sect:ssh}

Requirements to create connections to Speed:
\begin{enumerate}
	\item
An active \textbf{ENCS user account} which has permission to connect to Speed.
	\item
If you are off campus, an active connection to Concordia's VPN.
Accessing Concordia's VPN requires a Concordia \textbf{netname}. 
	\item
Windows systems require a terminal emulator such as PuTTY, Cygwin (or MobaXterm). 
\end{enumerate}

Open up a terminal window and type in the following SSH command being sure to replace
\verb!<ENCSusername>! with your ENCS account's username.

\begin{verbatim}
ssh <ENCSusername>@speed.encs.concordia.ca
\end{verbatim}

% ------------------------------------------------------------------------------
% TMP scheduler-specific section
\input{scheduler-env}

% ------------------------------------------------------------------------------
\subsection{Job Submission Basics}

Preparing your job for submission is fairly straightforward. Start by basing your job script on one of the examples available in the ``src''
directory of our GitHub's (\url{https://github.com/NAG-DevOps/speed-hpc}).

Job scripts are broken into four main sections: 
\begin{itemize}
	\item Directives
	\item Module Loads
	\item User Scripting
\end{itemize}

% ------------------------------------------------------------------------------
% TMP scheduler-specific section
\input{scheduler-directives}

% ------------------------------------------------------------------------------
\subsubsection{Module Loads}

As your job will run on a compute or GPU ``Speed'' node, and not the submit node,
any software that is needed must be loaded by the job script. Software is loaded
within the script just as it would be from the command line.

To see a list of which modules are available, execute the following from the 
command line on \texttt{speed-submit}.

\begin{verbatim}
module avail
\end{verbatim}

To list for a particular program (\tool{matlab}, for example):

\begin{verbatim}
module -t avail matlab
\end{verbatim}

Which, of course, can be shortened to match all that start with a
particular letter:

\begin{verbatim}
module -t avail m
\end{verbatim}

Insert the following in your script to load the \tool{matlab/R2020a}) module:

\begin{verbatim}
module load matlab/R2020a/default
\end{verbatim}

Use, \option{unload}, in place of, \option{load}, to remove a module from active use.

To list loaded modules:

\begin{verbatim}
module list
\end{verbatim}

To purge all software in your working environment:

\begin{verbatim}
module purge
\end{verbatim}

Typically, only the \texttt{module load} command will be used in your script.

% ------------------------------------------------------------------------------
% TMP scheduler-specific section
\input{scheduler-scripting}

% ------------------------------------------------------------------------------
\subsection{SSH Keys For MPI}

Some programs effect their parallel processing via MPI (which is a 
communication protocol). An example of such software is Fluent. MPI needs to 
have `passwordless login' set up, which means SSH keys. In your NFS-mounted 
home directory:

\begin{itemize}
\item
\texttt{cd .ssh}
\item
\texttt{ssh-keygen -t ed25519} (default location; blank passphrase) 
\item
\texttt{cat id\_ed25519.pub >> authorized\_keys} (if the \texttt{\href{https://www.ssh.com/academy/ssh/authorized-keys-file}{authorized\_keys}}
file already exists) \emph{OR} \texttt{cat id\_ed25519.pub > authorized\_keys} (if does not) 
\item
Set file permissions of \texttt{authorized\_keys} to 600; of your NFS-mounted home
to 700 (note that you likely will not have to do anything here, as most people
will have those permissions by default). 
\end{itemize}

% ------------------------------------------------------------------------------
\subsection{Creating Virtual Environments}
\label{sect:environments}

The following documentation is specific to the \textbf{Speed} HPC Facility at the
Gina Cody School of Engineering and Computer Science.

% ------------------------------------------------------------------------------
\subsubsection{Anaconda}

To create an anaconda environment in your speed-scratch directory, use the \texttt{\-\-prefix} 
option when executing \texttt{conda create}. For example, to create an anaconda environment for 
\texttt{a\_user}, execute the following at the command line:

\begin{verbatim}
conda create --prefix /speed-scratch/a_user/myconda
\end{verbatim}

\vspace{10pt}
\noindent
\textbf{Note:} Without the \texttt{\-\-prefix} option, the \texttt{conda create} command creates the 
environment in \texttt{a\_user}'s home directory by default.
\vspace{10pt}

% ------------------------------------------------------------------------------
\paragraph{List Environments.}

To view your conda environments, type: \texttt{conda info --envs}

\begin{verbatim}
# conda environments:
#
base                  *  /encs/pkg/anaconda3-2019.07/root
                         /speed-scratch/a_user/myconda
\end{verbatim}      

% ------------------------------------------------------------------------------
\paragraph{Activate an Environment.}

Activate the environment \texttt{\/speed\-scratch\/a\_user\/myconda} as follows
\begin{verbatim}
conda activate /speed-scratch/a_user/myconda
\end{verbatim}
After activating your environment, add \tool{pip} to your environment by using 
\begin{verbatim}
conda install pip
\end{verbatim}
This will install \tool{pip} and \tool{pip}'s dependencies, including python, 
into the environment.

\vspace{10pt}
\noindent
\textbf{Important Note:} \tool{pip} (and \tool{pip3}) are used to install modules
 from the python distribution while \texttt{conda install} installs modules from 
 anaconda's repository.
\vspace{10pt}

% ------------------------------------------------------------------------------
% TMP scheduler-specific section
\input{scheduler-job-examples}

% ------------------------------------------------------------------------------
\section{Conclusion}
\label{sect:conclusion}

The cluster is, ``first come, first served'', until it fills, and then job
position in the queue is based upon past usage. The scheduler does attempt
to fill gaps, though, so sometimes a single-core job of lower priority
will schedule before a multi-core job of higher priority, for example.

% ------------------------------------------------------------------------------
\subsection{Important Limitations}
\label{sect:limitations}

\begin{itemize}
\item
New users are restricted to a total of 32 cores: write to \url{rt-ex-hpc@encs.concordia.ca}
if you need more temporarily (256 is the maximum possible, or, 8 jobs of 32 cores each).

\item
Job sessions are a maximum of one week in length (only 24 hours, though,
for interactive jobs).

\item
Scripts can live in your NFS-provided home, but any substantial data need
to be in your cluster-specific directory
(located at \verb+/speed-scratch/<ENCSusername>/+).

NFS is great for acute activity, but is not ideal for chronic activity.
Any data that a job will 
read more than once should be copied at the start to the scratch disk of a 
compute node using \api{\$TMPDIR} (and, perhaps, \api{\$SLURM\_SUBMIT\_DIR}), 
any intermediary job data should be produced in \api{\$TMPDIR}, and once a 
job is near to finishing, those data should be copied to your NFS-mounted 
home (or other NFS-mounted space) from \api{\$TMPDIR} (to, perhaps,
\api{\$SLURM\_SUBMIT\_DIR}). In other words, IO-intensive operations should be effected 
locally whenever possible, saving network activity for the start and end of 
jobs. 

\item
Your current resource allocation is based upon past usage, which is an 
amalgamation of approximately one week's worth of past wallclock (i.e., time 
spent on the node(s)) and compute activity (on the node(s)).

\item
Jobs should NEVER be run outside of the province of the scheduler.
Repeat offenders risk loss of cluster access. 

\end{itemize}

% ------------------------------------------------------------------------------
% TMP scheduler-specific section
\input{scheduler-tips}

% ------------------------------------------------------------------------------
\subsection{Use Cases}
\label{sect:cases}

\begin{itemize}
	\item 
HPC Committee's initial batch about 6 students (end of 2019):
\begin{itemize}
	\item 
10000 iterations job in Fluent finished in $<26$ hours vs. 46 hours in Calcul Quebec
\end{itemize}
	\item 
NAG's MAC spoofer analyzer~\cite{mac-spoofer-analyzer-intro-c3s2e2014,mac-spoofer-analyzer-detail-fps2014},
such as \url{https://github.com/smokhov/atsm/tree/master/examples/flucid}
\begin{itemize}
	\item 
compilation of forensic computing reasoning cases about false or true positives of hardware address spoofing in the labs
\end{itemize}
	\item 
S4 LAB/GIPSY R\&D Group's:
\begin{itemize}
	\item 
MARFCAT and MARFPCAT (OSS signal processing and machine learning tools for 
vulnerable and weak code analysis and network packet capture
analysis)~\cite{marfcat-nlp-ai2014,marfcat-sate2010-nist,fingerprinting-mal-traffic}
	\item 
Web service data conversion and analysis
	\item 
{\flucid} encoders (translation of large log data into {\flucid}~\cite{mokhov-phd-thesis-2013} for forensic analysis)
	\item 
Genomic alignment exercises
\end{itemize}
\item
\bibentry{oi-containers-poster-siggraph2023}
\item
\bibentry{Gopal2024Sep}
\item
\bibentry{Gopal2023Mob}
\item
\bibentry{niksirat2020}

\item
The work ``\bibentry{lai-haotao-mcthesis19}'' using TensorFlow and Keras on OpenISS
adjusted to run on Speed based on the repositories:
\begin{itemize}
	\item 
\bibentry{openiss-reid-tfk} and
	\item
\bibentry{openiss-yolov3}
\end{itemize}
and theirs forks by the team.

\end{itemize}

% ------------------------------------------------------------------------------
\appendix

% ------------------------------------------------------------------------------
\section{History}

% ------------------------------------------------------------------------------
\subsection{Acknowledgments}
\label{sect:acks}

\begin{itemize}
	\item 
The first 6 (to 6.5) versions of this manual and early UGE job script samples,
Singularity testing and user support were produced/done by Dr.~Scott Bunnell
during his time at Concordia as a part of the NAG/HPC group. We thank
him for his contributions.
	\item 
The HTML version with devcontainer support was contributed by Anh H Nguyen.
	\item 
Tariq Daradkeh, PhD, was our IT Instructional Specialist August 2022 to September 2023;
working on the scheduler, scheduling research, end user support, and integration of
examples, such as YOLOv3 in \xs{sect:openiss-yolov3} other tasks. We have a continued
collaboration on HPC/scheduling research.
\end{itemize}

% ------------------------------------------------------------------------------
\subsection{Phase 4}

Phase 4 had 7 SuperMicro servers with 4x A100 80GB GPUs each added,
dubbed as ``SPEED2''.

% ------------------------------------------------------------------------------
\subsection{Phase 3}

Phase 3 had 4 vidpro nodes added from Dr.~Amer totalling 6x P6 and 6x V100
GPUs added.

% ------------------------------------------------------------------------------
\subsection{Phase 2}

Phase 2 saw 6x NVIDIA Tesla P6 added and 8x more compute nodes.
The P6s replaced 4x of FirePro S7150.

% ------------------------------------------------------------------------------
\subsection{Phase 1}

Phase 1 of Speed was of the following configuration:

\begin{itemize}
\item
Sixteen, 32-core nodes, each with 512~GB of memory and approximately 1~TB of volatile-scratch disk space. 
\item
Five AMD FirePro S7150 GPUs, with 8~GB of memory (compatible with the Direct X, OpenGL, OpenCL, and Vulkan APIs). 
\end{itemize}

% ------------------------------------------------------------------------------
% TMP scheduler-specific section
\input{scheduler-faq}

% ------------------------------------------------------------------------------
\section{Sister Facilities}

Below is a list of resources and facilities similar to Speed at various capacities.
Depending on your research group and needs, they might be available to you. They
are not managed by HPC/NAG of AITS, so contact their respective representatives.

\begin{itemize}
\item
\texttt{computation.encs} CPU only 3-machine cluster running longer jobs
without a scheduler at the moment
\item
\texttt{apini.encs} cluster for teaching and MPI programming (see the corresponding
course in CSSE)
\item
Computer Science and Software Engineering (CSSE) Virya GPU Cluster. For CSSE 
members only. The cluster has 4 nodes with total of 32 NVIDIA GPUs (a mix of
V100s and A100s). To request access send email to \texttt{virya.help@concordia.ca}.
\item
Dr. Maria Amer's VidPro group's nodes in Speed (-01, -03, -25, -27) with additional V100 and P6 GPUs.
\item
There are various Lambda Labs other GPU servers and like computers
acquired by individual researchers; if you are member of their
research group, contact them directly. These resources are not
managed by us.
\begin{itemize}
\item
Dr. Amin Hammad's \texttt{construction.encs} Lambda Labs station
\item
Dr. Hassan Rivaz's \texttt{impactlab.encs} Lambda Labs station
\item
Dr. Nizar Bouguila's \texttt{xailab.encs} Lambda Labs station
\item
Dr. Roch Glitho's \texttt{femto.encs} server
\item
Dr. Maria Amer's \texttt{venom.encs} Lambda Labs station
\item
Dr. Leon Wang's \texttt{guerrera.encs} DGX station
\end{itemize}
\item
Dr. Ivan Contreras' servers (managed by AITS)
\item
If you are a member of School of Health (formerly PERFORM Center),
you may have access to their local 
\href
{https://perform-wiki.concordia.ca/mediawiki/index.php/HPC_Cluster}
{PERFORM's High Performance Computing (HPC) Cluster}.
Contact Thomas Beaudry for details and how to obtain access.
\item
Digital Research Alliance Canada (Compute Canada / Calcul Quebec),\\
\url{https://alliancecan.ca/}

\end{itemize}

% ------------------------------------------------------------------------------
% Refs:
%
\nocite{aosa-book-vol1}
\label{sect:bib}
%\bibliographystyle{IEEEtran}
\bibliographystyle{plain}
%\bibliographystyle{alpha}
%\bibliographystyle{unsrt}
%\bibliographystyle{abbrv}
% Create a section for references otherwise it appears to be part of the "Sister Facilities" Appendix
\clearpage
\addcontentsline{toc}{section}{Annotated Bibliography} 
\bibliography{speed-manual}

%------------------------------------------------------------------------------
\end{document}
